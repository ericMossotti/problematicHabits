[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Problematic Internet Habits in Children",
    "section": "",
    "text": "The grounds for analyses in this case is the problematic behavior as it pertains to the subjects’ experience and their parents’ via questionnaires, physical fitness tests, worn instrument measurements and demographics. The resulting data is to be processed and connections drawn.\nThere are quite a few variables to consider here. The largest amount of data was collected in real time. This was recorded by a device that could measure acceleration in 3D space, ambient light, time, and the quarter of the year while the participant went about their lives. Many health issues could stem from a lack of exercise. It may follow that the more someone is actively using the internet, the less likely they are to meet daily activity recommendations.\nSome issues is that it is uncertain where this feat of data collection took place. Perhaps, this was for the sake of privacy? The exact dates are not given for the real-time data either. So, we have to go by the quarter of the year and or season in that respect. Not sure if seasonal information is consequential to anything.\nThe goal of the models is to, ultimately, make progress in the field of childhood to adolescent psychology. Progress, in this case, is defined by accurately predicting risks of undesirable outcomes based on a young persons’ daily activities.\nNowadays, life is centered around the internet. At the same time, our biology has not undergone any systemic changes. First hand experience certainly tells me that that high levels of internet activity can have a negative impact on the mind.\nIs the quantity of internet activity itself truly determinant of negative outcome?\n\n\n\n\n\n\nx\n# https://emilhvitfeldt.github.io/paletteer/\n\nlibrary(gt)\nlibrary(purrr)\nlibrary(tibble)\nlibrary(reticulate)\n\nr_table_theming &lt;- function(rDataFrame,\n                            title,\n                            subtitle,\n                            footnotes_df,\n                            source_note) {\n  r_table &lt;- gt(rDataFrame) |&gt;\n    data_color(palette = \"ggsci::legacy_tron\",\n               columns = footnotes_df$locations[[1]],\n               target_columns = everything()) |&gt;\n    tab_header(title = title, subtitle = subtitle)\n  \n  r_table &lt;- r_table |&gt;\n      tab_options(\n        heading.background.color = '#222',\n        column_labels.background.color = '#333',\n        #table.background.color = '#222',\n        footnotes.background.color = '#222',\n        source_notes.background.color = '#222'\n        )\n    \n    r_table &lt;- r_table |&gt;\n      tab_source_note(source_note = source_note)\n\n  # Footnotes are added to the accumulator, building up to the final result\n  r_table &lt;- seq_len(nrow(footnotes_df)) |&gt;\n    reduce(\\(acc, i) {\n      tab_footnote(\n        acc,\n        footnote = footnotes_df$notes[[i]],\n        location = cells_column_labels(columns = footnotes_df$locations[[i]]),\n        placement = \"auto\"\n        )\n      }, .init = r_table)\n  \n  return(r_table)\n}\n\n\n\n\n\n\n\nCode\nlibrary(reticulate)\nlibrary(ggplot2)\n\nggplot_theming_flipped_axes &lt;- function() {\n  theme_linedraw() +\n    theme(\n      plot.title = element_text(color = 'white'),\n      plot.background = element_rect(fill = '#222'),\n      panel.background = element_rect(fill = '#222'),\n      panel.grid.major.x = element_line(linetype = 'dashed'),\n      panel.grid.minor.x = element_line(linetype = \"dotted\"),\n      panel.grid.major.y = element_line(linetype = 'solid'),\n      panel.grid.minor.y = element_line(linetype = 'dotted'),\n      axis.title = element_text(color = 'gray100'),\n      axis.text = element_text(color = 'gray'))\n}\n\nggplot_theming &lt;- function() {\n  theme_linedraw() +\n    theme(\n      plot.title = element_text(color = 'white'),\n      plot.background = element_rect(fill = 'transparent'),\n      panel.background = element_rect(fill = 'transparent'),\n      panel.grid.major.x = element_line(linetype = 'solid'),\n      panel.grid.minor.x = element_line(linetype = \"dotted\"),\n      panel.grid.major.y = element_line(linetype = 'dashed'),\n      panel.grid.minor.y = element_line(linetype = 'dotted'),\n      axis.title = element_text(color = 'gray100'),\n      axis.text = element_text(color = 'gray'))\n  }"
  },
  {
    "objectID": "index.html#rationale-and-initial-thoughts",
    "href": "index.html#rationale-and-initial-thoughts",
    "title": "Problematic Internet Habits in Children",
    "section": "",
    "text": "The grounds for analyses in this case is the problematic behavior as it pertains to the subjects’ experience and their parents’ via questionnaires, physical fitness tests, worn instrument measurements and demographics. The resulting data is to be processed and connections drawn.\nThere are quite a few variables to consider here. The largest amount of data was collected in real time. This was recorded by a device that could measure acceleration in 3D space, ambient light, time, and the quarter of the year while the participant went about their lives. Many health issues could stem from a lack of exercise. It may follow that the more someone is actively using the internet, the less likely they are to meet daily activity recommendations.\nSome issues is that it is uncertain where this feat of data collection took place. Perhaps, this was for the sake of privacy? The exact dates are not given for the real-time data either. So, we have to go by the quarter of the year and or season in that respect. Not sure if seasonal information is consequential to anything.\nThe goal of the models is to, ultimately, make progress in the field of childhood to adolescent psychology. Progress, in this case, is defined by accurately predicting risks of undesirable outcomes based on a young persons’ daily activities.\nNowadays, life is centered around the internet. At the same time, our biology has not undergone any systemic changes. First hand experience certainly tells me that that high levels of internet activity can have a negative impact on the mind.\nIs the quantity of internet activity itself truly determinant of negative outcome?\n\n\n\n\n\n\nx\n# https://emilhvitfeldt.github.io/paletteer/\n\nlibrary(gt)\nlibrary(purrr)\nlibrary(tibble)\nlibrary(reticulate)\n\nr_table_theming &lt;- function(rDataFrame,\n                            title,\n                            subtitle,\n                            footnotes_df,\n                            source_note) {\n  r_table &lt;- gt(rDataFrame) |&gt;\n    data_color(palette = \"ggsci::legacy_tron\",\n               columns = footnotes_df$locations[[1]],\n               target_columns = everything()) |&gt;\n    tab_header(title = title, subtitle = subtitle)\n  \n  r_table &lt;- r_table |&gt;\n      tab_options(\n        heading.background.color = '#222',\n        column_labels.background.color = '#333',\n        #table.background.color = '#222',\n        footnotes.background.color = '#222',\n        source_notes.background.color = '#222'\n        )\n    \n    r_table &lt;- r_table |&gt;\n      tab_source_note(source_note = source_note)\n\n  # Footnotes are added to the accumulator, building up to the final result\n  r_table &lt;- seq_len(nrow(footnotes_df)) |&gt;\n    reduce(\\(acc, i) {\n      tab_footnote(\n        acc,\n        footnote = footnotes_df$notes[[i]],\n        location = cells_column_labels(columns = footnotes_df$locations[[i]]),\n        placement = \"auto\"\n        )\n      }, .init = r_table)\n  \n  return(r_table)\n}\n\n\n\n\n\n\n\nCode\nlibrary(reticulate)\nlibrary(ggplot2)\n\nggplot_theming_flipped_axes &lt;- function() {\n  theme_linedraw() +\n    theme(\n      plot.title = element_text(color = 'white'),\n      plot.background = element_rect(fill = '#222'),\n      panel.background = element_rect(fill = '#222'),\n      panel.grid.major.x = element_line(linetype = 'dashed'),\n      panel.grid.minor.x = element_line(linetype = \"dotted\"),\n      panel.grid.major.y = element_line(linetype = 'solid'),\n      panel.grid.minor.y = element_line(linetype = 'dotted'),\n      axis.title = element_text(color = 'gray100'),\n      axis.text = element_text(color = 'gray'))\n}\n\nggplot_theming &lt;- function() {\n  theme_linedraw() +\n    theme(\n      plot.title = element_text(color = 'white'),\n      plot.background = element_rect(fill = 'transparent'),\n      panel.background = element_rect(fill = 'transparent'),\n      panel.grid.major.x = element_line(linetype = 'solid'),\n      panel.grid.minor.x = element_line(linetype = \"dotted\"),\n      panel.grid.major.y = element_line(linetype = 'dashed'),\n      panel.grid.minor.y = element_line(linetype = 'dotted'),\n      axis.title = element_text(color = 'gray100'),\n      axis.text = element_text(color = 'gray'))\n  }"
  },
  {
    "objectID": "index.html#importextract",
    "href": "index.html#importextract",
    "title": "Problematic Internet Habits in Children",
    "section": "Import/Extract",
    "text": "Import/Extract\n\n\nCode\n!kaggle competitions download -c child-mind-institute-problematic-internet-use\n\n\n\n\nCode\nfrom zipfile import ZipFile\nimport os\nimport duckdb\nimport pandas as pd\nimport re\n\n\n\n\nCheck if the directory, ‘data’, exists.\ntry:\n  os.mkdir('data')\nexcept FileExistsError:\n  print(f\"data exists\")\n\n\ndata exists\n\n\n\n\nExtracts the files and lists files.\nzip_path = \"child-mind-institute-problematic-internet-use.zip\"\n\nif not os.path.exists('data/train.csv'):\n  with ZipFile(zip_path) as zf:\n    ZipFile.extractall(zf, path = 'data')\n    print(\n      f\"files were extracted\"\n      ,pd.DataFrame(data = os.listdir('data'), columns = ['data'])\n      ,sep = \"\\n\"*2\n      )\nelse:\n  print(\n    f\"files were already extracted\"\n    ,pd.DataFrame(data = os.listdir('data'), columns = ['data'])\n    ,sep = \"\\n\"*2\n    )\n\n\nfiles were already extracted\n\n                    data\n0               test.csv\n1    data_dictionary.csv\n2  sample_submission.csv\n3    series_test.parquet\n4   series_train.parquet\n5              train.csv\n\n\n\nEstablish a Database Connection\n\n\nCode\nconn = duckdb.connect(':memory:')\nconn.sql(f\"SET memory_limit = '24GB';\")\nconn.sql(f\"SET default_order = 'ASC';\")\n\n\n\n\nNew Data Types (Enums/Enumerations)\nI noticed that using ENUM data types significantly sped up many database operations (Holanda, 2021).\n\nHolanda, P. (2021, November 26). DuckDB – the lord of enums: The fellowship of the categorical and factors. DuckDB. https://duckdb.org/2021/11/26/duck-enum.html\n\n\nDefines the series and dictionary objects for enum functions.\nenums = ['internet_hours_enum'\n         ,'enroll_season_enum'\n         ,'disease_risk_enum'\n         ,'sii_enum', 'age_enum'\n         ,'sex_enum'\n         ,'pciat_season_enum'\n         ,'weekday_enum'\n         ,'quarter_enum'\n         ,'hour_enum'\n         ,'minute_enum'\n         ,'second_enum'\n         ,'id_actigraphy_enum']\n\n#siiSeries = pd.Series(data = ['None', 'Mild', 'Moderate', 'Severe'], dtype = str)\nsiiSeries = pd.Series(data = ['0', '1', '2', '3'], dtype = str)\nageSeries = pd.Series(data = range(5, 23), dtype = str)\nsexSeries = pd.Series(data = ['0', '1'], dtype = str)\npciatSeasonSeries = pd.Series(data = ['Fall'\n                                      ,'Spring'\n                                      ,'Summer'\n                                      ,'Winter'], dtype = str)\n\ninternetHrsSeries = pd.Series(data = ['0', '1', '2', '3'], dtype = str)\nquarterSeries = pd.Series(data = range(1, 5), dtype = str)\nweekdaySeries = pd.Series(data = range(1, 8), dtype = str)\nhourSeries = pd.Series(data = range(0, 24), dtype = str)\nminuteSeries = pd.Series(data = range(0, 60), dtype = str)\nsecondSeries = pd.Series(data = range(0, 60), dtype = str)\ndiseaseRiskSeries = pd.Series(data = ['Underweight'\n                                      ,'Normal'\n                                      ,'Increased'\n                                      ,'High'\n                                      ,'Very High'\n                                      ,'Extremely High'], dtype = str)\n\nid_df = conn.execute(f\"\"\"\n     SELECT \n         DISTINCT(id) AS id\n     FROM \n         read_parquet('data/series_train*/*/*'\n         ,hive_partitioning = true)\n     ORDER BY \n         id ASC;\n     \"\"\").df() \n\nidList = id_df['id'].to_list()\nidSeries = pd.Series(data = idList, dtype = str)\n\nenumDict = {\n  'disease_risk_enum': f\"{tuple(diseaseRiskSeries)}\"\n  ,'enroll_season_enum': f\"{tuple(pciatSeasonSeries)}\"\n  ,'sii_enum': f\"{tuple(siiSeries)}\"\n  ,'age_enum': f\"{tuple(ageSeries)}\"\n  ,'sex_enum': f\"{tuple(sexSeries)}\"\n  ,'pciat_season_enum': f\"{tuple(pciatSeasonSeries)}\"\n  ,'quarter_enum': f\"{tuple(quarterSeries)}\"\n  ,'weekday_enum': f\"{tuple(weekdaySeries)}\"\n  ,'hour_enum': f\"{tuple(hourSeries)}\"\n  ,'minute_enum': f\"{tuple(minuteSeries)}\"\n  ,'second_enum': f\"{tuple(secondSeries)}\"\n  ,'id_actigraphy_enum': f\"{tuple(idSeries)}\"\n  ,'internet_hours_enum': f\"{tuple(internetHrsSeries)}\"\n  }\n\nif len(enumDict) == len(enums):\n  print(f\"drops and creates are same length\")\nelse:\n  print(f\"drops and creates are not the same length\")\n\n\ndrops and creates are same length\n\n\n\n\nCan there be a modular approach to creating and dropping ENUM types? Yes.\ndef try_create(conn,type_str: str, enum_str: str) -&gt; pd.Series:\n  \"\"\"\n  :::WHY(s):::\n    Is there a modular approach to creating ENUM types in a database?\n  :::HOW(s):::\n    Utilize python data structures such as pandas Series and dictionary as parameters.\n  \"\"\"\n  # the try, except can help categorize the outputs for ui/ux\n  try:\n    conn.execute(f\"CREATE TYPE {type_str} AS ENUM {enum_str};\")\n    return pd.Series(type_str, index = ['created'])\n  except duckdb.duckdb.CatalogException:\n    return pd.Series(type_str, index = ['already existed'])\n\ndef try_drop(conn, type_str: str) -&gt; pd.Series:\n  \"\"\"\n  :::WHY(s):::\n    Is there a modular approach to dropping ENUM types in a database?\n  :::HOW(s):::\n    Utilize python data structures such as pandas Series and dictionary as parameters.\n  \"\"\"\n  # the try, except can help categorize the outputs for ui/ux\n  try:\n    conn.execute(f\"DROP TYPE {type_str};\")\n    return pd.Series(type_str, index = ['dropped'])\n  except duckdb.duckdb.CatalogException:\n    return pd.Series(type_str, index = ['did not exist'])\n  \ndroplist = []\nfor e in enums:   \n   droplist.append(try_drop(conn, type_str = e))\n   \ndropFrame = pd.DataFrame(droplist)\ndropFrame = dropFrame.sort_values(by = dropFrame.columns.to_list()\n                                  ,ascending=True\n                                  ,ignore_index=True)\ncreateList = []\nfor eType, eSeries in enumDict.items():\n    createList.append(try_create(conn\n                                 ,type_str = eType\n                                 ,enum_str = eSeries))\ncreateFrame = pd.DataFrame(createList)\ncreateFrame = createFrame.sort_values(by = createFrame.columns.to_list()\n                                      ,ascending=True\n                                      ,ignore_index=True)\n\npd.concat([dropFrame, createFrame], axis = 1)\n\n\n          did not exist              created\n0              age_enum             age_enum\n1     disease_risk_enum    disease_risk_enum\n2    enroll_season_enum   enroll_season_enum\n3             hour_enum            hour_enum\n4    id_actigraphy_enum   id_actigraphy_enum\n5   internet_hours_enum  internet_hours_enum\n6           minute_enum          minute_enum\n7     pciat_season_enum    pciat_season_enum\n8          quarter_enum         quarter_enum\n9           second_enum          second_enum\n10             sex_enum             sex_enum\n11             sii_enum             sii_enum\n12         weekday_enum         weekday_enum\n\n\nNot sure why the data had hyphens in the columns names, originally. These hyphens interfere with SQL query operations, so I have the mind to replace them with underscore characters. I’m not going to overwrite the original CSVs as I wish to maintain integrity and a clear trail of data processing steps. Also, having the original data to fall back on if need be, without re-downloading could be helpful.\n\n\nSetup Database Pipeline\n\n\nDoes the extraction and initial transformation following download of compressed files.\ndef setup_duckdb_pipeline(\n  csvDict: dict, \n  parquetDict: dict, \n  conn: duckdb.DuckDBPyConnection) -&gt; None:\n  try:\n    {\n      table_name: duckdb.sql(f\"\"\"\n      CREATE OR REPLACE TABLE {table_name} AS \n      SELECT \n        *\n      FROM \n        df;\n      \"\"\", connection = conn) \n      for table_name, df in csvDict.items()\n      }\n    for key, value in csvDict.items():\n      result = conn.execute(f\"SELECT COUNT(*) FROM {key}\").fetchone()\n      print(f\"Successfully created table: {key}, Row count: {result[0]}\")\n  except Exception as e:\n    print(f\"Error loading table: {str(e)}\")\n    raise\n  \n  if parquetDict:\n    write_datasets(conn, parquetDict)\n  \n# Create tables from Parquet files\ndef write_datasets (\n  conn: duckdb.DuckDBPyConnection, parquetDict: dict):\n      try:\n        {\n          table_name: duckdb.sql(f\"\"\"\n           CREATE OR REPLACE TABLE {table_name} AS\n           SELECT \n             id::id_actigraphy_enum AS id\n             ,quarter::TEXT::quarter_enum AS quarter\n             ,weekday::TEXT::weekday_enum AS weekday\n             ,light\n             ,time_of_day\n             ,relative_date_PCIAT\n           FROM read_parquet(\n             '{file_path}'\n             ,hive_partitioning = true\n             );\"\"\", connection=conn)\n           for table_name, file_path in parquetDict.items()\n           }\n        for key, value in parquetDict.items():\n          result = conn.execute(f\"SELECT COUNT(*) FROM {key}\").fetchone()\n          print(\n            f\"Successfully created table: {key}, Row count, {result[0]}\")\n      except Exception as e:\n        print(f\"Error writing dataset: {str(e)}\")\n        raise\n\ntrainCsvDf = pd.read_csv(\"data/train.csv\")\ntestCsvDf = pd.read_csv(\"data/test.csv\")\n\ndictDf = pd.read_csv(\"data/data_dictionary.csv\")\n\ntrainCsvDf.columns = trainCsvDf.columns.str.replace('-','_') \ntrainCsvDf.columns = trainCsvDf.columns.str.lower() \ntestCsvDf.columns = testCsvDf.columns.str.replace('-','_') \ntestCsvDf.columns = testCsvDf.columns.str.lower() \n\ndictDf.Field = dictDf.Field.replace(\"-\", \"_\", regex = True)\n\ncsvDict = {\n  \"TrainCsv\": trainCsvDf\n  ,\"TestCsv\": testCsvDf\n  ,\"DataDict\": dictDf\n  }\n\nparquetDict = {\n  \"ActigraphyTrain\": 'data//series_train.parquet*/*/*'\n  ,\"ActigraphyTest\": 'data//series_test*/*/*'\n  }\n\ntry:\n  setup_duckdb_pipeline(csvDict, parquetDict, conn)\n  conn.sql(f\"CHECKPOINT;\")\nexcept:\n  print(f\"Could not set up data pipeline.\")\n\n\nSuccessfully created table: TrainCsv, Row count: 3960\nSuccessfully created table: TestCsv, Row count: 20\nSuccessfully created table: DataDict, Row count: 81\nSuccessfully created table: ActigraphyTrain, Row count, 314569149\nSuccessfully created table: ActigraphyTest, Row count, 439726\n┌─────────┐\n│ Success │\n│ boolean │\n├─────────┤\n│ 0 rows  │\n└─────────┘\n\n\n\n\nFeature Tables\n\n\nCode\nimport re\n\ndef filter_columns_by_regex(col_dict: dict, regex_pattern: str) -&gt; dict:\n  \"\"\"\n  :::WHY(s):::\n    Can there be a way to use dictionaries to specify columns?\n \n  :::GOAL(s):::\n    Regex selection of column names from a dictionary.\n  \"\"\"\n  return {\n    col: dtype \n    for col, dtype in col_dict.items() \n    if re.search(regex_pattern, col)\n    }\n\n\ndef create_table_with_regex_columns(\n  conn: duckdb.duckdb\n  ,source_table: str\n  ,new_table_name: str \n  ,regex_pattern: str \n  ,col_dict: dict\n  ) -&gt; None:  \n  \n  \"\"\"\n  :::WHY(s):::\n    There should to be a more streamlined way to utilize columnar information following regular string patterns to generate tables based on such information. This could follow and carry out pre-existing data modeling plans.\n  \n  :::GOAL(s):::\n    To create new database tables with modularized SQL queries generated with the help of regex pattern matching selection.\n  \n  :::EXAMPLE(s):::\n    The dictionary item:\n      \"Demographic\": r\"^id|^sii|^basic\\S+\"\n  \n    Would generate a table named 'Demographic' from data with columns for 'id', 'sii' and all columns starting with 'basic' strings. \n  \"\"\"\n\n  # filter columns\n  filtered_col_dict = filter_columns_by_regex(col_dict, regex_pattern)\n  \n  # regex column selecting via dict comprehension and vectorized filtering\n  regex_select_sql = f\"\"\"\n  CREATE OR REPLACE TABLE {new_table_name} AS \n  SELECT\n    {', '.join([f'\"{col}\"' for col in filtered_col_dict.keys()])}\n  FROM {source_table};\n  \"\"\"\n\n  conn.execute(regex_select_sql)\n \n# It'd be useful to get the data types for creating a new table of values\ncoltype_overview = conn.execute(f\"\"\"\n  SELECT \n    column_name\n    ,data_type\n  FROM \n    information_schema.columns\n  WHERE \n    table_name = 'TrainCsv';\"\"\").df()\n\n# Map the column names with data types\ncol_dict = dict(zip(coltype_overview.column_name, coltype_overview.data_type))\n\nregex_dict_train = {\n  \"Demographic\": r\"^id|^sii|^basic\\S+\"\n  ,\"Physical\": r\"^id|^sii|^physical\\S+\"\n  ,\"FgVital\": r\"^id|^sii|^fitness_E\\S+\"\n  ,\"FgChild\": r\"^id|^sii|^fgc\\S+\"\n  ,\"Bia\": r\"^id|^sii|^bia\\S+\"\n  ,\"Paqa\": r\"^id|^sii|^paq_a\\S+\"\n  ,\"Pciat\": r\"^id|^sii|^pciat\\S+\"\n  ,\"Sds\": r\"^id|^sii|^sds\\S+\"\n  ,\"InternetUse\": r\"^id|^sii|^preint\\S+\"\n  }\n\n# There's no Pciat in the test set\nregex_dict_test = {\n  \"Demographic_OfTest\": r\"^id|^basic\\S+\"\n  ,\"Physical_OfTest\": r\"^id|^physical\\S+\"\n  ,\"FgVital_OfTest\": r\"^id|^fitness_E\\S+\"\n  ,\"FgChild_OfTest\": r\"^id|^fgc\\S+\"\n  ,\"Bia_OfTest\": r\"^id|^bia\\S+\"\n  ,\"Paqa_OfTest\": r\"^id|^paq_a\\S+\"\n # ,\"Pciat_OfTest\": r\"^id|^pciat\\S+\" \n  ,\"Sds_OfTest\": r\"^id|^sds\\S+\"\n  ,\"InternetUse_OfTest\": r\"^id|^preint\\S+\"\n  }\n \n\n# Loop through the data structures to create tables for the train set\nfor new_table_name, regex_pattern in regex_dict_train.items():\n  create_table_with_regex_columns(\n    conn \n    ,'TrainCsv'\n    ,new_table_name\n    ,regex_pattern\n    ,col_dict\n    ) \n\n# Loop through the data structures to create tables for the test set\nfor new_table_name, regex_pattern in regex_dict_test.items():\n  create_table_with_regex_columns(\n    conn\n    ,'TestCsv'\n    ,new_table_name\n    ,regex_pattern \n    ,col_dict\n    )\n\n\n\n\nCode\nconn.sql(f\"SELECT * from ActigraphyTest LIMIT 10;\").df()\n\n\n         id quarter weekday      light     time_of_day  relative_date_PCIAT\n0  00115b9f       3       4  53.000000  56940000000000                 41.0\n1  00115b9f       3       4  51.666668  56945000000000                 41.0\n2  00115b9f       3       4  50.333332  56950000000000                 41.0\n3  00115b9f       3       4  50.500000  56955000000000                 41.0\n4  00115b9f       3       4  33.166668  57235000000000                 41.0\n5  00115b9f       3       4  31.333334  57240000000000                 41.0\n6  00115b9f       3       4  29.500000  57245000000000                 41.0\n7  00115b9f       3       4  27.666666  57250000000000                 41.0\n8  00115b9f       3       4  25.833334  57255000000000                 41.0\n9  00115b9f       3       4  24.000000  57260000000000                 41.0\n\n\n\n\nCode\n# 3.6e+12\nconn.sql(f\"\"\"\nCREATE OR REPLACE TABLE ActigraphyTest AS\nSELECT\n  id\n  ,light\n  ,weekday\n  ,quarter\n  ,(time_of_day / 3_600_000_000_000) AS hour_of_day\nFROM \n  ActigraphyTest;\n\"\"\")\n\n\n\n\nCode\n# 3.6e+12\nconn.sql(f\"\"\"\nCREATE OR REPLACE TABLE ActigraphyTrain AS\nSELECT\n id\n ,light\n ,weekday\n ,quarter\n ,(time_of_day / 3_600_000_000_000) AS hour_of_day\nFROM \n  ActigraphyTrain;\n\"\"\")\n\nconn.sql(f\"CHECKPOINT;\")\n\n\n┌─────────┐\n│ Success │\n│ boolean │\n├─────────┤\n│ 0 rows  │\n└─────────┘\n\n\n\n\nCode\nconn.sql(f\"SELECT * FROM ActigraphyTrain LIMIT 10;\").df()\n\n\n         id      light weekday quarter  hour_of_day\n0  00115b9f  53.000000       4       3    15.816667\n1  00115b9f  51.666668       4       3    15.818056\n2  00115b9f  50.333332       4       3    15.819444\n3  00115b9f  50.500000       4       3    15.820833\n4  00115b9f  33.166668       4       3    15.898611\n5  00115b9f  31.333334       4       3    15.900000\n6  00115b9f  29.500000       4       3    15.901389\n7  00115b9f  27.666666       4       3    15.902778\n8  00115b9f  25.833334       4       3    15.904167\n9  00115b9f  24.000000       4       3    15.905556\n\n\n\n\nCode\nconn.sql(f\"\"\"\nCREATE OR REPLACE TABLE IntermediateActigraphy AS\nSELECT\n  id\n  ,basic_demos_enroll_season::TEXT::enroll_season_enum AS enroll_season\n  ,basic_demos_age::TEXT::age_enum AS age\n  ,basic_demos_sex::TEXT AS sex\n  ,sii::INTEGER::TEXT::sii_enum AS sii\nFROM \n  Demographic\nORDER BY\n  id ASC;\n\"\"\")\n\nconn.sql(f\"CHECKPOINT;\")\n\n\n┌─────────┐\n│ Success │\n│ boolean │\n├─────────┤\n│ 0 rows  │\n└─────────┘\n\n\n\n\nCode\nconn.sql(f\"\"\"\nCREATE OR REPLACE TABLE \n  ActigraphyTrain\nAS\n  SELECT\n    ia.*\n    ,at.hour_of_day\n    ,at.light\n  FROM \n    ActigraphyTrain at \n  LEFT JOIN \n    IntermediateActigraphy ia\n  ON \n    ia.id = at.id;\n\"\"\")\n\nconn.sql(f\"CHECKPOINT;\")\n\n\n┌─────────┐\n│ Success │\n│ boolean │\n├─────────┤\n│ 0 rows  │\n└─────────┘\n\n\n\n\nExtract quartile summary values into a dictionary with intuitive key labels.\ndef quartiler (\n  conn: duckdb.duckdb, \n  col_name: str, \n  source_name: str) -&gt; dict:\n  \"\"\"\n  ::WHY(s):: \n    SQL can sometimes require a lot of code for repetitive commands, but in a Python environment, database queries can be modularized.\n  \n  ::GOAL(s)::  \n    To process SQL queries into useful quartile information represented by intuitive key labels.\n  \"\"\"\n\n  summaryDf = conn.sql(f\"\"\"\n  SUMMARIZE\n  SELECT\n    {col_name}\n  FROM \n    {source_name};\"\"\").df()\n\n  quartileDict = {\n    'min': summaryDf['min'][0]\n    ,'Q1': summaryDf.q25[0]\n    ,'Q2': summaryDf.q50[0]\n    ,'Q3': summaryDf.q75[0]\n    ,'max': summaryDf['max'][0]\n    }\n  \n  return quartileDict\n\n\n\n\nData Model\n\n\n\n\n\n\n\n---\nconfig:\n  theme: dark\n---\nerDiagram\n    PARTICIPANT ||--o{ DEMOGRAPHICS : has\n    PARTICIPANT ||--o{ PHYSICAL_MEASURES : has\n    PARTICIPANT ||--o{ FITNESSGRAM : has\n    PARTICIPANT ||--o{ BIA : has\n    PARTICIPANT ||--o{ BEHAVIORAL_MEASURES : has\n    PARTICIPANT {\n        string id\n    }\n    DEMOGRAPHICS {\n        string season\n        float age\n        int sex\n    }\n    PHYSICAL_MEASURES {\n        string season\n        float bmi\n        float height\n        float weight\n        int waist_circumference\n        int blood_pressure\n        int heart_rate\n    }\n    FITNESSGRAM {\n        string season\n        int curl_ups\n        float grip_strength\n        int push_ups\n        float sit_reach\n        int trunk_lift\n        categorical fitness_zones\n    }\n    BIA {\n        string season\n        float body_fat\n        float muscle_mass\n        float bone_mass\n        categorical activity_level\n        categorical frame_size\n    }\n    BEHAVIORAL_MEASURES {\n        string season\n        float activity_score\n        int internet_usage\n        int sleep_score\n        int internet_addiction_score\n    }\n\n\n\n\nFigure 1: child"
  },
  {
    "objectID": "index.html#actigraphy-physical-and-demographics",
    "href": "index.html#actigraphy-physical-and-demographics",
    "title": "Problematic Internet Habits in Children",
    "section": "Actigraphy, Physical, and Demographics",
    "text": "Actigraphy, Physical, and Demographics\ncool (Zierle-Ghosh & Jan, 2024).\n\nZierle-Ghosh, A., & Jan, A. (2024). Physiology, body mass index. In StatPearls. StatPearls Publishing. http://www.ncbi.nlm.nih.gov/books/NBK535456/\n\n\nCode\ndef intermediateLighter(conn: duckdb.duckdb, new_tables: list, quarters: dict, quartuples: pd.Series) -&gt; None:\n  \"\"\"\n  :::WHY(s):::\n    Tables based on the same parameters but different parameter values could be modularized.\n    \n  :::GOAL(s):::\n    Process repetitive SQL efficiently and intuitively using data structures that simplify the process.\n  \"\"\"\n  for i in list(range(4)):\n    conn.sql(f\"\"\"\n      CREATE OR REPLACE TABLE '{new_tables[i]}' AS\n      SELECT\n        id\n        ,enroll_season\n        ,age\n        ,sex\n        ,sii\n        ,AVG(light) AS '{quartuples.index[i]}'\n      FROM \n        ActigraphyTrain\n      WHERE \n        hour_of_day BETWEEN \n            '{quarters[quartuples.iloc[i][0]]}'::DOUBLE \n          AND \n            '{quarters[quartuples.iloc[i][1]]}'::DOUBLE\n      GROUP BY \n        ALL\n      ORDER BY \n        id ASC;\"\"\")\n        \n# execute quartiler()\nquarters = quartiler(conn, 'hour_of_day', 'ActigraphyTrain')\n\nquartuples = pd.Series(\n  data = \n  [('min','Q1')\n  ,('Q1', 'Q2')\n  ,('Q2', 'Q3')\n  ,('Q3' ,'max')]\n  ,index =\n  ['min_q1'\n  ,'q1_q2'\n  ,'q2_q3'\n  ,'q3_max'])\n\nnew_tables = ['Light1', 'Light2', 'Light3', 'Light4']\n\nintermediateLighter(conn, new_tables, quarters, quartuples)\n\nconn.sql(f\"CHECKPOINT;\")\n\n\n┌─────────┐\n│ Success │\n│ boolean │\n├─────────┤\n│ 0 rows  │\n└─────────┘\n\n\n\n\nCode\nconn.sql(f\"\"\"\nCREATE OR REPLACE TABLE AggregatedAnalysis AS\nSELECT \n  l1.*\n  ,l2.q1_q2\n  ,l3.q2_q3\n  ,l4.q3_max\nFROM \n  Light1 l1\nLEFT JOIN Light2 l2 ON l1.id = l2.id\nLEFT JOIN Light3 l3 ON l1.id = l3.id\nLEFT JOIN Light4 l4 ON l1.id = l4.id;\n\"\"\")\n\nconn.sql(f\"CHECKPOINT;\")\n\n\n┌─────────┐\n│ Success │\n│ boolean │\n├─────────┤\n│ 0 rows  │\n└─────────┘\n\n\n\n\nCode\nconn.sql(f\"SELECT * FROM AggregatedAnalysis LIMIT 20;\").df()\n\n\n          id enroll_season age  ...       q1_q2       q2_q3     q3_max\n0   00115b9f        Winter   9  ...   61.588560   52.833906  17.016357\n1   001f3379        Spring  13  ...   38.884232   27.373008   3.440905\n2   00f332d1        Winter  14  ...   70.673193  172.210090  20.110854\n3   012e3869        Summer   6  ...   21.508461   14.488170  11.505682\n4   029a19c9        Winter  17  ...   58.929479   35.612819   7.528811\n5   02cebf33        Spring  12  ...   21.778611   44.673833  52.732079\n6   02cf7384        Summer  16  ...   21.477920   24.100715   7.125696\n7   035c96dd        Summer  13  ...   16.632041   19.237285  14.558526\n8   0417c91e        Spring   6  ...  132.202018  197.809210  17.329693\n9   04afb6f9        Summer  17  ...   19.531380   19.777250  10.798602\n10  04bb1a76        Winter   9  ...    6.873207    7.229969   6.914688\n11  04d06a9c          Fall  10  ...   21.100172   29.815348  15.427587\n12  051680a0        Winter   7  ...  132.732977  244.673424  42.979938\n13  055156e2        Summer  12  ...   31.630828   27.566469  12.964960\n14  05db1b9b        Spring   8  ...   67.778410   60.621720   4.874964\n15  05e94f88        Summer  10  ...    4.367184    2.981214   2.505070\n16  063b16fc        Spring  10  ...  119.143395  121.558640  12.892419\n17  064e8da5        Spring   5  ...   65.005358   59.709542  12.156673\n18  0668373f        Winter   9  ...  117.977459  149.462894   5.077692\n19  067b9287        Spring   7  ...   13.762301   24.314711   9.307871\n\n[20 rows x 9 columns]\n\n\n\n\nCode\nconn.sql(f\"\"\"\nCREATE OR REPLACE TABLE AggregatedAnalysis AS\nSELECT\n  aa.*\n  ,preint_eduhx_computerinternet_hoursday AS useHrs \nFROM\n  AggregatedAnalysis aa\nLEFT JOIN \n  InternetUse iu\nON\n  aa.id = iu.id;\n\"\"\")\n\n\n\n\nCode\nall_aa = conn.sql(f\"SELECT * FROM AggregatedAnalysis;\").df()\nall_aa['useHrs'].describe()\n\n\ncount    981.000000\nmean       1.018349\nstd        1.092493\nmin        0.000000\n25%        0.000000\n50%        1.000000\n75%        2.000000\nmax        3.000000\nName: useHrs, dtype: float64\n\n\n\n\nCode\nconn.sql(f\"\"\"\nSUMMARIZE\nSELECT useHrs\nFROM AggregatedAnalysis;\n\"\"\").df()\n\n\n  column_name column_type  min  ...  q75  count null_percentage\n0      useHrs      DOUBLE  0.0  ...  2.0    992            1.11\n\n[1 rows x 12 columns]\n\n\n\n\nCode\nriskyDictionary = {\n  'Risk1': \n    (f\",CASE WHEN ph.physical_bmi &lt; 18.5 THEN 'Underweight'\"\n    ,f\"WHEN ph.physical_bmi BETWEEN 18.5 AND 24.9 THEN 'Normal'\"\n    ,f\"WHEN ph.physical_bmi BETWEEN 25.0 AND 29.9 THEN 'Increased'\"\n    ,f\"WHEN ph.physical_bmi BETWEEN 30.0 AND 34.9 THEN 'High'\"\n    ,f\"WHEN ph.physical_bmi BETWEEN 35.0 AND 39.9 THEN 'Very High'\"\n    ,f\"WHEN ph.physical_bmi &gt;= 40 THEN 'Extremely High'\"\n    ,f\"ph.physical_waist_circumference &lt;= 35 AND aa.sex = '0'\"),\n  'Risk2':\n    (f\",CASE WHEN ph.physical_bmi &lt; 18.5 THEN 'Underweight'\"\n    ,f\"WHEN ph.physical_bmi BETWEEN 18.5 AND 24.9 THEN 'Normal'\"\n    ,f\"WHEN ph.physical_bmi BETWEEN 25.0 AND 29.9 THEN 'High'\"\n    ,f\"WHEN ph.physical_bmi BETWEEN 30.0 AND 34.9 THEN 'Very High'\"\n    ,f\"WHEN ph.physical_bmi BETWEEN 35.0 AND 39.9 THEN 'Very High'\"\n    ,f\"WHEN ph.physical_bmi &gt;= 40 THEN 'Extremely High'\"\n    ,f\"ph.physical_waist_circumference &gt; 35 AND aa.sex = '0'\"),\n  'Risk3':\n    (f\",CASE WHEN ph.physical_bmi &lt; 18.5 THEN 'Underweight'\"\n    ,f\"WHEN ph.physical_bmi BETWEEN 18.5 AND 24.9 THEN 'Normal'\"\n    ,f\"WHEN ph.physical_bmi BETWEEN 25.0 AND 29.9 THEN 'Increased'\"\n    ,f\"WHEN ph.physical_bmi BETWEEN 30.0 AND 34.9 THEN 'High'\"\n    ,f\"WHEN ph.physical_bmi BETWEEN 35.0 AND 39.9 THEN 'Very High'\"\n    ,f\"WHEN ph.physical_bmi &gt;= 40 THEN 'Extremely High'\"\n    ,f\"ph.physical_waist_circumference &lt;= 40 AND aa.sex = '1'\"),\n  'Risk4':\n    (f\",CASE WHEN ph.physical_bmi &lt; 18.5 THEN 'Underweight'\"\n    ,f\"WHEN ph.physical_bmi BETWEEN 18.5 AND 24.9 THEN 'Normal'\"\n    ,f\"WHEN ph.physical_bmi BETWEEN 25.0 AND 29.9 THEN 'High'\"\n    ,f\"WHEN ph.physical_bmi BETWEEN 30.0 AND 34.9 THEN 'Very High'\"\n    ,f\"WHEN ph.physical_bmi BETWEEN 35.0 AND 39.9 THEN 'Very High'\"\n    ,f\"WHEN ph.physical_bmi &gt;= 40 THEN 'Extremely High'\"\n    ,f\"ph.physical_waist_circumference &gt; 40 AND aa.sex = '1'\")}\n\n\n\n\nCode\nriskyDf = pd.DataFrame(data = riskyDictionary)\n\n\n\n\nCode\nfor key, value in riskyDf.items():\n  try:\n    conn.sql(f\"\"\"\n    CREATE OR REPLACE TABLE {key} AS\n    SELECT\n      aa.*\n      {value[0]}\n      {value[1]}\n      {value[2]}\n      {value[3]}\n      {value[4]}\n      {value[5]}\n      ELSE NULL\n      END AS risk_cat\n    ,risk_cat::disease_risk_enum AS risk_category\n    FROM \n      Physical ph \n    LEFT JOIN \n      AggregatedAnalysis aa \n    ON \n      aa.id = ph.id\n    WHERE \n      {value[6]}\n    ORDER BY \n      aa.id ASC;\"\"\")\n    result = conn.execute(f\"SELECT COUNT(*) FROM {key}\").fetchone()\n    print(f\"Successfully created table: {key}, Row count: {result[0]}\")\n  except:\n    print(f\"Error loading this table: {key}\")\n\n\nSuccessfully created table: Risk1, Row count: 24\nSuccessfully created table: Risk2, Row count: 1\nSuccessfully created table: Risk3, Row count: 20\nSuccessfully created table: Risk4, Row count: 2\n\n\nCode\n    \nconn.sql(f\"CHECKPOINT\")\n\n\n┌─────────┐\n│ Success │\n│ boolean │\n├─────────┤\n│ 0 rows  │\n└─────────┘\n\n\n\n\nCode\nconn.sql(f\"\"\"\nCREATE OR REPLACE TABLE \n  DiseaseRiskDemographic AS\nSELECT * EXCLUDE(risk_cat) FROM Risk1\nUNION BY NAME\nSELECT * EXCLUDE(risk_cat) FROM Risk2\nUNION BY NAME\nSELECT * EXCLUDE(risk_cat) FROM Risk3\nUNION BY NAME \nSELECT * EXCLUDE(risk_cat) FROM Risk4;\n\"\"\")\n\n\n\n\nCode\nconn.sql(f\"\"\"\nCREATE OR REPLACE TABLE RiskCategorySummary AS\nSELECT \n  risk_category\n  ,AVG(sii::INTEGER) AS sii\n  ,AVG(useHrs) AS useHrs\n  ,AVG(min_q1) AS q1_avg_light_exposure\n  ,AVG(q1_q2) AS q2_avg_light_exposure\n  ,AVG(q2_q3) AS q3_avg_light_exposure\n  ,AVG(q3_max) AS q4_avg_light_exposure\nFROM \n  DiseaseRiskDemographic\nGROUP BY \n  risk_category;\n\"\"\")\n\n\n\n\nCode\nconn.sql(f\"\"\"\nSELECT * \nFROM RiskCategorySummary;\n\"\"\").df()\n\n\n    risk_category       sii  ...  q3_avg_light_exposure  q4_avg_light_exposure\n0     Underweight  0.242424  ...             115.448457              13.224182\n1          Normal  0.500000  ...              90.449547              30.382192\n2       Increased  0.000000  ...              81.211704               8.071494\n3       Very High  0.500000  ...              65.659110              19.607551\n4  Extremely High  0.000000  ...             117.824139              24.963036\n\n[5 rows x 7 columns]\n\n\n\n\nCode\npydf = conn.execute(f\"\"\"\nSELECT \n  risk_category\n  ,AVG(min_q1) AS qLight\nFROM \n  DiseaseRiskDemographic\nGROUP BY \n  risk_category;\"\"\").df()\n\n\n\n\nCode\nr_df &lt;- py$pydf\n\n\n\n\nCode\nnotes_list =  list(\"The disease risk category.\", \"The average light exposure for Q1.\")\nlocations_list = list(\"risk_category\", \"qLight\")\n\n# Create a table dataframe, or tibble, of footnotes\nfootnotes_df &lt;- tibble(notes = notes_list, locations = locations_list)\n\nrTable &lt;- r_table_theming(rDataFrame = r_df,\n                          title = \"Title\",\n                          subtitle = NULL,\n                          footnotes_df,\n                          source_note = md(\"**source**: Kaggle\"))\n\nrTable\n\n\n\n\nTable 1\n\n\n\n\n\n\n  \n    \n      Title\n    \n    \n    \n      risk_category1\n      qLight2\n    \n  \n  \n    Underweight\n13.330165\n    Normal\n35.113059\n    Increased\n6.729335\n    Very High\n23.488205\n    Extremely High\n20.271764\n  \n  \n    \n      source: Kaggle\n    \n  \n  \n    \n      1 The disease risk category.\n    \n    \n      2 The average light exposure for Q1.\n    \n  \n\n\n\n\n\n\n\n\n\nCode\nr_plot &lt;- ggplot(data = r_df, \n                 mapping = aes(x = risk_category, y = qLight)\n                 ) +\n  geom_col(fill = 'skyblue') +\n  labs(title = 'Risk Categories', \n       x = 'Category', y = 'Light Exposure') +\n  coord_flip()\n\nr_plot + ggplot_theming_flipped_axes()"
  },
  {
    "objectID": "index.html#close-connection",
    "href": "index.html#close-connection",
    "title": "Problematic Internet Habits in Children",
    "section": "Close Connection",
    "text": "Close Connection\n\n\nCode\ntry:\n  conn.close()\n  print(f\"database connection closed\")\nexcept:\n  print(f\"could not close the connection\")\n\n\ndatabase connection closed"
  },
  {
    "objectID": "notebooks/data-modeling.html",
    "href": "notebooks/data-modeling.html",
    "title": "",
    "section": "",
    "text": "---\nconfig:\n  theme: dark\n---\nerDiagram\n    PARTICIPANT ||--o{ DEMOGRAPHICS : has\n    PARTICIPANT ||--o{ PHYSICAL_MEASURES : has\n    PARTICIPANT ||--o{ FITNESSGRAM : has\n    PARTICIPANT ||--o{ BIA : has\n    PARTICIPANT ||--o{ BEHAVIORAL_MEASURES : has\n    PARTICIPANT {\n        string id\n    }\n    DEMOGRAPHICS {\n        string season\n        float age\n        int sex\n    }\n    PHYSICAL_MEASURES {\n        string season\n        float bmi\n        float height\n        float weight\n        int waist_circumference\n        int blood_pressure\n        int heart_rate\n    }\n    FITNESSGRAM {\n        string season\n        int curl_ups\n        float grip_strength\n        int push_ups\n        float sit_reach\n        int trunk_lift\n        categorical fitness_zones\n    }\n    BIA {\n        string season\n        float body_fat\n        float muscle_mass\n        float bone_mass\n        categorical activity_level\n        categorical frame_size\n    }\n    BEHAVIORAL_MEASURES {\n        string season\n        float activity_score\n        int internet_usage\n        int sleep_score\n        int internet_addiction_score\n    }\n\n\n\n\nFigure 1: child1"
  }
]