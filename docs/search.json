[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Problematic Internet Habits in Children",
    "section": "",
    "text": "The grounds for analyses in this case is the problematic behavior as it pertains to the subjects’ experience and their parents’ via questionnaires, physical fitness tests, worn instrument measurements and demographics. The resulting data is to be processed and connections drawn. (Santorelli et al., 2024)\n\nSantorelli, A., Zuanazzi, A., Leyden, M., Lawler, L., Devkin, M., Kotani, Y., & Kiar, G. (2024). Child mind institute — problematic internet use. Kaggle. https://kaggle.com/competitions/child-mind-institute-problematic-internet-use\n\n\n\n\nA notable and comprehsive R library was used to give the tables and plots some styling. (Hvitfeldt, 2021)\n\nHvitfeldt, E. (2021). Paletteer: Comprehensive collection of color palettes. https://github.com/EmilHvitfeldt/paletteer\n\n\n\nThis is a means to apply formatting to every column in a table using a desired paletteer palette if desired using the gt tables. Table output were customized to a high degree. Principles stemming from functional programming, in particular, are implemented here as they seem to be most appropriate to achieve the desired effect. (Iannone et al., 2025)\n\nIannone, R., Cheng, J., Schloerke, B., Hughes, E., Lauer, A., Seo, J., Brevoort, K., & Roy, O. (2025). Gt: Easily create presentation-ready display tables. https://gt.rstudio.com\n\n\n\nHard-core table formatting code.\nlibrary(gt)\nlibrary(purrr)\nlibrary(tibble)\nlibrary(reticulate)\nlibrary(paletteer)\n\neval_palette &lt;- function(pal_name, n = 10, pal_type, direction = NULL) {\n  if (pal_type == \"c\") {\n    return(paletteer_c(pal_name, n, direction))\n    } else if (pal_type == \"d\") {\n      return(paletteer_d(pal_name, n, direction))\n    } else if (pal_type == \"dynamic\") {\n      return(paletteer_dynamic(pal_name, n, direction))\n    }\n  }\n\nr_table_theming &lt;- function(r_df,\n                            title,\n                            subtitle,\n                            footnotes_df,\n                            source_note,\n                            pal_df,\n                            multiline_feet = NULL,\n                            tbl_font_size = NULL,\n                            color_by_columns = NULL,\n                            do_col_labels = FALSE,\n                            target_everything = FALSE\n                            ) {\n  \n  r_table &lt;- gt(r_df)\n\n  if(nrow(r_df) &gt; 1 && target_everything == FALSE) {\n    r_table &lt;- seq_len(nrow(pal_df)) |&gt;\n    reduce(\\(acc, i) {\n      data_color(\n        acc,\n        palette = pal_df$pals[[i]],\n        columns = pal_df$cols[[i]]\n        )\n    },.init = r_table)\n  } \n  else if(nrow(r_df) &gt; 1 && target_everything == TRUE) {\n    r_table &lt;- seq_len(nrow(pal_df)) |&gt;\n    reduce(\\(acc, i) {\n      data_color(\n        acc,\n        columns = color_by_columns,\n        palette = pal_df$pals[[i]],\n        target_columns = everything()\n        )\n    },.init = r_table)\n  }\n  \n  r_table &lt;- r_table |&gt;\n    tab_header(title = title, subtitle = subtitle)\n  \n  r_table &lt;- r_table |&gt;\n    tab_options(\n      column_labels.padding = px(10),\n      column_labels.font.weight = \"bold\",\n      column_labels.background.color = '#333',\n      #column_labels.font.size = pct(115),\n      column_labels.border.top.width = px(0),\n      column_labels.border.bottom.color = 'black',\n      column_labels.vlines.width = px(1),\n      column_labels.border.lr.width = px(1),\n      column_labels.border.bottom.width = px(0),\n      column_labels.border.lr.color = 'black',\n      column_labels.vlines.color = 'black',\n      #container.width = pct(99),\n      footnotes.padding = px(5),\n      footnotes.background.color = '#222',\n      footnotes.sep = \", \",\n      footnotes.multiline = multiline_feet,\n      heading.padding = px(20),\n      heading.background.color = '#222',\n      heading.title.font.size = pct(125),\n      heading.subtitle.font.size = pct(110),\n      heading.border.bottom.width = px(0),\n      row.striping.include_table_body   = TRUE,\n      row.striping.background_color = '#333',\n      source_notes.background.color = '#222',\n      table.margin.left = px(1),\n      table.margin.right = px(1),\n      table.align = \"center\",\n      table.border.top.width = px(0),\n      table.border.bottom.width = px(0),\n      table.background.color = '#222',\n      table.font.size = tbl_font_size,\n      table.layout = \"auto\",\n      table_body.hlines.color = 'black',\n      table_body.hlines.width = px(0),\n      table_body.vlines.width = px(0),\n      table_body.border.bottom.color = 'black',\n      table_body.border.top.color = 'black',\n      table_body.border.bottom.width = px(0),\n      table_body.border.top.width = px(0)\n     # quarto.use_bootstrap = TRUE\n      )\n  \n  r_table &lt;- r_table |&gt;\n    tab_source_note(source_note = source_note)\n  \n  # Footnotes are added to the accumulator, building up to the final result\n  r_table &lt;- seq_len(nrow(footnotes_df)) |&gt;\n    reduce(\\(acc, i) {\n      tab_footnote(\n        acc,\n        footnote = footnotes_df$notes[[i]],\n        location = cells_column_labels(columns = footnotes_df$locations[[i]]),\n        placement = \"auto\"\n        )\n      }, .init = r_table)\n  \n  if(ncol(r_df) &gt; 1 && do_col_labels == TRUE) {\n      cell_col_fills = pal_df$pals[[1]]\n      \n      r_table &lt;- seq_len(nrow(pal_df)) |&gt;\n        reduce(\\(acc, i) {\n          tab_style(\n            acc,\n            style = cell_fill(\n              color = cell_col_fills[i]\n              ), locations = cells_column_labels(columns = pal_df$cols[[i]]))\n        }, .init = r_table)\n  }\n  \n  return(r_table)\n  }\n\n\n\n\n\n\n\nHere are some relatively simpler functions developed with the intention to append and apply consistent theming to ggplots. R libraries are currently preferred for plotting outputs as they can be extensively customized. (Wickham, 2016)\n\nWickham, H. (2016). ggplot2: Elegant graphics for data analysis. Springer-Verlag New York. https://ggplot2.tidyverse.org\n\n\n\n\nSoft-core plot theme code.\nlibrary(reticulate)\nlibrary(ggplot2)\n\n# NORMAL AXES\nggplot_theming &lt;- function() {\n  theme_minimal() +\n    theme(\n      plot.title = element_text(color = 'white',\n                                size = rel(1.5)),\n      plot.background = element_rect(fill = '#222'),\n      panel.background = element_rect(fill = '#222'),\n      panel.grid.major.x = element_line(linetype = 'solid',\n                                        color = 'black'),\n      panel.grid.minor.x = element_line(linetype = \"dotted\",\n                                        color = 'gray'),\n      panel.grid.major.y = element_line(linetype = 'dashed',\n                                        color = 'black'),\n      panel.grid.minor.y = element_line(linetype = 'dotted',\n                                        color = 'gray'),\n      axis.title = element_text(color = 'gray100'),\n      axis.text = element_text(color = 'gray'),\n      legend.background = element_rect(fill = '#222'),\n      legend.text = element_text(color = 'gray'),\n      legend.title = element_text(color = 'white'))\n  }\n\n# FLIPPED AXES\nggplot_theming_flipped_axes &lt;- function() {\n  theme_minimal() +\n    theme(\n     # borders(color = \"#222\"),\n      plot.title = element_text(color = 'white',\n                                size = rel(2)),\n      plot.background = element_rect(fill = '#222'),\n      panel.background = element_rect(fill = '#222'),\n      panel.grid.major.x = element_line(linetype = 'dashed'),\n      panel.grid.minor.x = element_line(linetype = \"dotted\"),\n      panel.grid.major.y = element_line(linetype = 'solid'),\n      panel.grid.minor.y = element_line(linetype = 'dotted'),\n      axis.title = element_text(color = 'gray100'),\n      axis.text = element_text(color = 'gray'),\n      legend.background = element_rect(fill = '#222'),\n      legend.text = element_text(color = 'gray'),\n      legend.title = element_text(color = 'white'))\n}"
  },
  {
    "objectID": "index.html#rationale-and-initial-thoughts",
    "href": "index.html#rationale-and-initial-thoughts",
    "title": "Problematic Internet Habits in Children",
    "section": "",
    "text": "The grounds for analyses in this case is the problematic behavior as it pertains to the subjects’ experience and their parents’ via questionnaires, physical fitness tests, worn instrument measurements and demographics. The resulting data is to be processed and connections drawn. (Santorelli et al., 2024)\n\nSantorelli, A., Zuanazzi, A., Leyden, M., Lawler, L., Devkin, M., Kotani, Y., & Kiar, G. (2024). Child mind institute — problematic internet use. Kaggle. https://kaggle.com/competitions/child-mind-institute-problematic-internet-use\n\n\n\n\nA notable and comprehsive R library was used to give the tables and plots some styling. (Hvitfeldt, 2021)\n\nHvitfeldt, E. (2021). Paletteer: Comprehensive collection of color palettes. https://github.com/EmilHvitfeldt/paletteer\n\n\n\nThis is a means to apply formatting to every column in a table using a desired paletteer palette if desired using the gt tables. Table output were customized to a high degree. Principles stemming from functional programming, in particular, are implemented here as they seem to be most appropriate to achieve the desired effect. (Iannone et al., 2025)\n\nIannone, R., Cheng, J., Schloerke, B., Hughes, E., Lauer, A., Seo, J., Brevoort, K., & Roy, O. (2025). Gt: Easily create presentation-ready display tables. https://gt.rstudio.com\n\n\n\nHard-core table formatting code.\nlibrary(gt)\nlibrary(purrr)\nlibrary(tibble)\nlibrary(reticulate)\nlibrary(paletteer)\n\neval_palette &lt;- function(pal_name, n = 10, pal_type, direction = NULL) {\n  if (pal_type == \"c\") {\n    return(paletteer_c(pal_name, n, direction))\n    } else if (pal_type == \"d\") {\n      return(paletteer_d(pal_name, n, direction))\n    } else if (pal_type == \"dynamic\") {\n      return(paletteer_dynamic(pal_name, n, direction))\n    }\n  }\n\nr_table_theming &lt;- function(r_df,\n                            title,\n                            subtitle,\n                            footnotes_df,\n                            source_note,\n                            pal_df,\n                            multiline_feet = NULL,\n                            tbl_font_size = NULL,\n                            color_by_columns = NULL,\n                            do_col_labels = FALSE,\n                            target_everything = FALSE\n                            ) {\n  \n  r_table &lt;- gt(r_df)\n\n  if(nrow(r_df) &gt; 1 && target_everything == FALSE) {\n    r_table &lt;- seq_len(nrow(pal_df)) |&gt;\n    reduce(\\(acc, i) {\n      data_color(\n        acc,\n        palette = pal_df$pals[[i]],\n        columns = pal_df$cols[[i]]\n        )\n    },.init = r_table)\n  } \n  else if(nrow(r_df) &gt; 1 && target_everything == TRUE) {\n    r_table &lt;- seq_len(nrow(pal_df)) |&gt;\n    reduce(\\(acc, i) {\n      data_color(\n        acc,\n        columns = color_by_columns,\n        palette = pal_df$pals[[i]],\n        target_columns = everything()\n        )\n    },.init = r_table)\n  }\n  \n  r_table &lt;- r_table |&gt;\n    tab_header(title = title, subtitle = subtitle)\n  \n  r_table &lt;- r_table |&gt;\n    tab_options(\n      column_labels.padding = px(10),\n      column_labels.font.weight = \"bold\",\n      column_labels.background.color = '#333',\n      #column_labels.font.size = pct(115),\n      column_labels.border.top.width = px(0),\n      column_labels.border.bottom.color = 'black',\n      column_labels.vlines.width = px(1),\n      column_labels.border.lr.width = px(1),\n      column_labels.border.bottom.width = px(0),\n      column_labels.border.lr.color = 'black',\n      column_labels.vlines.color = 'black',\n      #container.width = pct(99),\n      footnotes.padding = px(5),\n      footnotes.background.color = '#222',\n      footnotes.sep = \", \",\n      footnotes.multiline = multiline_feet,\n      heading.padding = px(20),\n      heading.background.color = '#222',\n      heading.title.font.size = pct(125),\n      heading.subtitle.font.size = pct(110),\n      heading.border.bottom.width = px(0),\n      row.striping.include_table_body   = TRUE,\n      row.striping.background_color = '#333',\n      source_notes.background.color = '#222',\n      table.margin.left = px(1),\n      table.margin.right = px(1),\n      table.align = \"center\",\n      table.border.top.width = px(0),\n      table.border.bottom.width = px(0),\n      table.background.color = '#222',\n      table.font.size = tbl_font_size,\n      table.layout = \"auto\",\n      table_body.hlines.color = 'black',\n      table_body.hlines.width = px(0),\n      table_body.vlines.width = px(0),\n      table_body.border.bottom.color = 'black',\n      table_body.border.top.color = 'black',\n      table_body.border.bottom.width = px(0),\n      table_body.border.top.width = px(0)\n     # quarto.use_bootstrap = TRUE\n      )\n  \n  r_table &lt;- r_table |&gt;\n    tab_source_note(source_note = source_note)\n  \n  # Footnotes are added to the accumulator, building up to the final result\n  r_table &lt;- seq_len(nrow(footnotes_df)) |&gt;\n    reduce(\\(acc, i) {\n      tab_footnote(\n        acc,\n        footnote = footnotes_df$notes[[i]],\n        location = cells_column_labels(columns = footnotes_df$locations[[i]]),\n        placement = \"auto\"\n        )\n      }, .init = r_table)\n  \n  if(ncol(r_df) &gt; 1 && do_col_labels == TRUE) {\n      cell_col_fills = pal_df$pals[[1]]\n      \n      r_table &lt;- seq_len(nrow(pal_df)) |&gt;\n        reduce(\\(acc, i) {\n          tab_style(\n            acc,\n            style = cell_fill(\n              color = cell_col_fills[i]\n              ), locations = cells_column_labels(columns = pal_df$cols[[i]]))\n        }, .init = r_table)\n  }\n  \n  return(r_table)\n  }\n\n\n\n\n\n\n\nHere are some relatively simpler functions developed with the intention to append and apply consistent theming to ggplots. R libraries are currently preferred for plotting outputs as they can be extensively customized. (Wickham, 2016)\n\nWickham, H. (2016). ggplot2: Elegant graphics for data analysis. Springer-Verlag New York. https://ggplot2.tidyverse.org\n\n\n\n\nSoft-core plot theme code.\nlibrary(reticulate)\nlibrary(ggplot2)\n\n# NORMAL AXES\nggplot_theming &lt;- function() {\n  theme_minimal() +\n    theme(\n      plot.title = element_text(color = 'white',\n                                size = rel(1.5)),\n      plot.background = element_rect(fill = '#222'),\n      panel.background = element_rect(fill = '#222'),\n      panel.grid.major.x = element_line(linetype = 'solid',\n                                        color = 'black'),\n      panel.grid.minor.x = element_line(linetype = \"dotted\",\n                                        color = 'gray'),\n      panel.grid.major.y = element_line(linetype = 'dashed',\n                                        color = 'black'),\n      panel.grid.minor.y = element_line(linetype = 'dotted',\n                                        color = 'gray'),\n      axis.title = element_text(color = 'gray100'),\n      axis.text = element_text(color = 'gray'),\n      legend.background = element_rect(fill = '#222'),\n      legend.text = element_text(color = 'gray'),\n      legend.title = element_text(color = 'white'))\n  }\n\n# FLIPPED AXES\nggplot_theming_flipped_axes &lt;- function() {\n  theme_minimal() +\n    theme(\n     # borders(color = \"#222\"),\n      plot.title = element_text(color = 'white',\n                                size = rel(2)),\n      plot.background = element_rect(fill = '#222'),\n      panel.background = element_rect(fill = '#222'),\n      panel.grid.major.x = element_line(linetype = 'dashed'),\n      panel.grid.minor.x = element_line(linetype = \"dotted\"),\n      panel.grid.major.y = element_line(linetype = 'solid'),\n      panel.grid.minor.y = element_line(linetype = 'dotted'),\n      axis.title = element_text(color = 'gray100'),\n      axis.text = element_text(color = 'gray'),\n      legend.background = element_rect(fill = '#222'),\n      legend.text = element_text(color = 'gray'),\n      legend.title = element_text(color = 'white'))\n}"
  },
  {
    "objectID": "index.html#importextract",
    "href": "index.html#importextract",
    "title": "Problematic Internet Habits in Children",
    "section": "2 Import/Extract",
    "text": "2 Import/Extract\nThe data was imported from Kaggle. It only needed to be done once, so the code chunk’s eval setting is set to false during most of this project’s development.\n\n\n\nExecuted once to import the data from the Kaggle server.\n!kaggle competitions download -c child-mind-institute-problematic-internet-use\n\n\n\nAlthough not necessary, creating the data directory can be made programmatic.\n\n\n\nChecked if the directory, ‘data’, exists.\nimport os\n\ntry:\n  os.mkdir('./data')\nexcept FileExistsError:\n  print(f\"data exists\")\n\n\ndata exists\n\n\n\n\nExracting the downloaded files was also done programmatically, but it is not necessary. It did seem useful to visualize the data directory following extraction to verify the files and folders that now exist in the ./data directory of this project.\n\n\n\n\nExtracted the compressed files and list files.\nfrom zipfile import ZipFile\nimport pandas as pd\n\nzip_path = \"child-mind-institute-problematic-internet-use.zip\"\n\nif not os.path.exists('./data/train.csv'):\n  with ZipFile(zip_path) as zf:\n    ZipFile.extractall(zf, path = './data')\n    print(\"Extracted files\", sep = \"\\n\"*2)\nelse:\n  print(\"Files already exist\", sep = \"\\n\"*2)\n\n\nExtracted files\n\n\nExtracted the compressed files and list files.\n\nextracts = pd.DataFrame(\n  data = os.listdir(path = './data'), columns = ['./data'])\n\n\n\n\n\n\ntable setup\nr_df &lt;- py$extracts\n\nnotes_list =  list(\"directory contents\")\nlocations_list = list(\"./data\")\n\n# Create a table dataframe, or tibble, of footnotes\nfootnotes_df &lt;- tibble(notes = notes_list, locations = locations_list)\n\npal_df &lt;- tibble(\n  cols = list(\n  \"./data\"),\n  pals = list(\n    eval_palette(\"ggsci::legacy_tron\", 7, 'd', 1)\n    )\n  )\n\nrTable &lt;- r_table_theming(r_df,\n                          title = \"Directory Files\",\n                          subtitle = NULL,\n                          footnotes_df,\n                          source_note = md(\"**source**: '/data/'\"),\n                          pal_df,\n                          tbl_font_size = pct(80))\n\n\n\n\n\n\n\n\nTable 1: Files and folders\n\n\n\n\n\n\n  \n    \n      Directory Files\n    \n    \n    \n      ./data1\n    \n  \n  \n    privateLeaderboard.csv\n    test.csv\n    data_dictionary.csv\n    sample_submission.csv\n    series_test.parquet\n    series_train.parquet\n    train.csv\n    profiles\n  \n  \n    \n      source: ‘/data/’\n    \n  \n  \n    \n      1 directory contents"
  },
  {
    "objectID": "index.html#establish-connection",
    "href": "index.html#establish-connection",
    "title": "Problematic Internet Habits in Children",
    "section": "3 Establish Connection",
    "text": "3 Establish Connection\n\nAn in-memory, DuckDB database connection was established for this project. The analysis in this project heavily relies on SQL syntax, and most of the computations are performed in database for efficiency.\n\n\n\n\nSet up an in-memory database.\nimport duckdb\n\nconn = duckdb.connect(':memory:')\n\n\n\n\n3.1 Customize Database Settings\n\n\n\nSet the database memory limit.\ntry:\n  conn.sql(f\"SET memory_limit = '24GB';\")\n  print(f\"Successfully changed memory limit to 24GB\")\nexcept:\n  print(f\"An error occurred in changing the memory limit.\")\n\n\nSuccessfully changed memory limit to 24GB\n\n\n\n\n\n\nSet the default ordering for the database.\ntry:\n  conn.sql(f\"SET default_order = 'ASC';\")\n  print(f\"Successfully changed the default order to ascending.\")\nexcept:\n  print(f\"The default order could not be changed.\")\n\n\nSuccessfully changed the default order to ascending."
  },
  {
    "objectID": "index.html#setup-database",
    "href": "index.html#setup-database",
    "title": "Problematic Internet Habits in Children",
    "section": "4 Setup Database",
    "text": "4 Setup Database\n\n4.1 Create New Data Types\n\nEnum data types significantly sped up many database operations. Enums are similar to the category data type in Python, and the factor datatype in R. (Holanda, 2021)\n\nHolanda, P. (2021, November 26). DuckDB – the lord of enums: The fellowship of the categorical and factors. DuckDB. https://duckdb.org/2021/11/26/duck-enum.html\n\n\n\n\nData structures setup for new data types.\nimport pandas as pd\n\nenums = ['internet_hours_enum'\n         ,'enroll_season_enum'\n         ,'disease_risk_enum'\n         ,'sii_enum', 'age_enum'\n         ,'sex_enum'\n         ,'pciat_season_enum'\n         ,'weekday_enum'\n         ,'quarter_enum'\n         ,'hour_enum'\n         ,'minute_enum'\n         ,'second_enum'\n         ,'id_actigraphy_enum']\n\n#siiSeries = pd.Series(data = ['None', 'Mild', 'Moderate', 'Severe'], dtype = str)\nsiiSeries = pd.Series(data = ['0', '1', '2', '3'], dtype = str)\nageSeries = pd.Series(data = range(5, 23), dtype = str)\nsexSeries = pd.Series(data = ['0', '1'], dtype = str)\npciatSeasonSeries = pd.Series(data = ['Fall'\n                                      ,'Spring'\n                                      ,'Summer'\n                                      ,'Winter'], dtype = str)\n\ninternetHrsSeries = pd.Series(data = ['0', '1', '2', '3'], dtype = str)\nquarterSeries = pd.Series(data = range(1, 5), dtype = str)\nweekdaySeries = pd.Series(data = range(1, 8), dtype = str)\nhourSeries = pd.Series(data = range(0, 24), dtype = str)\nminuteSeries = pd.Series(data = range(0, 60), dtype = str)\nsecondSeries = pd.Series(data = range(0, 60), dtype = str)\ndiseaseRiskSeries = pd.Series(data = ['Underweight'\n                                      ,'Normal'\n                                      ,'Increased'\n                                      ,'High'\n                                      ,'Very High'\n                                      ,'Extremely High'], dtype = str)\n\nid_df = conn.execute(f\"\"\"\n     SELECT \n         DISTINCT(id) AS id\n     FROM \n         read_parquet('data/series_train*/*/*'\n         ,hive_partitioning = true)\n     ORDER BY \n         id ASC;\n     \"\"\").df() \n\nidList = id_df['id'].to_list()\nidSeries = pd.Series(data = idList, dtype = str)\n\nenumDict = {\n  'disease_risk_enum': f\"{tuple(diseaseRiskSeries)}\"\n  ,'enroll_season_enum': f\"{tuple(pciatSeasonSeries)}\"\n  ,'sii_enum': f\"{tuple(siiSeries)}\"\n  ,'age_enum': f\"{tuple(ageSeries)}\"\n  ,'sex_enum': f\"{tuple(sexSeries)}\"\n  ,'pciat_season_enum': f\"{tuple(pciatSeasonSeries)}\"\n  ,'quarter_enum': f\"{tuple(quarterSeries)}\"\n  ,'weekday_enum': f\"{tuple(weekdaySeries)}\"\n  ,'hour_enum': f\"{tuple(hourSeries)}\"\n  ,'minute_enum': f\"{tuple(minuteSeries)}\"\n  ,'second_enum': f\"{tuple(secondSeries)}\"\n  ,'id_actigraphy_enum': f\"{tuple(idSeries)}\"\n  ,'internet_hours_enum': f\"{tuple(internetHrsSeries)}\"\n  }\n\nif len(enumDict) == len(enums):\n  print(f\"drops and creates are same length\")\nelse:\n  print(f\"drops and creates are not the same length\")\n\n\ndrops and creates are same length\n\n\n\n\nThis is designed to manage ENUM types in a DuckDB database in a modular and user-friendly way. It provides functions to create and drop ENUM types, and it uses Python data structures like pandas Series and dictionaries to handle inputs and outputs.\nIt is part of the data preprocessing and database setup pipeline in this project. It ensures that the database schema (specifically ENUM types) is properly configured before performing further operations, such as aggregating light exposure data.\n\n\n\n\nAn approach for defining and verifying custom data types in the database.\ndef try_create(conn,type_str: str, enum_str: str) -&gt; pd.Series:\n  \"\"\"\n  :::WHY(s):::\n    Is there a modular approach to creating ENUM types in a database?\n  :::HOW(s):::\n    Utilize python data structures such as pandas Series and dictionary as parameters.\n  \"\"\"\n  # the try, except can help categorize the outputs for ui/ux\n  try:\n    conn.execute(f\"CREATE TYPE {type_str} AS ENUM {enum_str};\")\n    return pd.Series(type_str, index = ['created'])\n  except duckdb.duckdb.CatalogException:\n    return pd.Series(type_str, index = ['already existed'])\n\ndef try_drop(conn, type_str: str) -&gt; pd.Series:\n  \"\"\"\n  :::WHY(s):::\n    Is there a modular approach to dropping ENUM types in a database?\n  :::HOW(s):::\n    Utilize python data structures such as pandas Series and dictionary as parameters.\n  \"\"\"\n  # the try, except can help categorize the outputs for ui/ux\n  try:\n    conn.execute(f\"DROP TYPE {type_str};\")\n    return pd.Series(type_str, index = ['dropped'])\n  except duckdb.duckdb.CatalogException:\n    return pd.Series(type_str, index = ['did not exist'])\n  \ndroplist = []\nfor e in enums:   \n   droplist.append(try_drop(conn, type_str = e))\n   \ndropFrame = pd.DataFrame(droplist)\ndropFrame = dropFrame.sort_values(by = dropFrame.columns.to_list(),\n                                  ascending = True,\n                                  ignore_index = True)\ncreateList = []\nfor eType, eSeries in enumDict.items():\n    createList.append(try_create(conn,\n                                 type_str = eType,\n                                 enum_str = eSeries))\ncreateFrame = pd.DataFrame(createList)\ncreateFrame = createFrame.sort_values(by = createFrame.columns.to_list(),\n                                      ascending = True,\n                                      ignore_index = True)\n\npydf = pd.concat([dropFrame, createFrame], axis = 1)\n\n\n\n\n\n\ntable setup\nr_df &lt;- py$pydf\n\nif (colnames(r_df[1]) == \"did not exist\"){\n  locations_list = list(\"did not exist\", \"created\") \n  notes_list =  list(\"Enums that did not exist.\",\n                     \"Enums that now exist.\")\n  } else {\n  locations_list = list(\"dropped\", \"created\")\n  notes_list =  list(\"Enums that were dropped.\",\n                     \"Enums that now exist.\")\n  }\n\n# Create a table dataframe, or tibble, of footnotes\nfootnotes_df &lt;- tibble(notes = notes_list, locations = locations_list)\n\npal_df &lt;- tibble(\n  cols = locations_list,\n  pals = list(\n    eval_palette(\"ggsci::legacy_tron\", 7, 'd', 1))\n  )\n\n#tbl_font_size = pct(80))\nrTable &lt;- r_table_theming(r_df,\n                          title = \"Custom Data Types\",\n                          subtitle = NULL,\n                          footnotes_df,\n                          source_note = md(\"**source**: Kaggle\"),\n                          pal_df,\n                          multiline_feet = TRUE,\n                          tbl_font_size = pct(80)\n                          )\n\n\n\n\n\n\n\n\nTable 2: Enums dropped and created.\n\n\n\n\n\n\n  \n    \n      Custom Data Types\n    \n    \n    \n      did not exist1\n      created2\n    \n  \n  \n    age_enum\nage_enum\n    disease_risk_enum\ndisease_risk_enum\n    enroll_season_enum\nenroll_season_enum\n    hour_enum\nhour_enum\n    id_actigraphy_enum\nid_actigraphy_enum\n    internet_hours_enum\ninternet_hours_enum\n    minute_enum\nminute_enum\n    pciat_season_enum\npciat_season_enum\n    quarter_enum\nquarter_enum\n    second_enum\nsecond_enum\n    sex_enum\nsex_enum\n    sii_enum\nsii_enum\n    weekday_enum\nweekday_enum\n  \n  \n    \n      source: Kaggle\n    \n  \n  \n    \n      1 Enums that did not exist.\n    \n    \n      2 Enums that now exist.\n    \n  \n\n\n\n\n\n\n\n\n\n\n4.2 Initial Extraction Transformation and Load Pipeline\n\nTransformations are applied while reading in data from files to minimize steps needed. Not sure why the data had hyphens in the columns names, originally. These hyphens interfere with SQL database queries, so it might be useful to replace them with underscore characters. There is no need to overwrite the original CSV’s to maintain a clear trail of data trasformations.\n\n\n\n\nInitial data pipeline extraction, transformation and loading to the database.\ndef setup_duckdb_pipeline(\n  csvDict: dict, \n  parquetDict: dict, \n  conn: duckdb.DuckDBPyConnection) -&gt; None:\n    \n  try:\n    {\n      table_name: duckdb.sql(f\"\"\"\n      CREATE OR REPLACE TABLE {table_name} AS \n      SELECT \n        *\n      FROM \n        df;\n      \"\"\", connection = conn) \n      for table_name, df in csvDict.items()\n      }\n    for key, value in csvDict.items():\n      result = conn.execute(f\"SELECT COUNT(*) FROM {key}\").fetchone()\n      print(f\"Successfully created table: {key}, Row count: {result[0]}\")\n  except Exception as e:\n    print(f\"Error loading table: {str(e)}\")\n    raise\n  \n  if parquetDict:\n    write_datasets(conn, parquetDict)\n  \n# Create tables from Parquet files\ndef write_datasets (\n  conn: duckdb.DuckDBPyConnection, parquetDict: dict):\n      try:\n        {\n          table_name: duckdb.sql(f\"\"\"\n           CREATE OR REPLACE TABLE {table_name} AS\n           SELECT \n             id::id_actigraphy_enum AS id\n             ,quarter::TEXT::quarter_enum AS quarter\n             ,weekday::TEXT::weekday_enum AS weekday\n             ,light\n             ,(time_of_day / 3_600_000_000_000) AS hour_of_day\n             ,relative_date_PCIAT\n           FROM read_parquet(\n             '{file_path}'\n             ,hive_partitioning = true\n             );\"\"\", connection=conn)\n           for table_name, file_path in parquetDict.items()\n           }\n        for key, value in parquetDict.items():\n          result = conn.execute(f\"SELECT COUNT(*) FROM {key}\").fetchone()\n          print(\n            f\"Successfully created table: {key}, Row count, {result[0]}\")\n      except Exception as e:\n        print(f\"Error writing dataset: {str(e)}\")\n        raise\n\ntrainCsvDf = pd.read_csv(\"data/train.csv\")\ntestCsvDf = pd.read_csv(\"data/test.csv\")\n\ndictDf = pd.read_csv(\"data/data_dictionary.csv\")\n\ntrainCsvDf.columns = trainCsvDf.columns.str.replace('-','_') \ntrainCsvDf.columns = trainCsvDf.columns.str.lower() \ntestCsvDf.columns = testCsvDf.columns.str.replace('-','_') \ntestCsvDf.columns = testCsvDf.columns.str.lower() \n\ndictDf.Field = dictDf.Field.replace(\"-\", \"_\", regex = True)\n\ncsvDict = {\n  \"TrainCsv\": trainCsvDf\n  ,\"TestCsv\": testCsvDf\n  ,\"DataDict\": dictDf\n  }\n\nparquetDict = {\n  \"ActigraphyTrain\": 'data/series_train.parquet*/*/*'\n  ,\"ActigraphyTest\": 'data/series_test*/*/*'\n  }\n\ntry:\n  setup_duckdb_pipeline(csvDict, parquetDict, conn)\nexcept:\n  print(f\"Could not set up data pipeline.\")\n\n\nSuccessfully created table: TrainCsv, Row count: 3960\nSuccessfully created table: TestCsv, Row count: 20\nSuccessfully created table: DataDict, Row count: 81\nSuccessfully created table: ActigraphyTrain, Row count, 314569149\nSuccessfully created table: ActigraphyTest, Row count, 439726\n\n\n\n\n\n\nVerify ActigraphyTrain in the database.\npydf = conn.sql(f\"\"\"\nSELECT *\nFROM ActigraphyTrain\nORDER BY \n  id ASC\n  ,relative_date_PCIAT ASC\n  ,hour_of_day ASC\nLIMIT 10;\"\"\").df()\n\n\n\n\n\n\ntable setup\nr_df &lt;- py$pydf\n\nr_df &lt;- r_df |&gt;\n  dplyr::select(id, quarter, weekday, relative_date_PCIAT, light, hour_of_day)\n\nnotes_list =  list(\n  \"Unique participant identifier code, to track related data from actigraphy to hbn datasets.\",\n  \"Annual quarter related to month of observation.\",\n  \"Day of the week (Monday = 1)\",\n  \"Light exposure, measured in 'lux'\",\n  \"24-hour time converted to a float for easier conversion for time-series analysis.\",\n  \"Days since questionairre, also convenient to help order daily values for time-series analysis.\"\n  )\n\nlocations_list = list(\"id\",\n                      \"quarter\",\n                      \"weekday\",\n                      \"light\",\n                      \"hour_of_day\",\n                      \"relative_date_PCIAT\")\n\n# Create a table data-frame, or tibble, of footnotes\nfootnotes_df &lt;- tibble(\n  notes = notes_list, \n  locations = locations_list)\n\npal_df &lt;- tibble(\n  cols = locations_list,\n  pals = list(eval_palette(\"harrypotter::ravenclaw\", 7, 'c', 1)))\n\nrTable &lt;- r_table_theming(r_df,\n                          title = \"Actigraphy Data - Training Set\",\n                          subtitle = NULL,\n                          footnotes_df,\n                          source_note = md(\"**source**: Kaggle\"),\n                          pal_df,\n                          multiline_feet = TRUE\n                          #tbl_font_size = pct(85)\n                          )\n\n\n\n\n\n\n\n\nTable 3: A preview of ActigraphyTrain database table.\n\n\n\n\n\n\n  \n    \n      Actigraphy Data - Training Set\n    \n    \n    \n      id1\n      quarter2\n      weekday3\n      relative_date_PCIAT4\n      light5\n      hour_of_day6\n    \n  \n  \n    00115b9f\n3\n4\n41\n53.00000\n15.81667\n    00115b9f\n3\n4\n41\n51.66667\n15.81806\n    00115b9f\n3\n4\n41\n50.33333\n15.81944\n    00115b9f\n3\n4\n41\n50.50000\n15.82083\n    00115b9f\n3\n4\n41\n33.16667\n15.89861\n    00115b9f\n3\n4\n41\n31.33333\n15.90000\n    00115b9f\n3\n4\n41\n29.50000\n15.90139\n    00115b9f\n3\n4\n41\n27.66667\n15.90278\n    00115b9f\n3\n4\n41\n25.83333\n15.90417\n    00115b9f\n3\n4\n41\n24.00000\n15.90556\n  \n  \n    \n      source: Kaggle\n    \n  \n  \n    \n      1 Unique participant identifier code, to track related data from actigraphy to hbn datasets.\n    \n    \n      2 Annual quarter related to month of observation.\n    \n    \n      3 Day of the week (Monday = 1)\n    \n    \n      4 Days since questionairre, also convenient to help order daily values for time-series analysis.\n    \n    \n      5 Light exposure, measured in 'lux'\n    \n    \n      6 24-hour time converted to a float for easier conversion for time-series analysis."
  },
  {
    "objectID": "index.html#eda",
    "href": "index.html#eda",
    "title": "Problematic Internet Habits in Children",
    "section": "5 EDA",
    "text": "5 EDA\n\nThis is designed to filter and create new database tables based on regular expression (regex) patterns applied to column names. It is part of a data preprocessing pipeline. The code modularizes the process of selecting columns and creating new tables, making it reusable and efficient.\n\n\n\n\nCreates several tables based on column label strings.\nimport re\n\ndef filter_columns_by_regex(col_dict: dict, regex_pattern: str) -&gt; dict:\n  \"\"\"\n  :::WHY(s):::\n    Can there be a way to use dictionaries to specify columns?\n\n  :::GOAL(s):::\n    Regex selection of column names from a dictionary.\n  \"\"\"\n  return {\n    col: dtype \n    for col, dtype in col_dict.items() \n    if re.search(regex_pattern, col)\n    }\n\ndef create_table_with_regex_columns(\n  conn: duckdb.duckdb\n  ,source_table: str\n  ,new_table_name: str \n  ,regex_pattern: str \n  ,col_dict: dict\n  ) -&gt; None:  \n\n  \"\"\"\n  :::WHY(s):::\n    There should to be a more streamlined way to utilize columnar information following regular string patterns to generate tables based on such information. This could follow and carry out pre-existing data modeling plans.\n\n  :::GOAL(s):::\n    To create new database tables with modularized SQL queries generated with the help of regex pattern matching selection.\n\n  :::EXAMPLE(s):::\n    The dictionary item:\n      \"Demographic\": r\"^id|^sii|^basic\\S+\"\n \n    Would generate a table named 'Demographic' from data with columns for 'id', 'sii' and all columns starting with 'basic' strings. \n  \"\"\"\n\n  # filter columns\n  filtered_col_dict = filter_columns_by_regex(col_dict, regex_pattern)\n  \n  # regex column selecting via dict comprehension and vectorized filtering\n  regex_select_sql = f\"\"\"\n  CREATE OR REPLACE TABLE {new_table_name} AS \n  SELECT\n    {', '.join([f'\"{col}\"' for col in filtered_col_dict.keys()])}\n  FROM {source_table};\n  \"\"\"\n\n  conn.execute(regex_select_sql)\n\n# It'd be useful to get the data types for creating a new table of values\ncoltype_overview = conn.sql(f\"\"\"\n  SELECT column_name\n         ,data_type\n  FROM \n    information_schema.columns\n  WHERE \n    table_name = 'TrainCsv';\"\"\").df()\n\n# Map the column names with data types\ncol_dict = dict(zip(coltype_overview.column_name, coltype_overview.data_type))\n\nregex_dict_train = {\n  \"Demographic\": r\"^id|^sii|^basic\\S+\"\n  ,\"Physical\": r\"^id|^sii|^physical\\S+\"\n  ,\"FgVital\": r\"^id|^sii|^fitness_E\\S+\"\n  ,\"FgChild\": r\"^id|^sii|^fgc\\S+\"\n  ,\"Bia\": r\"^id|^sii|^bia\\S+\"\n  ,\"Paqa\": r\"^id|^sii|^paq_a\\S+\"\n  ,\"Pciat\": r\"^id|^sii|^pciat\\S+\"\n  ,\"Sds\": r\"^id|^sii|^sds\\S+\"\n  ,\"InternetUse\": r\"^id|^sii|^preint\\S+\"\n  }\n\n# There's no Pciat in the test set\nregex_dict_test = {\n  \"DemographicTest\": r\"^id|^basic\\S+\"\n  ,\"PhysicalTest\": r\"^id|^physical\\S+\"\n  ,\"FgVitalTest\": r\"^id|^fitness_E\\S+\"\n  ,\"FgChildTest\": r\"^id|^fgc\\S+\"\n  ,\"BiaTest\": r\"^id|^bia\\S+\"\n  ,\"PaqaTest\": r\"^id|^paq_a\\S+\"\n # ,\"Pciat_OfTest\": r\"^id|^pciat\\S+\" \n  ,\"SdsTest\": r\"^id|^sds\\S+\"\n  ,\"InternetUseTest\": r\"^id|^preint\\S+\"\n  }\n\n# Loop through the data structures to create tables for the train set\nfor new_table_name, regex_pattern in regex_dict_train.items():\n  create_table_with_regex_columns(\n    conn \n    ,'TrainCsv'\n    ,new_table_name\n    ,regex_pattern\n    ,col_dict\n    ) \n\n# Loop through the data structures to create tables for the test set\nfor new_table_name, regex_pattern in regex_dict_test.items():\n  create_table_with_regex_columns(\n    conn\n    ,'TestCsv'\n    ,new_table_name\n    ,regex_pattern \n    ,col_dict\n    )\n\n\n\n\n\n\nPerform a join on matching Ids to combine demographics data with actigraphy data such as time and light exposure.\nconn.sql(\"\"\"\nCREATE OR REPLACE TABLE IntermediateActigraphy AS\nSELECT id\n       ,basic_demos_enroll_season::TEXT::enroll_season_enum AS enroll_season\n       ,basic_demos_age::TEXT::age_enum AS age\n       ,basic_demos_sex::TEXT AS sex\n       ,sii::INTEGER::TEXT::sii_enum AS sii\nFROM \n  Demographic\nORDER BY\n  id ASC;\n\"\"\")\n\nconn.sql(\"\"\"\nCREATE OR REPLACE TABLE \n  ActigraphyTrain\nAS\n  SELECT\n    ia.*\n    ,at.hour_of_day\n    ,at.weekday\n    ,at.quarter\n    ,at.light\n  FROM \n    ActigraphyTrain at \n  LEFT JOIN \n    IntermediateActigraphy ia\n  ON \n    ia.id = at.id;\n\"\"\")\n\nconn.sql(\"\"\"\nCREATE OR REPLACE TABLE IntermediateActigraphy AS\nSELECT\n  id\n  ,basic_demos_enroll_season::TEXT::enroll_season_enum AS enroll_season\n  ,basic_demos_age::TEXT::age_enum AS age\n  ,basic_demos_sex::TEXT AS sex\nFROM \n  DemographicTest\nORDER BY\n  id ASC;\n\"\"\")\n\nconn.sql(\"\"\"\nCREATE OR REPLACE TABLE \n  ActigraphyTest\nAS\n  SELECT\n    ia.*\n    ,at.hour_of_day\n    ,at.quarter\n    ,at.weekday\n    ,at.light\n  FROM \n    ActigraphyTest at \n  LEFT JOIN \n    IntermediateActigraphy ia\n  ON \n    ia.id = at.id;\n\"\"\")\n\nconn.sql(\"DROP TABLE IntermediateActigraphy;\")\n\ntry:\n conn.sql(f\"CHECKPOINT;\")\n print(f\"Cleared unused allocated space from memory.\")\nexcept:\n  print(f\"Could not clear from memory.\")\n\n\n┌─────────┐\n│ Success │\n│ boolean │\n├─────────┤\n│ 0 rows  │\n└─────────┘\n\nCleared unused allocated space from memory.\n\n\n\n\n5.1 Actigraphy, Physical, and Demographics\n\nIt might be useful to explore how the time of day and associated exposure to light might correlate to other factors, such as sii, physical characteristics, and internet use.\n\n\n5.1.1 Informal Hypothesis\n\nHypothesis: More time spent indoors on the internet might be associated with lower light exposure during daytime (Q1-Q2, Q2-Q3) and higher light exposure during nighttime (min-Q1, Q3-max).\nThe BMI risk categories found at, (Zierle-Ghosh & Jan, 2024), have been borrowed for this project. The SQL query syntax, therefore, aims to reflect a structure of evidence-based classification. Though not used in this project, it might be useful to classify lux statistics in a way that reflect a classification standard (Lux Measurements, n.d.).\n\nZierle-Ghosh, A., & Jan, A. (2024). Physiology, body mass index. In StatPearls. StatPearls Publishing. http://www.ncbi.nlm.nih.gov/books/NBK535456/\n\nLux measurements. (n.d.). Retrieved November 24, 2024, from https://actigraphcorp.my.site.com/support/s/article/Lux-Measurements\n\n\n\n5.1.2 Lux by Hour of Day Quartile\n\nRepetitive sql commands are quite verbose and repetitive, at times. Python functions, alongside data structures, such as dataframes, and dictionaries could actually help make running SQL commands less repetitive and more modular.\nThe code is designed to process and aggregate light exposure data from actigraphy datasets (likely from wearable devices) using DuckDB as an in-memory database. The code is modularized into several functions to handle different tasks, such as calculating quartiles, detecting specific columns, creating intermediate tables, and joining these tables into a final aggregated result.\nAdditionally, the code is useful for analyzing light exposure data over different times of the day (based on quartiles of hour_of_day). It aggregates the data into meaningful segments and creates a final table that can be used for further analysis or visualization. Included is a try-except block to handle any potential errors during the table joining process, ensuring that the user is informed if something goes wrong.\n\n\n\n\nExtract actigraphy data based on the time of day quartile.\ndef quartiler(\n  conn: duckdb.duckdb, \n  col_name: str, \n  source_name: str) -&gt; dict:\n  \"\"\"\n  ::WHY(s):: \n    SQL can sometimes require a lot of code for repetitive commands, but in a Python environment, database queries can be modularized.\n  \n  ::GOAL(s)::  \n    To process SQL queries into useful quartile information represented by intuitive key labels.\n  \"\"\"\n  summaryDf = conn.sql(f\"\"\"\n  SUMMARIZE\n  SELECT\n    {col_name}\n  FROM \n    {source_name};\"\"\").df()\n\n  quartileDict = {\n    'min': summaryDf['min'][0]\n    ,'Q1': summaryDf.q25[0]\n    ,'Q2': summaryDf.q50[0]\n    ,'Q3': summaryDf.q75[0]\n    ,'max': summaryDf['max'][0]\n    }\n  \n  return quartileDict\n\ndef siiDetect (detect_frame: pd.DataFrame) -&gt; bool:\n  if 'sii' in detect_frame.column_name.values:\n    return True\n\ndef intermediateLighter(\n  conn: duckdb.duckdb,\n  new_tables: list,\n  quarters: dict, \n  quartuples: pd.Series,\n  from_table: str) -&gt; None:\n  \"\"\"\n  :::WHY(s):::\n    Tables based on the same parameters but different parameter values could be modularized.\n    \n  :::GOAL(s):::\n    Process repetitive SQL efficiently and intuitively using data structures that simplify the process. \n  \"\"\"\n  \n  detect_frame = conn.execute(f\"\"\"\n  SELECT *\n  FROM \n    information_schema.columns\n  WHERE \n    table_name = '{from_table}';\n  \"\"\").df()\n  \n  for i in list(range(4)):\n    if siiDetect(detect_frame) == True:\n      conn.sql(f\"\"\"\n        CREATE OR REPLACE TABLE {new_tables[i]} AS\n        SELECT\n          id\n          ,enroll_season\n          ,age\n          ,sex\n          ,sii\n          ,AVG(light) AS {quartuples.index[i]}\n        FROM \n          '{from_table}'\n        WHERE \n          hour_of_day BETWEEN \n              {quarters[quartuples.iloc[i][0]]}::DOUBLE \n            AND \n              {quarters[quartuples.iloc[i][1]]}::DOUBLE\n        GROUP BY \n          ALL\n        ORDER BY \n          id ASC;\"\"\")\n    else:\n      conn.sql(f\"\"\"\n        CREATE OR REPLACE TABLE {new_tables[i]} AS\n        SELECT\n          id\n          ,enroll_season\n          ,age\n          ,sex\n          ,AVG(light) AS {quartuples.index[i]}\n        FROM \n          '{from_table}'\n        WHERE \n          hour_of_day BETWEEN \n              {quarters[quartuples.iloc[i][0]]}::DOUBLE \n            AND \n              {quarters[quartuples.iloc[i][1]]}::DOUBLE\n        GROUP BY \n          ALL\n        ORDER BY \n          id ASC;\"\"\")\n\ndef joinLights (from_to_tables: dict) -&gt; None:\n  \n  for from_table, to_table in from_to_tables.items():\n    \n    new_tables = ['Light1', 'Light2', 'Light3', 'Light4']\n    \n    quarters = quartiler(conn, 'hour_of_day', from_table)\n\n    intermediateLighter(conn, new_tables, quarters, quartuples, from_table)\n  \n    conn.sql(f\"\"\"\n    CREATE OR REPLACE TABLE {to_table} AS\n    SELECT \n      l1.*\n      ,l2.q1_q2\n      ,l3.q2_q3\n      ,l4.q3_max\n    FROM \n      Light1 l1\n    LEFT JOIN Light2 l2 ON l1.id = l2.id\n    LEFT JOIN Light3 l3 ON l1.id = l3.id\n    LEFT JOIN Light4 l4 ON l1.id = l4.id;\n    \"\"\")\n    \n    for table in new_tables:\n      conn.sql(f\"DROP TABLE {table};\")\n      \nquartuples = pd.Series(\n  data = \n  [('min','Q1')\n  ,('Q1', 'Q2')\n  ,('Q2', 'Q3')\n  ,('Q3' ,'max')]\n  ,index =\n  ['min_q1'\n  ,'q1_q2'\n  ,'q2_q3'\n  ,'q3_max'])\n\nfrom_to_tables = {\n  'ActigraphyTrain': 'AggregatedLights'\n  ,'ActigraphyTest': 'AggregatedLightsTest'\n  }\n\ntry:\n  joinLights(from_to_tables)\n  pydf = conn.sql(\"SELECT * FROM AggregatedLights LIMIT 10;\").df()\n  print(f\"Joined the lux tables and saved a preview of the aggregated table.\")\nexcept:\n  print(f\"Lux tables were not joined.\")\n\n\nJoined the lux tables and saved a preview of the aggregated table.\n\n\n\n\n\n\nFree unused database memory from dropped tables and such.\ntry:\n  conn.sql(f\"CHECKPOINT;\")\n  print(f\"Cleared unused memory.\")\nexcept:\n  print(f\"Could not clean unused memory.\")\n\n\n┌─────────┐\n│ Success │\n│ boolean │\n├─────────┤\n│ 0 rows  │\n└─────────┘\n\nCleared unused memory.\n\n\n\n\n\n\ntable setup\nr_df &lt;- py$pydf\n\nnotes_list =  list(\n  \"Unique participant identifier.\",\n  \"Participant enrollment into study season.\",\n  \"Age in years.\",\n  \"Sex (0 = Female, 1 = Male)\",\n  \"Severity Impairment Index\",\n  \"Mean light exposure across Q1 of a 24 hour time scale.\",\n  \"Mean light exposure across Q2 of a 24 hour time scale.\",\n  \"Mean light exposure across Q3 of a 24 hour time scale.\",\n  \"Mean light exposure across Q4 of a 24 hour time scale.\"\n  )\n\nlocations_list = list(\n  \"id\"\n  ,\"enroll_season\"\n  ,\"age\"\n  ,\"sex\"\n  ,\"sii\"\n  ,'min_q1'\n  ,'q1_q2'\n  ,'q2_q3'\n  ,'q3_max')\n\n# Create a table data-frame, or tibble, of footnotes\nfootnotes_df &lt;- tibble(notes = notes_list, \n                       locations = locations_list)\n\npal_df &lt;- tibble(\n  cols = locations_list,\n  pals = list(\n    eval_palette(\"harrypotter::ravenclaw\", 7, 'c', 1))\n  )\n\nrTable &lt;- r_table_theming(r_df,\n                          title = \"Quartile Data - Lights\",\n                          subtitle = NULL,\n                          footnotes_df,\n                          source_note = md(\"**source**: Kaggle\"),\n                          pal_df,\n                          multiline_feet = TRUE)\n\n\n\n\nTable 4, returns the averages of light exposure, grouped by individual participants across the 4 6-hour time-frames of the day.\n\n\n\n\n\n\nTable 4: Quantiles related to mean lux exposure.\n\n\n\n\n\n\n  \n    \n      Quartile Data - Lights\n    \n    \n    \n      id1\n      enroll_season2\n      age3\n      sex4\n      sii5\n      min_q16\n      q1_q27\n      q2_q38\n      q3_max9\n    \n  \n  \n    00115b9f\nWinter\n9\n0\n1\n23.055476\n61.55684\n52.87599\n17.019149\n    001f3379\nSpring\n13\n1\n1\n2.092033\n38.91856\n27.36159\n3.443391\n    00f332d1\nWinter\n14\n0\n1\n20.998220\n70.31315\n172.43806\n20.116062\n    012e3869\nSummer\n6\n0\n0\n13.294440\n21.52306\n14.48648\n11.505535\n    029a19c9\nWinter\n17\n1\n1\n11.332627\n58.89862\n35.69254\n7.528905\n    02cebf33\nSpring\n12\n1\n1\n35.521826\n21.78129\n44.61624\n52.744249\n    02cf7384\nSummer\n16\n1\n0\n7.042051\n21.40214\n24.17295\n7.129046\n    035c96dd\nSummer\n13\n1\n0\n15.085412\n16.63577\n19.22980\n14.558703\n    0417c91e\nSpring\n6\n1\n0\n13.859709\n131.72620\n198.18277\n17.371655\n    04afb6f9\nSummer\n17\n1\n2\n14.195046\n19.53276\n19.77811\n10.798290\n  \n  \n    \n      source: Kaggle\n    \n  \n  \n    \n      1 Unique participant identifier.\n    \n    \n      2 Participant enrollment into study season.\n    \n    \n      3 Age in years.\n    \n    \n      4 Sex (0 = Female, 1 = Male)\n    \n    \n      5 Severity Impairment Index\n    \n    \n      6 Mean light exposure across Q1 of a 24 hour time scale.\n    \n    \n      7 Mean light exposure across Q2 of a 24 hour time scale.\n    \n    \n      8 Mean light exposure across Q3 of a 24 hour time scale.\n    \n    \n      9 Mean light exposure across Q4 of a 24 hour time scale.\n    \n  \n\n\n\n\n\n\n\n\n\n\n\nJoin light data with internet use hours data.\nlightsDict = {\n  'AggregatedLights': 'InternetUse', \n  'AggregatedLightsTest': 'InternetUseTest'\n  }\n\nfor from_tbl, join_tbl in lightsDict.items():\n  conn.sql(f\"\"\"\n  CREATE OR REPLACE TABLE '{from_tbl}' AS\n  SELECT\n    ft.*\n    ,jt.preint_eduhx_computerinternet_hoursday::INTEGER::TEXT::internet_hours_enum AS useHrs \n  FROM\n    '{from_tbl}' ft\n    LEFT JOIN \n      {join_tbl} jt\n    ON \n      ft.id = jt.id;\n  \"\"\")\n\n\n\n\nChecking the internet use hours parameter to get an overview along with null percentage to see if we might be able to use it, as-is, for joining with the corresponding lux quartile.\n\n\n\n\n\n\n5.1.3 Sii, Lux, and Internet Use\n\nTable 5, is returning some descriptive statistics, such as count and mean, across the behavioral index, and mean lux by quartile, grouped by internet use hours.\n\n\n\n\nGroup-averaging sii, internet use hours, and pre-averaged lux quartile by age and sex.\npydf = conn.sql(f\"\"\"\nSELECT\n  useHrs\n  ,COUNT(*) AS observations\n  ,AVG(sii::INTEGER) AS sii\n  ,AVG(min_q1) AS 'q1'\n  ,AVG(q1_q2) AS 'q2'\n  ,AVG(q2_q3) AS 'q3'\n  ,AVG(q3_max) AS 'q4'\nFROM \n  AggregatedLights al\nGROUP BY \n  useHrs;\"\"\").df()\n\n#obsSum = pydf.observations.sum()\n\npydf = pydf.dropna()\n\n\n\n\n\n\ntable setup\nr_df &lt;- py$pydf\n\nnotes_list =  list(\n  \"Hours spent on the internet, as reported by caregivers of the participants.\",\n  \"Count of observations in the dataset.\",\n  \"Problematic behavior score.\",\n  \"Mean lux (12am to 6am)\",\n  \"Mean lux (6am to 12pm)\",\n  \"Mean lux (12pm to 6pm)\",\n  \"Mean lux (6pm to 12am)\"\n  )\n\nlocations_list = list(\n  \"useHrs\", \n  \"observations\",\n  \"sii\",\n  \"q1\",\n  \"q2\",\n  \"q3\",\n  \"q4\"\n  )\n\nfootnotes_df &lt;- tibble(\n  notes = notes_list, \n  locations = locations_list\n  )\n\npal_df &lt;- tibble(\n  cols = locations_list,\n  pals = list(\n    eval_palette(\"harrypotter::ravenclaw\", 7, 'c', 1))\n  )\n\nrTable &lt;- r_table_theming(r_df,\n                          title = \"Problematic Behavior to Internet Usage\",\n                          subtitle = NULL,\n                          footnotes_df,\n                          source_note = md(\"**source**: Kaggle\"),\n                          pal_df,\n                          multiline_feet = FALSE\n                          )\n\n\n\n\n\n\n\n\nTable 5: Grouped averages of lux to sii.\n\n\n\n\n\n\n  \n    \n      Problematic Behavior to Internet Usage\n    \n    \n    \n      useHrs1\n      observations2\n      sii3\n      q14\n      q25\n      q36\n      q47\n    \n  \n  \n    0\n474\n0.3291139\n12.35074\n54.28695\n75.38090\n14.38823\n    1\n117\n0.6666667\n15.14193\n46.03265\n66.82300\n15.32112\n    2\n288\n0.7534722\n14.24200\n43.75740\n65.44579\n15.35031\n    3\n102\n1.1176471\n18.15643\n48.20455\n70.30213\n19.06226\n  \n  \n    \n      source: Kaggle\n    \n  \n  \n    \n      \n        1 Hours spent on the internet, as reported by caregivers of the participants., 2 Count of observations in the dataset., 3 Problematic behavior score., 4 Mean lux (12am to 6am), 5 Mean lux (6am to 12pm), 6 Mean lux (12pm to 6pm), 7 Mean lux (6pm to 12am)\n      \n    \n  \n\n\n\n\n\n\n\n\n\n\n\nplot setup\nlibrary(scales)\n\nr_plot &lt;- ggplot(data = r_df, \n                 mapping = aes(x = useHrs, y = sii)\n                 ) +\n  geom_col(aes(fill = sii)) +\n  scale_fill_gradient(low = muted(\"yellow\"), high = muted(\"purple\")) +\n  labs(title = 'Internet Use to Behavioral Index', \n       x = 'Hours of Internet Use', \n       y = \"Problematic Behavior Index (sii)\")\n\nr_plot &lt;- r_plot + ggplot_theming()\n\n\n\n\n\n\n\n\n\n\n\nFigure 1: Categorized bmi risk to internet use and sii.\n\n\n\n\n\n\n\n5.1.4 Disease Risk Categories\n\nSomething that people might not see everyday is SQL sytnax used within a Python environment. The performance of the sql database, in this case, duckdb, likely outperforms pandas, polars, and such. My aim in this project is to demonstrate some competence with sql, but also approach practical use-case scenarios.\n\n\n\n\nCombining SQL with Python data structures to organized data according to several conditions, such as: bmi, age, sex, and waist_circumference.\nriskyDictionary = {\n  'Risk1': \n    (f\",CASE WHEN ph.physical_bmi &lt; 18.5 THEN 'Underweight'\"\n    ,f\"WHEN ph.physical_bmi BETWEEN 18.5 AND 24.9 THEN 'Normal'\"\n    ,f\"WHEN ph.physical_bmi BETWEEN 25.0 AND 29.9 THEN 'Increased'\"\n    ,f\"WHEN ph.physical_bmi BETWEEN 30.0 AND 34.9 THEN 'High'\"\n    ,f\"WHEN ph.physical_bmi BETWEEN 35.0 AND 39.9 THEN 'Very High'\"\n    ,f\"WHEN ph.physical_bmi &gt;= 40 THEN 'Extremely High'\"\n    ,f\"ph.physical_waist_circumference &lt;= 35 AND al.sex = '0'\"),\n  'Risk2':\n    (f\",CASE WHEN ph.physical_bmi &lt; 18.5 THEN 'Underweight'\"\n    ,f\"WHEN ph.physical_bmi BETWEEN 18.5 AND 24.9 THEN 'Normal'\"\n    ,f\"WHEN ph.physical_bmi BETWEEN 25.0 AND 29.9 THEN 'High'\"\n    ,f\"WHEN ph.physical_bmi BETWEEN 30.0 AND 34.9 THEN 'Very High'\"\n    ,f\"WHEN ph.physical_bmi BETWEEN 35.0 AND 39.9 THEN 'Very High'\"\n    ,f\"WHEN ph.physical_bmi &gt;= 40 THEN 'Extremely High'\"\n    ,f\"ph.physical_waist_circumference &gt; 35 AND al.sex = '0'\"),\n  'Risk3':\n    (f\",CASE WHEN ph.physical_bmi &lt; 18.5 THEN 'Underweight'\"\n    ,f\"WHEN ph.physical_bmi BETWEEN 18.5 AND 24.9 THEN 'Normal'\"\n    ,f\"WHEN ph.physical_bmi BETWEEN 25.0 AND 29.9 THEN 'Increased'\"\n    ,f\"WHEN ph.physical_bmi BETWEEN 30.0 AND 34.9 THEN 'High'\"\n    ,f\"WHEN ph.physical_bmi BETWEEN 35.0 AND 39.9 THEN 'Very High'\"\n    ,f\"WHEN ph.physical_bmi &gt;= 40 THEN 'Extremely High'\"\n    ,f\"ph.physical_waist_circumference &lt;= 40 AND al.sex = '1'\"),\n  'Risk4':\n    (f\",CASE WHEN ph.physical_bmi &lt; 18.5 THEN 'Underweight'\"\n    ,f\"WHEN ph.physical_bmi BETWEEN 18.5 AND 24.9 THEN 'Normal'\"\n    ,f\"WHEN ph.physical_bmi BETWEEN 25.0 AND 29.9 THEN 'High'\"\n    ,f\"WHEN ph.physical_bmi BETWEEN 30.0 AND 34.9 THEN 'Very High'\"\n    ,f\"WHEN ph.physical_bmi BETWEEN 35.0 AND 39.9 THEN 'Very High'\"\n    ,f\"WHEN ph.physical_bmi &gt;= 40 THEN 'Extremely High'\"\n    ,f\"ph.physical_waist_circumference &gt; 40 AND al.sex = '1'\")\n    }\n\nriskyDf = pd.DataFrame(data = riskyDictionary)\n\nfor key, value in riskyDf.items():\n  try:\n    conn.sql(f\"\"\"\n    CREATE OR REPLACE TABLE {key} AS\n    SELECT\n      al.*\n      {value[0]}\n      {value[1]}\n      {value[2]}\n      {value[3]}\n      {value[4]}\n      {value[5]}\n      ELSE NULL\n      END AS risk_cat\n    ,risk_cat::disease_risk_enum AS risk_category\n    FROM \n      Physical ph \n    LEFT JOIN \n      AggregatedLights al\n    ON \n      al.id = ph.id\n    WHERE \n      {value[6]}\n    ORDER BY \n      al.id ASC;\n      \"\"\")\n    result = conn.sql(f\"SELECT COUNT(*) FROM {key}\").fetchone()\n    print(f\"created table: {key}, \\n\\tRow count: {result[0]}\\n\")\n  except:\n    print(f\"Error loading this table: {key}\")\n\n\ncreated table: Risk1, \n    Row count: 24\n\ncreated table: Risk2, \n    Row count: 1\n\ncreated table: Risk3, \n    Row count: 20\n\ncreated table: Risk4, \n    Row count: 2\n\n\nCombining SQL with Python data structures to organized data according to several conditions, such as: bmi, age, sex, and waist_circumference.\n\nconn.sql(f\"\"\"\nCREATE OR REPLACE TABLE \n  DiseaseRiskDemographic AS\nSELECT * EXCLUDE(risk_cat) FROM Risk1\nUNION BY NAME\nSELECT * EXCLUDE(risk_cat) FROM Risk2\nUNION BY NAME\nSELECT * EXCLUDE(risk_cat) FROM Risk3\nUNION BY NAME \nSELECT * EXCLUDE(risk_cat) FROM Risk4;\n\"\"\")\n\n\n\n\nIt might be useful to see what some descriptive statistics might look when grouped by risk categories.\n\n\n\n\nGrouped averaging by risk category.\nconn.sql(f\"\"\"\nCREATE OR REPLACE TABLE RiskCategorySummary AS\nSELECT \n  risk_category\n  ,AVG(sii::NUMERIC) AS sii\n  ,AVG(useHrs::INTEGER) AS useHrs\n  ,AVG(min_q1) AS 'q1'\n  ,AVG(q1_q2) AS 'q2'\n  ,AVG(q2_q3) AS 'q3'\n  ,AVG(q3_max) AS 'q4'\nFROM \n  DiseaseRiskDemographic\nGROUP BY \n  risk_category;\n\"\"\")\n\npydf = conn.sql(\"\"\"\nSELECT * \nFROM RiskCategorySummary;\n\"\"\").df()\n\n\n\n\n\n\ntable setup\nr_df &lt;- py$pydf\n\nnotes_list =  list(\"The disease risk category.\",\n                   \"Problematic behavior index.\",\n                   \"Reported interet usage (hours).\",\n                   \"12am to 6am\",\n                   \"6am to 12pm.\",\n                   \"12pm to 6pm\",\n                   \"6pm to 12am\")\n\nlocations_list = list(\"risk_category\"\n                      ,\"sii\"\n                      ,\"useHrs\"\n                      ,\"q1\"\n                      ,\"q2\"\n                      ,\"q3\"\n                      ,\"q4\")\n\n# Create a table dataframe, or tibble, of footnotes\nfootnotes_df &lt;- tibble(notes = notes_list, \n                       locations = locations_list)\n\npal_df &lt;- tibble(\n  cols = locations_list,\n  pals = list(eval_palette(\"harrypotter::ravenclaw\", 7, 'c', 1))\n  )\n\nrTable &lt;- r_table_theming(r_df,\n                          title = \"Disease Risk Categories to Lux\",\n                          subtitle = NULL,\n                          footnotes_df,\n                          source_note = md(\"**source**: Kaggle\"),\n                          pal_df,\n                          multiline_feet = FALSE,\n                          \n                          )\n\n\n\n\nTable 6 shows the grouped-averages of the problematic behavior index, internet use hours, and mean lux across the time of day quartiles by disease risk category of participants.\n\n\n\n\n\n\nTable 6: Disease risk categories by lux exposure.\n\n\n\n\n\n\n  \n    \n      Disease Risk Categories to Lux\n    \n    \n    \n      risk_category1\n      sii2\n      useHrs3\n      q14\n      q25\n      q36\n      q47\n    \n  \n  \n    Underweight\n0.2424242\n0.5757576\n13.330165\n66.33369\n115.48439\n13.228521\n    Normal\n0.5000000\n1.2000000\n35.113059\n61.25409\n90.38614\n30.387628\n    Increased\n0.0000000\n2.0000000\n6.729335\n36.00079\n81.13881\n8.073397\n    Very High\n0.5000000\n2.5000000\n23.488205\n35.47516\n65.59638\n19.614029\n    Extremely High\n0.0000000\n3.0000000\n20.271764\n73.00532\n117.68300\n24.962896\n  \n  \n    \n      source: Kaggle\n    \n  \n  \n    \n      \n        1 The disease risk category., 2 Problematic behavior index., 3 Reported interet usage (hours)., 4 12am to 6am, 5 6am to 12pm., 6 12pm to 6pm, 7 6pm to 12am\n      \n    \n  \n\n\n\n\n\n\n\n\n\n\n\nPivoting the data longer to accommodate more useful plot output.\nlibrary(tidyr)\nlibrary(dplyr)\n\ndata_long &lt;- r_df |&gt;\n  pivot_longer(\n    cols = c(q1, q2, q3, q4),\n    names_to = \"quarter\",\n    values_to = \"lux\") |&gt;\n  mutate(quarter = factor(quarter, \n                          levels = c(\"q1\",\n                                     \"q2\",\n                                     \"q3\",\n                                     \"q4\")))\n\n\n\n\n\n\ntable setup\nlocations_list = list(\"risk_category\", \"sii\", \"useHrs\", \"quarter\", \"lux\")\nnotes_list =  list(\"Disease risk based on BMI, waist circumference, and sex.\",\n                   \"Problematic behavior index.\",\n                   \"Hours of reported internet use.\",\n                   \"Quartile of day.\",\n                   \"Average light exposure.\")\n\n# Create a table dataframe, or tibble, of footnotes\nfootnotes_df &lt;- tibble(notes = notes_list, locations = locations_list)\n\npal_df &lt;- tibble(\n  cols = locations_list,\n  pals = list(eval_palette(\"harrypotter::ravenclaw\", 7, 'c', 1)))\n\n#tbl_font_size = pct(80))\nrTable &lt;- r_table_theming(data_long,\n                          title = \"Disease Risk Categories to Lux\",\n                          subtitle = \"Pivoted Longer\",\n                          footnotes_df,\n                          source_note = md(\"**source**: Kaggle\"),\n                          pal_df,\n                          multiline_feet = TRUE,\n                          tbl_font_size = pct(80)\n                          )\n\n\n\n\nTable 7 is a pivoted version of the previous table, Table 6.\n\n\n\n\n\n\nTable 7\n\n\n\n\n\n\n  \n    \n      Disease Risk Categories to Lux\n    \n    \n      Pivoted Longer\n    \n    \n      risk_category1\n      sii2\n      useHrs3\n      quarter4\n      lux5\n    \n  \n  \n    Underweight\n0.2424242\n0.5757576\nq1\n13.330165\n    Underweight\n0.2424242\n0.5757576\nq2\n66.333692\n    Underweight\n0.2424242\n0.5757576\nq3\n115.484385\n    Underweight\n0.2424242\n0.5757576\nq4\n13.228521\n    Normal\n0.5000000\n1.2000000\nq1\n35.113059\n    Normal\n0.5000000\n1.2000000\nq2\n61.254087\n    Normal\n0.5000000\n1.2000000\nq3\n90.386141\n    Normal\n0.5000000\n1.2000000\nq4\n30.387628\n    Increased\n0.0000000\n2.0000000\nq1\n6.729335\n    Increased\n0.0000000\n2.0000000\nq2\n36.000790\n    Increased\n0.0000000\n2.0000000\nq3\n81.138807\n    Increased\n0.0000000\n2.0000000\nq4\n8.073397\n    Very High\n0.5000000\n2.5000000\nq1\n23.488205\n    Very High\n0.5000000\n2.5000000\nq2\n35.475157\n    Very High\n0.5000000\n2.5000000\nq3\n65.596375\n    Very High\n0.5000000\n2.5000000\nq4\n19.614029\n    Extremely High\n0.0000000\n3.0000000\nq1\n20.271764\n    Extremely High\n0.0000000\n3.0000000\nq2\n73.005320\n    Extremely High\n0.0000000\n3.0000000\nq3\n117.683002\n    Extremely High\n0.0000000\n3.0000000\nq4\n24.962896\n  \n  \n    \n      source: Kaggle\n    \n  \n  \n    \n      1 Disease risk based on BMI, waist circumference, and sex.\n    \n    \n      2 Problematic behavior index.\n    \n    \n      3 Hours of reported internet use.\n    \n    \n      4 Quartile of day.\n    \n    \n      5 Average light exposure.\n    \n  \n\n\n\n\n\n\n\n\n\n\n\nplot setup\nr_plot &lt;- ggplot(data = data_long, \n                 aes(x = quarter, \n                     y = lux, \n                     group = risk_category, \n                     color = risk_category)) +\n  geom_line(linewidth = 1) +\n  geom_point(size = 3) +\n  labs(title = \"Is Disease risk related to light exposure\\n during certain times of the day?\",\n       x = \"Time Period\",\n       y = \"Mean Lux Value\",\n       color = \"Risk Category\")\n\nr_plot &lt;- r_plot + ggplot_theming()\n\n\n\nFigure 2 was an attempt at visualizing the relationship, if any, by mean lux exposure across participants, time of day quartile, and disease risk. Disease risk was based on BMI, waist-circumference, and sex data.\n\n\n\n\n\n\n\n\nFigure 2: Grouped risk by lux and time period."
  },
  {
    "objectID": "index.html#summary-profiles",
    "href": "index.html#summary-profiles",
    "title": "Problematic Internet Habits in Children",
    "section": "6 Summary Profiles",
    "text": "6 Summary Profiles\n\nIn the future, we might want to explore further, but for now we can save these descriptive statistics tables to files.\n\n\n\n\nCreate future training and testing data profiles from summary statistics to inform modeling choices.\nsummary_dict = {\n  'ActigraphyTrain': \n    ('ActigraphyTrainSummary'\n    ,'data/profiles/actigraphy_train_profile.csv'),\n  'ActigraphyTest': \n    ('ActigraphyTestSummary'\n    ,'data/profiles/actigraphy_test_profile.csv'),\n  'TrainCsv': \n    ('TrainCsvSummary'\n    ,'data/profiles/train_csv_profile.csv'),\n  'TestCsv': \n    ('TestCsvSummary'\n    ,'data/profiles/test_csv_profile.csv')\n    }\n\nsummary_frame = pd.DataFrame(summary_dict)\n\nfor key, value in summary_frame.items():\n  try:\n    py_summary_df = conn.sql(f\"SUMMARIZE SELECT * FROM '{key}';\").df()\n    conn.register(value[0], py_summary_df)\n    conn.sql(f\"COPY {value[0]} TO '{value[1]}' (HEADER , DELIMITER ',');\")\n    print(f\"'{value[1]}' was CREATED\\n\")\n  except:\n    print(f\"'{value[1]}' was not CREATED\\n\")\n\n\n&lt;duckdb.duckdb.DuckDBPyConnection object at 0x749db2be1870&gt;\n'data/profiles/actigraphy_train_profile.csv' was CREATED\n\n&lt;duckdb.duckdb.DuckDBPyConnection object at 0x749db2be1870&gt;\n'data/profiles/actigraphy_test_profile.csv' was CREATED\n\n&lt;duckdb.duckdb.DuckDBPyConnection object at 0x749db2be1870&gt;\n'data/profiles/train_csv_profile.csv' was CREATED\n\n&lt;duckdb.duckdb.DuckDBPyConnection object at 0x749db2be1870&gt;\n'data/profiles/test_csv_profile.csv' was CREATED"
  },
  {
    "objectID": "index.html#close-connection",
    "href": "index.html#close-connection",
    "title": "Problematic Internet Habits in Children",
    "section": "7 Close Connection",
    "text": "7 Close Connection\nAt the end of the analysis, we can safely close the in-memory database from RAM. The author of this analysis had 32GB of DDR4 RAM, originally with a set limit of 24GB used by the database.\n\n\n\nReleases all database memory and de-references the connection variable.\ntry:\n  conn.close()\n  print(f\"database connection closed\")\nexcept:\n  print(f\"could not close the connection\")\n\n\ndatabase connection closed"
  },
  {
    "objectID": "index.html#closing-thoughts",
    "href": "index.html#closing-thoughts",
    "title": "Problematic Internet Habits in Children",
    "section": "8 Closing Thoughts",
    "text": "8 Closing Thoughts\n\nThe purpose of the article was to explore a Kaggle dataset and improve workflow methodologies and output. It can be expanded to be more insightful for this particular domain, but that was not the goal. The objective has been achieved, as better outputting functions have been developed alongside improved CSS styling.\nThis particular dataset contained a lot of missing data, despite having come from large dataset. Due to this incomplete data across many of the columns,some of the data transformations resulted in as low as ~50 observations from the original ~1000. The leaderboard that came along with this dataset from a past competition suggests that the model accuracy tops out only around 0.48, Table 8. I believe, while some patterns could be gleaned, the predictive power of any modeling attempts might as well have been doomed.\n\n\n\n\nReads a recently downloaded the private leaderboard csv (from the competition website’s Leaderboard tab) to a dataframe.\nleaderboard_path = \"data/privateLeaderboard.csv\"\nleaderboard_df = pd.read_csv(leaderboard_path)\n\npy_df = leaderboard_df.head(n = 10)\n\n\n\n\n\n\ntable setup\nr_df &lt;- py$py_df\n\nnotes_list =  list(\"Overall ranking\",\n                   \"Unique team ID.\",\n                   \"Chosen name of the team.\",\n                   \"Their final submission date.\",\n                   \"The accuracy score of model.\",\n                   \"Total competition submissions.\",\n                   \"Names of the team members.\")\n\nlocations_list = list(\n  \"Rank\", \n  \"TeamId\", \n  \"TeamName\", \n  \"LastSubmissionDate\", \n  \"Score\", \n  \"SubmissionCount\", \n  \"TeamMemberUserNames\")\n\n# Create a table dataframe, or tibble, of footnotes\nfootnotes_df &lt;- tibble(notes = notes_list, \n                       locations = locations_list)\n\npal_df &lt;- tibble(\n  cols = locations_list,\n  pals = list(eval_palette(\"ggsci::legacy_tron\", 7, 'd', 1))\n  )\n\nrTable &lt;- r_table_theming(r_df,\n                          title = \"Leaderboard Rankings\",\n                          subtitle = NULL,\n                          footnotes_df,\n                          source_note = md(\"**source**: Kaggle\"),\n                          pal_df,\n                          multiline_feet = FALSE,\n                          target_everything = TRUE,\n                          color_by_columns = \"Rank\",\n                          tbl_font_size = pct(75)\n                          )\n\n\n\n\n\n\n\n\nTable 8\n\n\n\n\n\n\n  \n    \n      Leaderboard Rankings\n    \n    \n    \n      Rank1\n      TeamId2\n      TeamName3\n      LastSubmissionDate4\n      Score5\n      SubmissionCount6\n      TeamMemberUserNames7\n    \n  \n  \n    1\n12642715\nLennart Haupts\n2024-12-19 10:51:55\n0.482\n127\nlennarthaupts\n    2\n12640776\nAradhye Agarwal\n2024-09-21 16:43:26\n0.478\n2\naradhyeagarwal\n    3\n12795402\nJobayer Hossain\n2024-12-19 23:21:50\n0.478\n71\njobayerhossain\n    4\n13005648\nUnderfit Squad\n2024-12-19 14:49:07\n0.477\n43\ndungquang8229,minhduonq\n    5\n12659689\npeyman armaghan\n2024-12-19 23:22:02\n0.477\n40\npeymanarmaghan\n    6\n12732446\nm_furu\n2024-10-27 12:28:49\n0.476\n21\nmiyafuru\n    7\n12633225\nsqrt4kaido\n2024-12-19 13:41:18\n0.475\n32\nnomorevotch\n    8\n12730664\nKzk Knmt\n2024-12-18 15:05:28\n0.474\n26\nkzkknmt\n    9\n12637841\nSedat Golgiyaz-BingolUniversty\n2024-12-19 22:44:26\n0.473\n384\nsedatgolgyaz\n    10\n12653466\nJpmr\n2024-12-19 06:59:01\n0.473\n33\njunpeimorioka\n  \n  \n    \n      source: Kaggle\n    \n  \n  \n    \n      \n        1 Overall ranking, 2 Unique team ID., 3 Chosen name of the team., 4 Their final submission date., 5 The accuracy score of model., 6 Total competition submissions., 7 Names of the team members."
  }
]