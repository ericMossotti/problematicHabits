---
title: Problematic Internet Habits in Children
authors:
  - name: Eric Mossotti
    affiliation: None
    roles: data analysis
    corresponding: true



bibliography: bibliography/references.bib
citation-location: margin
citations-hover: true
link-citations: true
csl: bibliography/csl/apa.csl

code-fold: true

lightbox: auto
---

## Rationale and Initial Thoughts

The grounds for analyses in this case is the problematic behavior as it pertains to the subjects' experience and their parents' via questionnaires, physical fitness tests, worn instrument measurements and demographics. The resulting data is to be processed and connections drawn.

There are quite a few variables to consider here. The largest amount of data was collected in real time. This was recorded by a device that could measure acceleration in 3D space, ambient light, time, and the quarter of the year while the participant went about their lives. Many health issues could stem from a lack of exercise. It may follow that the more someone is actively using the internet, the less likely they are to meet daily activity recommendations.

Some issues is that it is uncertain where this feat of data collection took place. Perhaps, this was for the sake of privacy? The exact dates are not given for the real-time data either. So, we have to go by the quarter of the year and or season in that respect. Not sure if seasonal information is consequential to anything.

The goal of the models is to, ultimately, make progress in the field of childhood to adolescent psychology. Progress, in this case, is defined by accurately predicting risks of undesirable outcomes based on a young persons' daily activities.

Nowadays, life is centered around the internet. At the same time, our biology has not undergone any systemic changes. First hand experience certainly tells me that that high levels of internet activity can have a negative impact on the mind.

Is the quantity of internet activity itself truly determinant of negative outcome? I think this is extremely limited as a hypothesis. Internet activity is not the same in all cases.

### Output Formatters

#### Tables

```{r}
#| label: "r_table_theming()"
#| code-summary: "Relatively hard-core table formatting code."
# https://emilhvitfeldt.github.io/paletteer/
# https://gt.rstudio.com

library(gt)
library(purrr)
library(tibble)
library(reticulate)
library(paletteer)

# if (is.function(palette_name)) {return(palette_name(n, option)){}
eval_palette <- function(pal_name, n = 10, pal_type, direction = NULL) {
  if (pal_type == "c") {
    return(paletteer_c(pal_name, n, direction))
    } else if (pal_type == "d") {
      return(paletteer_d(pal_name, n, direction))
    } else if (pal_type == "dynamic") {
      return(paletteer_dynamic(pal_name, n, direction))
    }
}

r_table_theming <- function(r_df,
                            title,
                            subtitle,
                            footnotes_df,
                            source_note,
                            pal_df) {
  
  r_table <- gt(r_df)
  
  r_table <- seq_len(nrow(pal_df)) |>
    reduce(\(acc, i) {
      data_color(
        acc,
        palette = pal_df$pals[[i]],
        columns = pal_df$cols[[i]]
        #target_columns = gt::everything()
        )
      },.init = r_table)
  
  r_table <- r_table |>
    tab_header(title = title, subtitle = subtitle)
  
  r_table <- r_table |>
    tab_options(
      heading.background.color = '#222',
      column_labels.background.color = '#333',
      table.background.color = '#222',
      footnotes.background.color = '#222',
      source_notes.background.color = '#222',
      row.striping.include_table_body	= TRUE,
      row.striping.background_color = '#333'
      )
  
  r_table <- r_table |>
    tab_source_note(source_note = source_note)

  # Footnotes are added to the accumulator, building up to the final result
  r_table <- seq_len(nrow(footnotes_df)) |>
    reduce(\(acc, i) {
      tab_footnote(
        acc,
        footnote = footnotes_df$notes[[i]],
        location = cells_column_labels(columns = footnotes_df$locations[[i]]),
        placement = "auto"
        )
      }, .init = r_table)
  
  if(ncol(r_df) > 1) {
    
    cell_col_fills = pal_df$pals[[1]]
    
    r_table <- seq_len(nrow(pal_df)) |>
      reduce(\(acc, i) {
        tab_style(
          acc,
          style = cell_fill(
            color = cell_col_fills[i]
            ), locations = cells_column_labels(columns = pal_df$cols[[i]]))
        }, .init = r_table)
  }

  return(r_table)
  }

```

#### Plots

```{r}
#| label: "ggplot_theming()"
#| code-summary: "Hard-core plot theme code."

library(reticulate)
library(ggplot2)

# NORMAL AXES
ggplot_theming <- function() {
  theme_linedraw() +
    theme(
      plot.title = element_text(color = 'white'),
      plot.background = element_rect(fill = '#222'),
      panel.background = element_rect(fill = '#222'),
      panel.grid.major.x = element_line(linetype = 'solid'),
      panel.grid.minor.x = element_line(linetype = "dotted"),
      panel.grid.major.y = element_line(linetype = 'dashed'),
      panel.grid.minor.y = element_line(linetype = 'dotted'),
      axis.title = element_text(color = 'gray100'),
      axis.text = element_text(color = 'gray'),
      legend.background = element_rect(fill = '#222'),
      legend.text = element_text(color = 'gray'),
      legend.title = element_text(color = 'white'))
  }

# FLIPPED AXES
ggplot_theming_flipped_axes <- function() {
  theme_linedraw() +
    theme(
      plot.title = element_text(color = 'white'),
      plot.background = element_rect(fill = '#222'),
      panel.background = element_rect(fill = '#222'),
      panel.grid.major.x = element_line(linetype = 'dashed'),
      panel.grid.minor.x = element_line(linetype = "dotted"),
      panel.grid.major.y = element_line(linetype = 'solid'),
      panel.grid.minor.y = element_line(linetype = 'dotted'),
      axis.title = element_text(color = 'gray100'),
      axis.text = element_text(color = 'gray'),
      legend.background = element_rect(fill = '#222'),
      legend.text = element_text(color = 'gray'),
      legend.title = element_text(color = 'white'))
}

```

## Import/Extract

```{python}
#| label: "mass import"
#| eval: false
!kaggle competitions download -c child-mind-institute-problematic-internet-use
```

```{python}
#| label: "makeDir"
#| code-summary: "Check if the directory, 'data', exists."
import os

try:
  os.mkdir('./data')
except FileExistsError:
  print(f"data exists")
```

```{python}
#| label: tbl-unzipping
#| tbl-cap: "unzippers"
#| code-summary: "Extracts the files and lists files."

from zipfile import ZipFile
import pandas as pd

zip_path = "child-mind-institute-problematic-internet-use.zip"

if not os.path.exists('./data/train.csv'):
  with ZipFile(zip_path) as zf:
    ZipFile.extractall(zf, path = './data')
    print("Extracted files", sep = "\n"*2)
else:
  print("Files already exist", sep = "\n"*2)

extracts = pd.DataFrame(
  data = os.listdir(path = './data'), columns = ['./data'])

```

```{r}

r_df <- py$extracts

notes_list =  list("directory contents")
locations_list = list("./data")

# Create a table dataframe, or tibble, of footnotes
footnotes_df <- tibble(notes = notes_list, locations = locations_list)

pal_df <- tibble(
  cols = list(
  "./data"),
  pals = list(
    eval_palette("ggsci::legacy_tron", 7, 'd', 1)
    )
  )

rTable <- r_table_theming(r_df,
                          title = "Directory Files",
                          subtitle = NULL,
                          footnotes_df,
                          source_note = md("**source**: '/data/'"),
                          pal_df)

rTable
```

### Establish a Database Connection

```{python}
import duckdb


conn = duckdb.connect(':memory:')
conn.sql(f"SET memory_limit = '24GB';")
conn.sql(f"SET default_order = 'ASC';")
```

### New Data Types (Enums/Enumerations)

I noticed that using ENUM data types significantly sped up many database operations [@holanda_duckdb_2021].

```{python}
#| label: "enum_series_to_dict"
#| code-summary: "Defines the series and dictionary objects for enum functions."

import pandas as pd

enums = ['internet_hours_enum'
         ,'enroll_season_enum'
         ,'disease_risk_enum'
         ,'sii_enum', 'age_enum'
         ,'sex_enum'
         ,'pciat_season_enum'
         ,'weekday_enum'
         ,'quarter_enum'
         ,'hour_enum'
         ,'minute_enum'
         ,'second_enum'
         ,'id_actigraphy_enum']

#siiSeries = pd.Series(data = ['None', 'Mild', 'Moderate', 'Severe'], dtype = str)
siiSeries = pd.Series(data = ['0', '1', '2', '3'], dtype = str)
ageSeries = pd.Series(data = range(5, 23), dtype = str)
sexSeries = pd.Series(data = ['0', '1'], dtype = str)
pciatSeasonSeries = pd.Series(data = ['Fall'
                                      ,'Spring'
                                      ,'Summer'
                                      ,'Winter'], dtype = str)

internetHrsSeries = pd.Series(data = ['0', '1', '2', '3'], dtype = str)
quarterSeries = pd.Series(data = range(1, 5), dtype = str)
weekdaySeries = pd.Series(data = range(1, 8), dtype = str)
hourSeries = pd.Series(data = range(0, 24), dtype = str)
minuteSeries = pd.Series(data = range(0, 60), dtype = str)
secondSeries = pd.Series(data = range(0, 60), dtype = str)
diseaseRiskSeries = pd.Series(data = ['Underweight'
                                      ,'Normal'
                                      ,'Increased'
                                      ,'High'
                                      ,'Very High'
                                      ,'Extremely High'], dtype = str)

id_df = conn.execute(f"""
     SELECT 
         DISTINCT(id) AS id
     FROM 
         read_parquet('data/series_train*/*/*'
         ,hive_partitioning = true)
     ORDER BY 
         id ASC;
     """).df() 

idList = id_df['id'].to_list()
idSeries = pd.Series(data = idList, dtype = str)

enumDict = {
  'disease_risk_enum': f"{tuple(diseaseRiskSeries)}"
  ,'enroll_season_enum': f"{tuple(pciatSeasonSeries)}"
  ,'sii_enum': f"{tuple(siiSeries)}"
  ,'age_enum': f"{tuple(ageSeries)}"
  ,'sex_enum': f"{tuple(sexSeries)}"
  ,'pciat_season_enum': f"{tuple(pciatSeasonSeries)}"
  ,'quarter_enum': f"{tuple(quarterSeries)}"
  ,'weekday_enum': f"{tuple(weekdaySeries)}"
  ,'hour_enum': f"{tuple(hourSeries)}"
  ,'minute_enum': f"{tuple(minuteSeries)}"
  ,'second_enum': f"{tuple(secondSeries)}"
  ,'id_actigraphy_enum': f"{tuple(idSeries)}"
  ,'internet_hours_enum': f"{tuple(internetHrsSeries)}"
  }

if len(enumDict) == len(enums):
  print(f"drops and creates are same length")
else:
  print(f"drops and creates are not the same length")
```

```{python}
#| label: "try_create_drop()"
#| code-summary: "Can there be a modular approach to creating and dropping ENUM types?\n \tYes."

def try_create(conn,type_str: str, enum_str: str) -> pd.Series:
  """
  :::WHY(s):::
    Is there a modular approach to creating ENUM types in a database?
  :::HOW(s):::
    Utilize python data structures such as pandas Series and dictionary as parameters.
  """
  # the try, except can help categorize the outputs for ui/ux
  try:
    conn.execute(f"CREATE TYPE {type_str} AS ENUM {enum_str};")
    return pd.Series(type_str, index = ['created'])
  except duckdb.duckdb.CatalogException:
    return pd.Series(type_str, index = ['already existed'])

def try_drop(conn, type_str: str) -> pd.Series:
  """
  :::WHY(s):::
    Is there a modular approach to dropping ENUM types in a database?
  :::HOW(s):::
    Utilize python data structures such as pandas Series and dictionary as parameters.
  """
  # the try, except can help categorize the outputs for ui/ux
  try:
    conn.execute(f"DROP TYPE {type_str};")
    return pd.Series(type_str, index = ['dropped'])
  except duckdb.duckdb.CatalogException:
    return pd.Series(type_str, index = ['did not exist'])
  
droplist = []
for e in enums:   
   droplist.append(try_drop(conn, type_str = e))
   
dropFrame = pd.DataFrame(droplist)
dropFrame = dropFrame.sort_values(by = dropFrame.columns.to_list(),
                                  ascending = True,
                                  ignore_index = True)
createList = []
for eType, eSeries in enumDict.items():
    createList.append(try_create(conn,
                                 type_str = eType,
                                 enum_str = eSeries))
createFrame = pd.DataFrame(createList)
createFrame = createFrame.sort_values(by = createFrame.columns.to_list(),
                                      ascending = True,
                                      ignore_index = True)

pd.concat([dropFrame, createFrame], axis = 1)
```

Not sure why the data had hyphens in the columns names, originally. These hyphens interfere with SQL query operations, so I have the mind to replace them with underscore characters. I'm not going to overwrite the original CSVs as I wish to maintain integrity and a clear trail of data processing steps. Also, having the original data to fall back on if need be, without re-downloading could be helpful.

### Meaningful Database Pipeline Setup

Doing my best to apply transformations as I read the data into the database from the files to minimize steps needed to derive meaning from the data.

```{python}
#| label: "setupPipeline"
#| code-summary: "Does the extraction and initial transformation following download of compressed files."

def setup_duckdb_pipeline(
  csvDict: dict, 
  parquetDict: dict, 
  conn: duckdb.DuckDBPyConnection) -> None:
    
  try:
    {
      table_name: duckdb.sql(f"""
      CREATE OR REPLACE TABLE {table_name} AS 
      SELECT 
        *
      FROM 
        df;
      """, connection = conn) 
      for table_name, df in csvDict.items()
      }
    for key, value in csvDict.items():
      result = conn.execute(f"SELECT COUNT(*) FROM {key}").fetchone()
      print(f"Successfully created table: {key}, Row count: {result[0]}")
  except Exception as e:
    print(f"Error loading table: {str(e)}")
    raise
  
  if parquetDict:
    write_datasets(conn, parquetDict)
  
# Create tables from Parquet files
def write_datasets (
  conn: duckdb.DuckDBPyConnection, parquetDict: dict):
    
      try:
        {
          table_name: duckdb.sql(f"""
           CREATE OR REPLACE TABLE {table_name} AS
           SELECT 
             id::id_actigraphy_enum AS id
             ,quarter::TEXT::quarter_enum AS quarter
             ,weekday::TEXT::weekday_enum AS weekday
             ,light
             ,(time_of_day / 3_600_000_000_000) AS hour_of_day
             ,relative_date_PCIAT
           FROM read_parquet(
             '{file_path}'
             ,hive_partitioning = true
             );""", connection=conn)
           for table_name, file_path in parquetDict.items()
           }
        for key, value in parquetDict.items():
          result = conn.execute(f"SELECT COUNT(*) FROM {key}").fetchone()
          print(
            f"Successfully created table: {key}, Row count, {result[0]}")
      except Exception as e:
        print(f"Error writing dataset: {str(e)}")
        raise

trainCsvDf = pd.read_csv("data/train.csv")
testCsvDf = pd.read_csv("data/test.csv")

dictDf = pd.read_csv("data/data_dictionary.csv")

trainCsvDf.columns = trainCsvDf.columns.str.replace('-','_') 
trainCsvDf.columns = trainCsvDf.columns.str.lower() 
testCsvDf.columns = testCsvDf.columns.str.replace('-','_') 
testCsvDf.columns = testCsvDf.columns.str.lower() 

dictDf.Field = dictDf.Field.replace("-", "_", regex = True)

csvDict = {
  "TrainCsv": trainCsvDf
  ,"TestCsv": testCsvDf
  ,"DataDict": dictDf
  }

parquetDict = {
  "ActigraphyTrain": 'data//series_train.parquet*/*/*'
  ,"ActigraphyTest": 'data//series_test*/*/*'
  }

try:
  setup_duckdb_pipeline(csvDict, parquetDict, conn)
  conn.sql(f"CHECKPOINT;")
except:
  print(f"Could not set up data pipeline.")
  
```

```{python}

pydf = conn.sql(f"""
SELECT *
FROM ActigraphyTrain
ORDER BY 
  id ASC
  ,relative_date_PCIAT ASC
  ,hour_of_day ASC
LIMIT 10;""").df()
```

```{r}
#| label: tbl-ActigraphyTrain


r_df <- py$pydf

r_df <- r_df |>
  dplyr::select(id, quarter, weekday, relative_date_PCIAT, light, hour_of_day)

notes_list =  list(
  "Unique participant identifier code, to track related data from actigraphy to hbn datasets.",
  "Annual quarter related to month of observation.",
  "Day of the week (Monday = 1)",
  "Light exposure, measured in 'lux'",
  "24-hour time converted to a float for easier conversion for time-series analysis.",
  "Days since questionairre, also convenient to help order daily values for time-series analysis."
  )

locations_list = list("id",
                      "quarter",
                      "weekday",
                      "light",
                      "hour_of_day",
                      "relative_date_PCIAT")

# Create a table data-frame, or tibble, of footnotes
footnotes_df <- tibble(
  notes = notes_list, 
  locations = locations_list
  )

pal_df <- tibble(
  cols = list(
    'id', 
    'quarter', 
    'weekday', 
    'relative_date_PCIAT', 
    'light', 
    'hour_of_day'),
  pals = list(
    eval_palette("harrypotter::hufflepuff", 20, 'c', 1),
    eval_palette("harrypotter::hufflepuff", 20, 'c', 1),
    eval_palette("harrypotter::hufflepuff", 20, 'c', 1),
    eval_palette("harrypotter::hufflepuff", 20, 'c', 1),
    eval_palette("harrypotter::hufflepuff", 20, 'c', 1),
    eval_palette("harrypotter::hufflepuff", 20, 'c', 1)
    )
  )

rTable <- r_table_theming(r_df,
                          title = "Actigraphy Data - Training Set",
                          subtitle = NULL,
                          footnotes_df,
                          source_note = md("**source**: Kaggle"),
                          pal_df
                          )

rTable
```

### Overview

```{python}
info_tables = conn.sql(f"Select * From main.duckdb_tables;").df()
```

```{python}
#| label: "summaryProfiles"
#| code-summary: "Create training and testing data profiles from summary statistics to better inform modeling choices."

summary_dict = {
  'ActigraphyTrain': 
    ('ActigraphyTrainSummary'
    ,'data/profiles/actigraphy_train_profile.csv'),
  'ActigraphyTest': 
    ('ActigraphyTestSummary'
    ,'data/profiles/actigraphy_test_profile.csv'),
  'TrainCsv': 
    ('TrainCsvSummary'
    ,'data/profiles/train_csv_profile.csv'),
  'TestCsv': 
    ('TestCsvSummary'
    ,'data/profiles/test_csv_profile.csv')
    }

summary_frame = pd.DataFrame(summary_dict)

for key, value in summary_frame.items():
  
  py_summary_df = conn.execute(
    f"SUMMARIZE SELECT * FROM '{key}';").df()
    
  conn.register(value[0], py_summary_df)

  conn.sql(
    f"COPY {value[0]} TO '{value[1]}' (HEADER , DELIMITER ',');")  

```

Might be useful: Visualize summary data

```{python}

pydf = conn.sql(f"""
SELECT *
FROM ActigraphyTrainSummary;""").df()
pydf
```

```{r}
library(tibble)
library(harrypotter)
library(paletteer)

r_df <- py$pydf

r_df <- r_df |>
  dplyr::select(id, quarter, weekday, relative_date_PCIAT, light, hour_of_day)

notes_list =  list(
  "Unique participant identifier code, to track related data from actigraphy to hbn datasets.",
  "Annual quarter related to month of observation.",
  "Day of the week (Monday = 1)",
  "Light exposure, measured in 'lux'",
  "24-hour time converted to a float for easier conversion for time-series analysis.",
  "Days since questionairre, also convenient to help order daily values for time-series analysis."
  )

locations_list = list("id",
                      "quarter",
                      "weekday",
                      "light",
                      "hour_of_day",
                      "relative_date_PCIAT")

# Create a table data-frame, or tibble, of footnotes
footnotes_df <- tibble(
  notes = notes_list, 
  locations = locations_list
  )

pal_df <- tibble(
  cols = list(
    'id', 
    'quarter', 
    'weekday', 
    'relative_date_PCIAT', 
    'light', 
    'hour_of_day'),
  pals = list(
    eval_palette("harrypotter::hufflepuff", 20, 'c', 1),
    eval_palette("harrypotter::hufflepuff", 20, 'c', 1),
    eval_palette("harrypotter::hufflepuff", 20, 'c', 1),
    eval_palette("harrypotter::hufflepuff", 20, 'c', 1),
    eval_palette("harrypotter::hufflepuff", 20, 'c', 1),
    eval_palette("harrypotter::hufflepuff", 20, 'c', 1)
    )
  )

rTable <- r_table_theming(r_df,
                          title = "Actigraphy Data - Training Set",
                          subtitle = NULL,
                          footnotes_df,
                          source_note = md("**source**: Kaggle"),
                          pal_df
                          )

rTable

```

## Machine Learning Models

Key Parameters in lgb_params

Since `sii` is a multi-class categorical variable, the task is a classification problem. Adjusting the `lgb_params`, evaluation metrics, and handling predictions as outlined above ensures the model correctly interprets and solves the problem.

**Categorical target (sii)**:

When the target is discrete (as in the case for sii, or, 'Severity Impairment Index'), it indicates a classification problem. A classification objective and corresponding evaluation metrics (e.g., accuracy, log loss) might be appropriate.

1.  **`objective`: `'multiclass'`**

    -   Specifies the learning objective of the model.

    -   `'multiclass'` is used when the target variable is categorical with more than two classes.

    -   Since the target variable (`sii`) has four possible values (0, 1, 2, 3), this is the appropriate choice.

2.  **`metric`: `'multi_logloss'`**

    -   Metric used to evaluate the model's performance during training.

    -   `'multi_logloss'` (Multi-class Logarithmic Loss) measures how well the predicted probability distributions match the actual class labels.

    -   Lower values indicate better performance. It's commonly used for classification tasks.

3.  **`num_class`: `4`**

    -   Specifies the number of unique classes in the target variable.

    -   Since `sii` has four possible values (0, 1, 2, 3), `num_class` is set to `4`.

4.  **`num_leaves`**

    -   **Purpose**: Determines the complexity of the trees.

    -   **Value**: `31`

        -   Higher values allow the model to capture more interactions between features but can lead to overfitting if too high.

        -   A smaller value may be used for simpler problems or to prevent overfitting.

5.  **`learning_rate`**

    -   **Purpose**: The step size for updating model weights during training.

    -   **Value**: `0.05`

        -   A smaller value makes the model learn slowly, often improving generalization.

        -   If one lowers `learning_rate`, they may need to increase `num_boost_round` (number of training iterations).

6.  **`feature_fraction`**

    -   **Purpose**: The proportion of features to randomly sample for each iteration of training.

    -   **Value**: `0.9`

        -   Prevents overfitting by forcing the model to rely on different subsets of features.

        -   A value close to 1 means most features are used, while smaller values increase regularization.

7.  **`bagging_fraction`**

    -   **Purpose**: The proportion of training samples to randomly sample for each iteration of training.

    -   **Value**: `0.8`

        -   Similar to `feature_fraction`, but applied to the training data.

        -   Helps the model generalize better by using only a subset of the data.

8.  **`bagging_freq`**

    -   **Purpose**: The frequency (in terms of iterations) at which bagging (random sampling) is applied.

    -   **Value**: `5`

        -   Bagging is applied every 5 iterations.

        -   A value of `0` disables bagging.

9.  **`verbose`**

    -   **Purpose**: Controls the verbosity of the training process.

    -   **Value**: `-1`

        -   Suppresses output unless there's an error.

        -   A value of `1` or `2` provides more detailed output during training.

```{python}

model_out = f"./data/model_output"

try:
  os.mkdir(model_out)
except FileExistsError:
  print(f"data/model_output exists")

```

```{python}

sii_model_out = f"./data/model_output/sii_model"

try:
  os.mkdir(sii_model_out)
except FileExistsError:
  print(f"data/model_output/sii_model exists")

```

### New ML

```{python}
#| label: "MLpipeline"
#| code-summary: "Statistical modeling with machine learning models."

import duckdb
import numpy as np
import pandas as pd
from sklearn.model_selection import TimeSeriesSplit
from sklearn.metrics import mean_squared_error, accuracy_score, log_loss
import lightgbm as lgb
import joblib
import json
from typing import Dict, Any, Tuple
import random

# Set random seeds for reproducibility
SEED = 14
np.random.seed(SEED)
random.seed(SEED)

# Function to load and merge data
def load_and_merge_data(
  conn: duckdb.DuckDBPyConnection, 
  dataset_type: str) -> pd.DataFrame:
    """
    Load, aggregate, and merge data for a specified dataset type 
    ('train' or 'test').
    """
    if dataset_type == 'train':
        query = """
        WITH TrainBase AS (
            SELECT id,
                   COALESCE(AVG(basic_demos_age::FLOAT), 0) AS age,
                   COALESCE(AVG(cgas_cgas_score::FLOAT), 0) AS cgas,
                   COALESCE(AVG(physical_bmi::FLOAT), 0) AS bmi,
                   COALESCE(AVG(physical_height::FLOAT), 0) AS height,
                   COALESCE(AVG(physical_weight::FLOAT), 0) AS weight,
                   COALESCE(AVG(physical_heartrate::FLOAT), 0) AS heartrate,
                   COALESCE(MAX(sii::INTEGER), 0) AS sii -- Target column
            FROM TrainCsv
            GROUP BY id
        ),
        TrainActivity AS (
            SELECT id,
                   COALESCE(AVG(light::FLOAT), 0) AS light_mean,
                   COALESCE(STDDEV(light::FLOAT), 0) AS light_std,
                   COALESCE(MAX(light::FLOAT), 0) AS light_max,
                   COALESCE(AVG(hour_of_day::FLOAT), 0) AS hour_mean,
                   COALESCE(STDDEV(hour_of_day::FLOAT), 0) AS hour_std,
                   COALESCE(MODE(weekday::TEXT::INTEGER), 0) AS common_weekday,
                   COALESCE(MODE(quarter::TEXT::INTEGER), 0) AS common_quarter
            FROM ActigraphyTrain
            GROUP BY id
        )
        SELECT *
        FROM TrainBase
        LEFT JOIN TrainActivity USING (id);
        """
    elif dataset_type == 'test':
        query = """
        WITH TestBase AS (
            SELECT id,
                   COALESCE(AVG(basic_demos_age::FLOAT), 0) AS age,
                   COALESCE(AVG(cgas_cgas_score::FLOAT), 0) AS cgas,
                   COALESCE(AVG(physical_bmi::FLOAT), 0) AS bmi,
                   COALESCE(AVG(physical_height::FLOAT), 0) AS height,
                   COALESCE(AVG(physical_weight::FLOAT), 0) AS weight,
                   COALESCE(AVG(physical_heartrate::FLOAT), 0) AS heartrate
            FROM TestCsv
            GROUP BY id
        ),
        TestActivity AS (
            SELECT id,
                   COALESCE(AVG(light::FLOAT), 0) AS light_mean,
                   COALESCE(STDDEV(light::FLOAT), 0) AS light_std,
                   COALESCE(MAX(light::FLOAT), 0) AS light_max,
                   COALESCE(AVG(hour_of_day::FLOAT), 0) AS hour_mean,
                   COALESCE(STDDEV(hour_of_day::FLOAT), 0) AS hour_std,
                   COALESCE(MODE(weekday::TEXT::INTEGER), 0) AS common_weekday,
                   COALESCE(MODE(quarter::TEXT::INTEGER), 0) AS common_quarter
            FROM ActigraphyTest
            GROUP BY id
        )
        SELECT *
        FROM TestBase
        LEFT JOIN TestActivity USING (id);
        """
    else:
        raise ValueError("Invalid dataset_type. Must be 'train' or 'test'.")
    
    return conn.execute(query).df()

# Data preparation function
def prepare_data(
  conn: duckdb.DuckDBPyConnection, 
  dataset_type: str, 
  target_col: str = None) -> Tuple[pd.DataFrame, pd.DataFrame, np.ndarray]:
    """
    Prepare the data for training or testing.
    """
    df = load_and_merge_data(conn, dataset_type)
    X = df.drop(columns = ['id', target_col], errors = 'ignore')
    y = df[target_col].values if target_col and target_col in df else None
    return df, X, y

# Cross-validation function
def cross_validate(
  conn: duckdb.DuckDBPyConnection, 
  target_col: str, 
  lgb_params: Dict[str, Any], 
  n_splits: int = 5) -> Tuple[Dict, Dict]:
  """
  Perform time-series cross-validation for LightGBM.
  """
  _, X, y = prepare_data(conn, 'train', target_col)

  tss = TimeSeriesSplit(n_splits = n_splits)
  cv_results = {
    'train_logloss': [], 
    'val_logloss': [], 
    'train_accuracy': [], 
    'val_accuracy': []
    }

  for train_idx, val_idx in tss.split(X):
    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]
    y_train, y_val = y[train_idx], y[val_idx]

    train_data = lgb.Dataset(X_train, label = y_train)
    val_data = lgb.Dataset(X_val, label = y_val)

    model = lgb.train(
      lgb_params,
      train_set = train_data,
      valid_sets = [train_data, val_data],
      valid_names = ['train', 'valid'],
      num_boost_round = 100
      #early_stopping_rounds=20,
     # verbose_eval=False
       )

    train_preds = model.predict(X_train).argmax(axis = 1)
    val_preds = model.predict(X_val).argmax(axis = 1)

    cv_results['train_logloss'].append(
      log_loss(y_train, model.predict(X_train)))
    
    cv_results['val_logloss'].append(
      log_loss(y_val, model.predict(X_val)))
    
    cv_results['train_accuracy'].append(
      accuracy_score(y_train, train_preds))
    
    cv_results['val_accuracy'].append(
      accuracy_score(y_val, val_preds))

  cv_summary = {
    'avg_train_logloss': np.mean(cv_results['train_logloss']),
    'avg_val_logloss': np.mean(cv_results['val_logloss']),
    'avg_train_accuracy': np.mean(cv_results['train_accuracy']),
    'avg_val_accuracy': np.mean(cv_results['val_accuracy'])
  }

  return cv_results, cv_summary

# Training function
def train_final_model(
  conn: duckdb.DuckDBPyConnection, 
  target_col: str, 
  lgb_params: Dict[str, Any]) -> lgb.Booster:
  """
  Train the final model on the full training data.
  """
  _, X, y = prepare_data(conn, 'train', target_col)
  
  train_data = lgb.Dataset(X, label = y)
  
  model = lgb.train(
    lgb_params, train_set = train_data, num_boost_round = 100)
  
  return model

# Testing function
def evaluate_test_data(
  conn: duckdb.DuckDBPyConnection, 
  model: lgb.Booster, 
  target_col: str) -> pd.DataFrame:
  """
  Evaluate the model on test data.
  """
  df, X_test, _ = prepare_data(conn, 'test', target_col)
  df['predictions'] = model.predict(X_test).argmax(axis = 1)
  
  return df

# Saving the model
def save_model(
  model: lgb.Booster, 
  lgb_params: Dict[str, Any], 
  path: str):
  """
  Save the trained LightGBM model and parameters.
  """
  model.save_model(f"{path}_model.txt")
  with open(f"{path}_params.json", 'w') as f:
    json.dump(lgb_params, f)

# Example usage
lgb_params = {
  'objective': 'multiclass',
  'metric': 'multi_logloss',
  'num_class': 4,
  'boosting_type': 'gbdt',
  'num_leaves': 31,
  'learning_rate': 0.05,
  'feature_fraction': 0.9,
  'bagging_fraction': 0.8,
  'bagging_freq': 5,
  'verbose': -1,
  'seed': SEED # Fixed seed for LightGBM
  }

#conn = duckdb.connect()
target_col = 'sii'

cv_results, cv_summary = cross_validate(conn, target_col, lgb_params)
  
print("Cross-validation summary:", cv_summary)

final_model = train_final_model(conn, target_col, lgb_params)

# Save the model
save_model(final_model, lgb_params, path = "data/model_output/final_lightgbm")

test_results = evaluate_test_data(conn, final_model, target_col)

print(test_results.head())

```

```{python}
test_results_renamed = test_results.sort_values(by = 'id')
test_results_renamed["id"]
```

```{python}
test_results_renamed = test_results_renamed.rename(columns={"predictions":"sii"})
test_results_renamed[["id", "sii"]]
```

```{python}
test_results_renamed = test_results_renamed.reset_index()
test_results_renamed[["id", "sii"]]
```

```{python}
submission = pd.read_csv("data/sample_submission.csv")
submission
```

```{python}
submission['id'] == test_results_renamed['id']
```

```{python}
submission['sii'] = test_results_renamed['sii']
submission
```

```{python}

submission.to_csv(
  "submission.csv", index = False)
  
```

## Parameter Tables

```{python}
#| label: "create_table_with_regex_columns()"
#| warning: false

import re

def filter_columns_by_regex(col_dict: dict, regex_pattern: str) -> dict:
  """
  :::WHY(s):::
    Can there be a way to use dictionaries to specify columns?

  :::GOAL(s):::
    Regex selection of column names from a dictionary.
  """
  return {
    col: dtype 
    for col, dtype in col_dict.items() 
    if re.search(regex_pattern, col)
    }

def create_table_with_regex_columns(
  conn: duckdb.duckdb
  ,source_table: str
  ,new_table_name: str 
  ,regex_pattern: str 
  ,col_dict: dict
  ) -> None:  

  """
  :::WHY(s):::
    There should to be a more streamlined way to utilize columnar information following regular string patterns to generate tables based on such information. This could follow and carry out pre-existing data modeling plans.

  :::GOAL(s):::
    To create new database tables with modularized SQL queries generated with the help of regex pattern matching selection.

  :::EXAMPLE(s):::
    The dictionary item:
      "Demographic": r"^id|^sii|^basic\S+"
 
    Would generate a table named 'Demographic' from data with columns for 'id', 'sii' and all columns starting with 'basic' strings. 
  """

  # filter columns
  filtered_col_dict = filter_columns_by_regex(col_dict, regex_pattern)
  
  # regex column selecting via dict comprehension and vectorized filtering
  regex_select_sql = f"""
  CREATE OR REPLACE TABLE {new_table_name} AS 
  SELECT
    {', '.join([f'"{col}"' for col in filtered_col_dict.keys()])}
  FROM {source_table};
  """

  conn.execute(regex_select_sql)

# It'd be useful to get the data types for creating a new table of values
coltype_overview = conn.execute(f"""
  SELECT column_name
         ,data_type
  FROM 
    information_schema.columns
  WHERE 
    table_name = 'TrainCsv';""").df()

# Map the column names with data types
col_dict = dict(zip(coltype_overview.column_name, coltype_overview.data_type))

regex_dict_train = {
  "Demographic": r"^id|^sii|^basic\S+"
  ,"Physical": r"^id|^sii|^physical\S+"
  ,"FgVital": r"^id|^sii|^fitness_E\S+"
  ,"FgChild": r"^id|^sii|^fgc\S+"
  ,"Bia": r"^id|^sii|^bia\S+"
  ,"Paqa": r"^id|^sii|^paq_a\S+"
  ,"Pciat": r"^id|^sii|^pciat\S+"
  ,"Sds": r"^id|^sii|^sds\S+"
  ,"InternetUse": r"^id|^sii|^preint\S+"
  }

# There's no Pciat in the test set
regex_dict_test = {
  "DemographicTest": r"^id|^basic\S+"
  ,"PhysicalTest": r"^id|^physical\S+"
  ,"FgVitalTest": r"^id|^fitness_E\S+"
  ,"FgChildTest": r"^id|^fgc\S+"
  ,"BiaTest": r"^id|^bia\S+"
  ,"PaqaTest": r"^id|^paq_a\S+"
 # ,"Pciat_OfTest": r"^id|^pciat\S+" 
  ,"SdsTest": r"^id|^sds\S+"
  ,"InternetUseTest": r"^id|^preint\S+"
  }

# Loop through the data structures to create tables for the train set
for new_table_name, regex_pattern in regex_dict_train.items():
  create_table_with_regex_columns(
    conn 
    ,'TrainCsv'
    ,new_table_name
    ,regex_pattern
    ,col_dict
    ) 

# Loop through the data structures to create tables for the test set
for new_table_name, regex_pattern in regex_dict_test.items():
  create_table_with_regex_columns(
    conn
    ,'TestCsv'
    ,new_table_name
    ,regex_pattern 
    ,col_dict
    )

conn.sql(f"DROP TABLE TrainCsv;")
conn.sql(f"DROP TABLE TestCsv;")

```

```{python}
#| label: "joinDemoAndActigraphyOnId"
#| code-summary: "Perform a join on matching Ids to combine demographics data with actigraphy data such as time and light exposure."

conn.execute(f"""
CREATE OR REPLACE TABLE IntermediateActigraphy AS
SELECT id
       ,basic_demos_enroll_season::TEXT::enroll_season_enum AS enroll_season
       ,basic_demos_age::TEXT::age_enum AS age
       ,basic_demos_sex::TEXT AS sex
       ,sii::INTEGER::TEXT::sii_enum AS sii
FROM 
  Demographic
ORDER BY
  id ASC;
""")

conn.execute(f"""
CREATE OR REPLACE TABLE 
  ActigraphyTrain
AS
  SELECT
    ia.*
    ,at.hour_of_day
    ,at.light
  FROM 
    ActigraphyTrain at 
  LEFT JOIN 
    IntermediateActigraphy ia
  ON 
    ia.id = at.id;
""")

conn.execute(f"""
CREATE OR REPLACE TABLE IntermediateActigraphy AS
SELECT
  id
  ,basic_demos_enroll_season::TEXT::enroll_season_enum AS enroll_season
  ,basic_demos_age::TEXT::age_enum AS age
  ,basic_demos_sex::TEXT AS sex
FROM 
  DemographicTest
ORDER BY
  id ASC;
""")

conn.execute(f"""
CREATE OR REPLACE TABLE 
  ActigraphyTest
AS
  SELECT
    ia.*
    ,at.hour_of_day
    ,at.light
  FROM 
    ActigraphyTest at 
  LEFT JOIN 
    IntermediateActigraphy ia
  ON 
    ia.id = at.id;
""")

conn.execute(f"""
DROP TABLE IntermediateActigraphy;
""")

conn.execute(f"CHECKPOINT;")

```

### Data Model Schematic

::: column-screen-inset
```{mermaid}
%%| label: fig-childMermaid
%%| fig-cap: "child"
%%| file: diagrams/emermaid.mmd
```
:::

## Actigraphy, Physical, and Demographics

It might be useful to explore how the time of day and associated exposure to light might correlate to other factors, such as sii, physical characteristics, and internet use.

**Hypothesis**:\
More time spent indoors on the internet might be associated with lower light exposure during daytime and higher light exposure during nighttime.

**Test**:

**Day**

-   Q1_Q2

-   Q2_Q3

**Night**

-   min_Q1

-   Q3_max

BMI Risk Categories [@zierle-ghosh_physiology_2024].

Lux Exposure Categories [@noauthor_lux_nodate].

### Quartile of Lux by Hour of Day

```{python}
#| label: "intermediateLighter()"
#| code-summary: "Extract actigraphy data based on time of day quartiles"

def quartiler(
  conn: duckdb.duckdb, 
  col_name: str, 
  source_name: str) -> dict:
  """
  ::WHY(s):: 
    SQL can sometimes require a lot of code for repetitive commands, but in a Python environment, database queries can be modularized.
  
  ::GOAL(s)::  
    To process SQL queries into useful quartile information represented by intuitive key labels.
  """
  summaryDf = conn.sql(f"""
  SUMMARIZE
  SELECT
    {col_name}
  FROM 
    {source_name};""").df()

  quartileDict = {
    'min': summaryDf['min'][0]
    ,'Q1': summaryDf.q25[0]
    ,'Q2': summaryDf.q50[0]
    ,'Q3': summaryDf.q75[0]
    ,'max': summaryDf['max'][0]
    }
  
  return quartileDict


def siiDetect (detect_frame: pd.DataFrame) -> bool:
  if 'sii' in detect_frame.column_name.values:
    return True

def intermediateLighter(
  conn: duckdb.duckdb,
  new_tables: list,
  quarters: dict, 
  quartuples: pd.Series,
  from_table: str) -> None:
  """
  :::WHY(s):::
    Tables based on the same parameters but different parameter values could be modularized.
    
  :::GOAL(s):::
    Process repetitive SQL efficiently and intuitively using data structures that simplify the process. 
  
  """
  detect_frame = conn.execute(f"""
  SELECT *
  FROM 
    information_schema.columns
  WHERE 
    table_name = '{from_table}';
  """).df()
  
  for i in list(range(4)):
    if siiDetect(detect_frame) == True:
      conn.sql(f"""
        CREATE OR REPLACE TABLE {new_tables[i]} AS
        SELECT
          id
          ,enroll_season
          ,age
          ,sex
          ,sii
          ,AVG(light) AS {quartuples.index[i]}
        FROM 
          '{from_table}'
        WHERE 
          hour_of_day BETWEEN 
              {quarters[quartuples.iloc[i][0]]}::DOUBLE 
            AND 
              {quarters[quartuples.iloc[i][1]]}::DOUBLE
        GROUP BY 
          ALL
        ORDER BY 
          id ASC;""")
    else:
      conn.sql(f"""
        CREATE OR REPLACE TABLE {new_tables[i]} AS
        SELECT
          id
          ,enroll_season
          ,age
          ,sex
          ,AVG(light) AS {quartuples.index[i]}
        FROM 
          '{from_table}'
        WHERE 
          hour_of_day BETWEEN 
              {quarters[quartuples.iloc[i][0]]}::DOUBLE 
            AND 
              {quarters[quartuples.iloc[i][1]]}::DOUBLE
        GROUP BY 
          ALL
        ORDER BY 
          id ASC;""")


def joinLights (from_to_tables: dict) -> None:
  
  for from_table, to_table in from_to_tables.items():
    
    new_tables = ['Light1', 'Light2', 'Light3', 'Light4']
    
    quarters = quartiler(conn, 'hour_of_day', from_table)

    intermediateLighter(conn, new_tables, quarters, quartuples, from_table)
  
    #conn.sql(f"CHECKPOINT;")
  
    conn.sql(f"""
    CREATE OR REPLACE TABLE {to_table} AS
    SELECT 
      l1.*
      ,l2.q1_q2
      ,l3.q2_q3
      ,l4.q3_max
    FROM 
      Light1 l1
    LEFT JOIN Light2 l2 ON l1.id = l2.id
    LEFT JOIN Light3 l3 ON l1.id = l3.id
    LEFT JOIN Light4 l4 ON l1.id = l4.id;
    """)
    
    for table in new_tables:
      conn.sql(f"DROP TABLE {table};")
      

quartuples = pd.Series(
  data = 
  [('min','Q1')
  ,('Q1', 'Q2')
  ,('Q2', 'Q3')
  ,('Q3' ,'max')]
  ,index =
  ['min_q1'
  ,'q1_q2'
  ,'q2_q3'
  ,'q3_max'])

from_to_tables = {
  'ActigraphyTrain': 'AggregatedLights'
  ,'ActigraphyTest': 'AggregatedLightsTest'
  }

joinLights(from_to_tables)

```

```{python}
info_tables = conn.sql(f"Select * From information_schema.tables").df()
```

```{python}
conn.sql(f"CHECKPOINT;")

pydf = conn.sql(f"SELECT * FROM AggregatedLights LIMIT 10;").df()
```

```{python}

lightdropList = ("Light1", "Light2", "Light3", "Light4")

for lightdrop in lightdropList:
  conn.sql(f"DROP TABLE {lightdrop};")
  
```

```{r}
#| label: tbl-AggregatedLights

r_df <- py$pydf

notes_list =  list(
  "Unique participant identifier.",
  "Participant enrollment into study season.",
  "Age in years.",
  "Sex (0 = Female, 1 = Male)",
  "Severity Impairment Index",
  "Mean light exposure across Q1 of a 24 hour time scale.",
  "Mean light exposure across Q2 of a 24 hour time scale.",
  "Mean light exposure across Q3 of a 24 hour time scale.",
  "Mean light exposure across Q4 of a 24 hour time scale."
  )

locations_list = list(
  "id"
  ,"enroll_season"
  ,"age"
  ,"sex"
  ,"sii"
  ,'min_q1'
  ,'q1_q2'
  ,'q2_q3'
  ,'q3_max')

# Create a table data-frame, or tibble, of footnotes
footnotes_df <- tibble(notes = notes_list, 
                       locations = locations_list)

rTable <- r_table_theming(rDataFrame = r_df,
                          title = "Quartile Data - Lights",
                          subtitle = NULL,
                          footnotes_df,
                          source_note = md("**source**: Kaggle"))

rTable

```

```{python}

lightsDict = {
  'AggregatedLights': 'InternetUse', 
  'AggregatedLightsTest': 'InternetUseTest'
  }

for from_tbl, join_tbl in lightsDict.items():
  conn.sql(f"""
  CREATE OR REPLACE TABLE '{from_tbl}' AS
  SELECT
    ft.*
    ,jt.preint_eduhx_computerinternet_hoursday::INTEGER::TEXT::internet_hours_enum AS useHrs 
  FROM
    '{from_tbl}' ft
    LEFT JOIN 
      {join_tbl} jt
    ON 
      ft.id = jt.id;
  """)
```

Checking the internet use hours parameter to get an overview along with null percentage to see if we might be able to use it, as-is, for joining with light quartiles.

```{python}

all_al = conn.sql(f"SELECT * FROM AggregatedLightsTest;").df()
pyDescribe = all_al.describe()

duckSummary = conn.sql(f"""
SUMMARIZE
SELECT *
FROM AggregatedLightsTest;
""").df()

duckSummary = duckSummary.set_index('column_name')

pyDescribe, duckSummary

```

Group-averaging sii, internet use hours, and pre-averaged lux quartiles by age and sex.

```{python}

#CREATE OR REPLACE TABLE DayQuartiles AS
pydf = conn.sql(f"""
SELECT
  useHrs
  ,COUNT(*) AS observations
  ,AVG(sii::INTEGER) AS sii
  ,AVG(min_q1) AS 'q1'
  ,AVG(q1_q2) AS 'q2'
  ,AVG(q2_q3) AS 'q3'
  ,AVG(q3_max) AS 'q4'
FROM 
  AggregatedLights al
GROUP BY 
  useHrs;""").df()

obsSum = pydf.observations.sum()
pydf = pydf.dropna()
```

```{r}
#| label: tbl-lightUseHrs

r_df <- py$pydf

notes_list =  list("Hours spent on the internet, as reported by caregivers of the participants.", "Problematic behavior score.")
locations_list = list("useHrs", "sii")

# Create a table dataframe, or tibble, of footnotes
footnotes_df <- tibble(notes = notes_list, locations = locations_list)

rTable <- r_table_theming(rDataFrame = r_df,
                          title = "Problematic Behavior to Internet Usage",
                          subtitle = NULL,
                          footnotes_df,
                          source_note = md("**source**: Kaggle"))

rTable
```

```{r}
#| label: "fig-agex"
library(scales)

r_plot <- ggplot(data = r_df, 
                 mapping = aes(x = useHrs, y = sii)
                 ) +
  geom_col(aes(fill = sii)) +
  scale_fill_gradient(low = muted("yellow"), high = muted("purple")) +
  labs(title = 'Risk Categories', 
       x = 'Hours of Internet Use', 
       y = "Problematic Behavior Index (sii)")

r_plot + ggplot_theming()
```

```{python}

riskyDictionary = {
  'Risk1': 
    (f",CASE WHEN ph.physical_bmi < 18.5 THEN 'Underweight'"
    ,f"WHEN ph.physical_bmi BETWEEN 18.5 AND 24.9 THEN 'Normal'"
    ,f"WHEN ph.physical_bmi BETWEEN 25.0 AND 29.9 THEN 'Increased'"
    ,f"WHEN ph.physical_bmi BETWEEN 30.0 AND 34.9 THEN 'High'"
    ,f"WHEN ph.physical_bmi BETWEEN 35.0 AND 39.9 THEN 'Very High'"
    ,f"WHEN ph.physical_bmi >= 40 THEN 'Extremely High'"
    ,f"ph.physical_waist_circumference <= 35 AND al.sex = '0'"),
  'Risk2':
    (f",CASE WHEN ph.physical_bmi < 18.5 THEN 'Underweight'"
    ,f"WHEN ph.physical_bmi BETWEEN 18.5 AND 24.9 THEN 'Normal'"
    ,f"WHEN ph.physical_bmi BETWEEN 25.0 AND 29.9 THEN 'High'"
    ,f"WHEN ph.physical_bmi BETWEEN 30.0 AND 34.9 THEN 'Very High'"
    ,f"WHEN ph.physical_bmi BETWEEN 35.0 AND 39.9 THEN 'Very High'"
    ,f"WHEN ph.physical_bmi >= 40 THEN 'Extremely High'"
    ,f"ph.physical_waist_circumference > 35 AND al.sex = '0'"),
  'Risk3':
    (f",CASE WHEN ph.physical_bmi < 18.5 THEN 'Underweight'"
    ,f"WHEN ph.physical_bmi BETWEEN 18.5 AND 24.9 THEN 'Normal'"
    ,f"WHEN ph.physical_bmi BETWEEN 25.0 AND 29.9 THEN 'Increased'"
    ,f"WHEN ph.physical_bmi BETWEEN 30.0 AND 34.9 THEN 'High'"
    ,f"WHEN ph.physical_bmi BETWEEN 35.0 AND 39.9 THEN 'Very High'"
    ,f"WHEN ph.physical_bmi >= 40 THEN 'Extremely High'"
    ,f"ph.physical_waist_circumference <= 40 AND al.sex = '1'"),
  'Risk4':
    (f",CASE WHEN ph.physical_bmi < 18.5 THEN 'Underweight'"
    ,f"WHEN ph.physical_bmi BETWEEN 18.5 AND 24.9 THEN 'Normal'"
    ,f"WHEN ph.physical_bmi BETWEEN 25.0 AND 29.9 THEN 'High'"
    ,f"WHEN ph.physical_bmi BETWEEN 30.0 AND 34.9 THEN 'Very High'"
    ,f"WHEN ph.physical_bmi BETWEEN 35.0 AND 39.9 THEN 'Very High'"
    ,f"WHEN ph.physical_bmi >= 40 THEN 'Extremely High'"
    ,f"ph.physical_waist_circumference > 40 AND al.sex = '1'")
    }

riskyDf = pd.DataFrame(data = riskyDictionary)

for key, value in riskyDf.items():
  try:
    conn.sql(f"""
    CREATE OR REPLACE TABLE {key} AS
    SELECT
      al.*
      {value[0]}
      {value[1]}
      {value[2]}
      {value[3]}
      {value[4]}
      {value[5]}
      ELSE NULL
      END AS risk_cat
    ,risk_cat::disease_risk_enum AS risk_category
    FROM 
      Physical ph 
    LEFT JOIN 
      AggregatedLights al
    ON 
      al.id = ph.id
    WHERE 
      {value[6]}
    ORDER BY 
      al.id ASC;
      """)
    result = conn.execute(f"SELECT COUNT(*) FROM {key}").fetchone()
    print(f"Successfully created table: {key}, Row count: {result[0]}")
  except:
    print(f"Error loading this table: {key}")

conn.execute(f"""
CREATE OR REPLACE TABLE 
  DiseaseRiskDemographic AS
SELECT * EXCLUDE(risk_cat) FROM Risk1
UNION BY NAME
SELECT * EXCLUDE(risk_cat) FROM Risk2
UNION BY NAME
SELECT * EXCLUDE(risk_cat) FROM Risk3
UNION BY NAME 
SELECT * EXCLUDE(risk_cat) FROM Risk4;
""")

conn.execute(f"CHECKPOINT")
```

```{python}
#| include: false
#| eval: false

tablesInfo = conn.sql(f"SELECT * FROM information_schema.tables;").df()
```

```{python}

conn.execute(f"""
CREATE OR REPLACE TABLE RiskCategorySummary AS
SELECT 
  risk_category
  ,AVG(sii::NUMERIC) AS sii
  ,AVG(useHrs::INTEGER) AS useHrs
  ,AVG(min_q1) AS 'q1'
  ,AVG(q1_q2) AS 'q2'
  ,AVG(q2_q3) AS 'q3'
  ,AVG(q3_max) AS 'q4'
FROM 
  DiseaseRiskDemographic
GROUP BY 
  risk_category;
""")

pydf = conn.execute(f"""
SELECT * FROM RiskCategorySummary;
""").df()

```

```{python}

def engineer_light_features(pydf):
  """
  Create engineered features from light exposure data
  """
  features = pydf.copy()
  
  # Light exposure ratios
  features['morning_afternoon_ratio'] = (features['q2'] / features['q3'])
  
  features['day_night_ratio'] = (features['q2'] + features['q3']) / (features['q1'] + features['q4'])
  
  # Daily variation
  light_cols = ['q1', 'q2', 'q3', 'q4']

  features['light_variance'] = features[light_cols].var(axis = 1)
  
  features['light_range'] = (features[light_cols].max(axis = 1) - features[light_cols].min(axis = 1))
  
  # Interactions with useHrs
  for col in light_cols:
    features[f'useHrs_{col}_interaction'] = features['useHrs'] * features[col]

  return features

```

```{python}
from sklearn.preprocessing import StandardScaler

def scale_features(X_train, X_test = None):
  """
  Scale features using StandardScaler
  """
  scaler = StandardScaler()
  X_train_scaled = pd.DataFrame(
      scaler.fit_transform(X_train),
      columns = X_train.columns,
      index = X_train.index
  )
  
  if X_test is not None:
      X_test_scaled = pd.DataFrame(
          scaler.transform(X_test),
          columns = X_test.columns,
          index = X_test.index
      )
      return X_train_scaled, X_test_scaled
  
  return X_train_scaled

```

```{python}
from sklearn.model_selection import cross_val_score, GridSearchCV
from sklearn.metrics import mean_squared_error
from mord import LogisticIT

def train_ordinal_model(X_train, y_train):
  """
  Train ordinal regression model with grid search
  """
  model = LogisticIT()
  
  param_grid = {
    'alpha': [0.01, 0.1, 1.0, 10.0],
    'max_iter': [100, 200, 500]
    }
  
  grid_search = GridSearchCV(
    model,
    param_grid,
    cv = 5,
    scoring = 'neg_mean_squared_error',
    n_jobs = -1,
    verbose = 1
  )
  
  grid_search.fit(X_train, y_train)
  
  return grid_search

```

```{python}

def evaluate_ordinal_model(model, X, y):
    """
    Evaluate model performance with multiple metrics
    """
    # Predictions
    y_pred = model.predict(X)
    
    # Calculate metrics
    mse = mean_squared_error(y, y_pred)
    exact_match = np.mean(y_pred == y)
    one_off = np.mean(np.abs(y_pred - y) <= 1)
    
    # Cross-validation scores
    cv_scores = cross_val_score(
        model,
        X,
        y,
        cv = 5,
        scoring='neg_mean_squared_error'
    )
    
    results = {
        'mse': mse,
        'rmse': np.sqrt(mse),
        'cv_mse_mean': -cv_scores.mean(),
        'cv_mse_std': cv_scores.std(),
        'exact_match_accuracy': exact_match,
        'one_off_accuracy': one_off
    }
    
    return results

```

```{python}

pydf = conn.sql(f"""
SELECT
  useHrs::INTEGER AS useHrs
  ,sii::TEXT::INTEGER AS sii
  ,min_q1 AS 'q1'
  ,q1_q2 AS 'q2'
  ,q2_q3 AS 'q3'
  ,q3_max AS 'q4'
FROM 
  AggregatedLights;""").df()

pydf = pydf.dropna()

```

```{python}

# 1. Load and prepare data
X = pydf[['useHrs', 'q1', 'q2', 'q3', 'q4']]

y = pydf['sii']

# 2. Engineer features
X_engineered = engineer_light_features(X)

# 3. Scale features
X_scaled = scale_features(X_engineered)

# 4. Train model
model = train_ordinal_model(X_scaled, y)
print(f"Best parameters: {model.best_params_}")

# 5. Evaluate model
results = evaluate_ordinal_model(model.best_estimator_, X_scaled, y)

for metric, value in results.items():
    print(f"{metric}: {value:.4f}")

```

```{python}

pydf = conn.sql(f"""
SELECT
  useHrs::INTEGER AS useHrs
  ,min_q1 AS 'q1'
  ,q1_q2 AS 'q2'
  ,q2_q3 AS 'q3'
  ,q3_max AS 'q4'
FROM 
  AggregatedLightsTest;""").df()

pydf = pydf.dropna()

```

```{python}

X = pydf[['useHrs', 'q1', 'q2', 'q3', 'q4']]

# Make predictions on new data
X_test_engineered = engineer_light_features(X)

X_test_scaled = scale_features(X_engineered, X_test_engineered)

predictions = model.predict(X_test_scaled)

predictions

```

```{r}
#| label: tbl-riskefe

r_df <- py$pydf

notes_list =  list("The disease risk category.",
                   "12am to 6am",
                   "6am to 12pm.",
                   "12pm to 6pm",
                   "6pm to 12am")

locations_list = list("risk_category"
                      ,"q1"
                      ,"q2"
                      ,"q3"
                      ,"q4")

# Create a table dataframe, or tibble, of footnotes
footnotes_df <- tibble(notes = notes_list, 
                       locations = locations_list)

rTable <- r_table_theming(rDataFrame = r_df,
                          title = "Disease Risk Categories to Lux",
                          subtitle = NULL,
                          footnotes_df,
                          source_note = md("**source**: Kaggle"))

rTable

```

add_glance_table

```{r}
library(tidyr)
library(ggplot2)
library(dplyr)

data_long <- r_df |>
  pivot_longer(
    cols = c(q1, q2, q3, q4),
    names_to = "quarter",
    values_to = "lux") |>
  mutate(quarter = factor(quarter, 
                          levels = c("q1",
                                     "q2",
                                     "q3",
                                     "q4")))

data_long
```

```{r}
#| label: ""

r_plot <- ggplot(data = data_long, 
                 aes(x = quarter, 
                     y = lux, 
                     group = risk_category, 
                     color = risk_category)) +
  geom_line(linewidth = 1) +
  geom_point(size = 3) +
  labs(title = "Light Exposure by Risk Category and Time Period",
       x = "Time Period",
       y = "Mean Lux Value",
       color = "Risk Category")

r_plot + ggplot_theming()

```

## Close Connection

```{python}
#| eval: true
try:
  conn.close()
  print(f"database connection closed")
except:
  print(f"could not close the connection")
```

```{python}
pydf = conn.sql(f"""
SELECT
  *
FROM 
  AggregatedLightsTest
LIMIT
  10;""").df()
  
pydf
```
