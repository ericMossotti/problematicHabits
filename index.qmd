---
title: Problematic Internet Habits in Children
authors:
  - name: Eric Mossotti
    affiliation: None
    roles: data analysis
    corresponding: true
    
abstract-title: "Objective"
abstract: "Explore a Kaggle dataset with an in-memory, analytical SQL database. Utilize a 'best-of-both-worlds' hybrid Python and R environment."

bibliography: bibliography/references.bib
citation-location: margin
citations-hover: true
link-citations: true
csl: bibliography/csl/apa.csl

code-fold: true
code-overflow: wrap

lightbox: auto
fig-responsive: true

table-of-contents: true
toc-location: left
number-sections: true

link-external-newwindow: true
---

## Rationale and Initial Thoughts

The grounds for analyses in this case is the problematic behavior as it pertains to the subjects' experience and their parents' via questionnaires, physical fitness tests, worn instrument measurements and demographics. The resulting data is to be processed and connections drawn. [@santorelli_child_2024]

### Output Notes

#### Palettes

A notable and comprehsive R library was used to give the tables and plots some styling. [@hvitfeldt_paletteer_2021]

#### Tables

This is a means to apply formatting to every column in a table using a desired paletteer palette if desired using the gt tables. Table output were customized to a high degree. Principles stemming from functional programming, in particular, are implemented here as they seem to be most appropriate to achieve the desired effect. [@iannone_gt_2025]

::: code-fold-flex
```{r}
#| label: "r_table_theming()"
#| code-summary: "Hard-core table formatting code."
#| warning: false

library(gt)
library(purrr)
library(tibble)
library(reticulate)
library(paletteer)

eval_palette <- function(pal_name, n = 10, pal_type, direction = NULL) {
  if (pal_type == "c") {
    return(paletteer_c(pal_name, n, direction))
    } else if (pal_type == "d") {
      return(paletteer_d(pal_name, n, direction))
    } else if (pal_type == "dynamic") {
      return(paletteer_dynamic(pal_name, n, direction))
    }
  }

r_table_theming <- function(r_df,
                            title,
                            subtitle,
                            footnotes_df,
                            source_note,
                            pal_df,
                            multiline_feet = NULL,
                            tbl_font_size = NULL,
                            color_by_columns = NULL,
                            do_col_labels = FALSE,
                            target_everything = FALSE
                            ) {
  
  r_table <- gt(r_df)

  if(nrow(r_df) > 1 && target_everything == FALSE) {
    r_table <- seq_len(nrow(pal_df)) |>
    reduce(\(acc, i) {
      data_color(
        acc,
        palette = pal_df$pals[[i]],
        columns = pal_df$cols[[i]]
        )
    },.init = r_table)
  } 
  else if(nrow(r_df) > 1 && target_everything == TRUE) {
    r_table <- seq_len(nrow(pal_df)) |>
    reduce(\(acc, i) {
      data_color(
        acc,
        columns = color_by_columns,
        palette = pal_df$pals[[i]],
        target_columns = everything()
        )
    },.init = r_table)
  }
  
  r_table <- r_table |>
    tab_header(title = title, subtitle = subtitle)
  
  r_table <- r_table |>
    tab_options(
      column_labels.padding = px(10),
      column_labels.font.weight = "bold",
      column_labels.background.color = '#333',
      #column_labels.font.size = pct(115),
      column_labels.border.top.width = px(0),
      column_labels.border.bottom.color = 'black',
      column_labels.vlines.width = px(1),
      column_labels.border.lr.width = px(1),
      column_labels.border.bottom.width = px(0),
      column_labels.border.lr.color = 'black',
      column_labels.vlines.color = 'black',
      #container.width = pct(99),
      footnotes.padding = px(5),
      footnotes.background.color = '#222',
      footnotes.sep = ", ",
      footnotes.multiline = multiline_feet,
      heading.padding = px(20),
      heading.background.color = '#222',
      heading.title.font.size = pct(125),
      heading.subtitle.font.size = pct(110),
      heading.border.bottom.width = px(0),
      row.striping.include_table_body	= TRUE,
      row.striping.background_color = '#333',
      source_notes.background.color = '#222',
      table.margin.left = px(1),
      table.margin.right = px(1),
      table.align = "center",
      table.border.top.width = px(0),
      table.border.bottom.width = px(0),
      table.background.color = '#222',
      table.font.size = tbl_font_size,
      table.layout = "auto",
      table_body.hlines.color = 'black',
      table_body.hlines.width = px(0),
      table_body.vlines.width = px(0),
      table_body.border.bottom.color = 'black',
      table_body.border.top.color = 'black',
      table_body.border.bottom.width = px(0),
      table_body.border.top.width = px(0)
     # quarto.use_bootstrap = TRUE
      )
  
  r_table <- r_table |>
    tab_source_note(source_note = source_note)
  
  # Footnotes are added to the accumulator, building up to the final result
  r_table <- seq_len(nrow(footnotes_df)) |>
    reduce(\(acc, i) {
      tab_footnote(
        acc,
        footnote = footnotes_df$notes[[i]],
        location = cells_column_labels(columns = footnotes_df$locations[[i]]),
        placement = "auto"
        )
      }, .init = r_table)
  
  if(ncol(r_df) > 1 && do_col_labels == TRUE) {
      cell_col_fills = pal_df$pals[[1]]
      
      r_table <- seq_len(nrow(pal_df)) |>
        reduce(\(acc, i) {
          tab_style(
            acc,
            style = cell_fill(
              color = cell_col_fills[i]
              ), locations = cells_column_labels(columns = pal_df$cols[[i]]))
        }, .init = r_table)
  }
  
  return(r_table)
  }
```
:::

#### Plots

::: p-1
Here are some relatively simpler functions developed with the intention to append and apply consistent theming to ggplots. R libraries are currently preferred for plotting outputs as they can be extensively customized. [@wickham_ggplot2_2016]
:::

::: code-fold-flex
```{r}
#| label: "ggplot_theming()"
#| code-summary: "Soft-core plot theme code."

library(reticulate)
library(ggplot2)

# NORMAL AXES
ggplot_theming <- function() {
  theme_minimal() +
    theme(
      plot.title = element_text(color = 'white',
                                size = rel(1.5)),
      plot.background = element_rect(fill = '#222'),
      panel.background = element_rect(fill = '#222'),
      panel.grid.major.x = element_line(linetype = 'solid',
                                        color = 'black'),
      panel.grid.minor.x = element_line(linetype = "dotted",
                                        color = 'gray'),
      panel.grid.major.y = element_line(linetype = 'dashed',
                                        color = 'black'),
      panel.grid.minor.y = element_line(linetype = 'dotted',
                                        color = 'gray'),
      axis.title = element_text(color = 'gray100'),
      axis.text = element_text(color = 'gray'),
      legend.background = element_rect(fill = '#222'),
      legend.text = element_text(color = 'gray'),
      legend.title = element_text(color = 'white'))
  }

# FLIPPED AXES
ggplot_theming_flipped_axes <- function() {
  theme_minimal() +
    theme(
     # borders(color = "#222"),
      plot.title = element_text(color = 'white',
                                size = rel(2)),
      plot.background = element_rect(fill = '#222'),
      panel.background = element_rect(fill = '#222'),
      panel.grid.major.x = element_line(linetype = 'dashed'),
      panel.grid.minor.x = element_line(linetype = "dotted"),
      panel.grid.major.y = element_line(linetype = 'solid'),
      panel.grid.minor.y = element_line(linetype = 'dotted'),
      axis.title = element_text(color = 'gray100'),
      axis.text = element_text(color = 'gray'),
      legend.background = element_rect(fill = '#222'),
      legend.text = element_text(color = 'gray'),
      legend.title = element_text(color = 'white'))
}

```
:::

## Import/Extract

The data was imported from Kaggle. It only needed to be done once, so the code chunk's eval setting is set to false during most of this project's development. 

::: code-fold-flex
```{python}
#| label: "mass import"
#| code-summary: "Executed once to import the data from the Kaggle server. "
#| eval: false
!kaggle competitions download -c child-mind-institute-problematic-internet-use
```
:::

Although not necessary, creating the data directory can be made programmatic. 

::: code-fold-flex
```{python}
#| label: "makeDir"
#| code-summary: "Checked if the directory, 'data', exists."

import os

try:
  os.mkdir('./data')
except FileExistsError:
  print(f"data exists")
```
:::

:::p-1
Exracting the downloaded files was also done programmatically, but it is not necessary. It did seem useful to visualize the data directory following extraction to verify the files and folders that now exist in the ./data directory of this project.
:::

::: code-fold-flex
```{python}
#| label: "extractUnzip"
#| code-summary: "Extracted the compressed files and list files."

from zipfile import ZipFile
import pandas as pd

zip_path = "child-mind-institute-problematic-internet-use.zip"

if not os.path.exists('./data/train.csv'):
  with ZipFile(zip_path) as zf:
    ZipFile.extractall(zf, path = './data')
    print("Extracted files", sep = "\n"*2)
else:
  print("Files already exist", sep = "\n"*2)

extracts = pd.DataFrame(
  data = os.listdir(path = './data'), columns = ['./data'])

```
:::

::: code-fold-flex
```{r}
#| label: "setupDataDirectory"
#| code-summary: "table setup"

r_df <- py$extracts

notes_list =  list("directory contents")
locations_list = list("./data")

# Create a table dataframe, or tibble, of footnotes
footnotes_df <- tibble(notes = notes_list, locations = locations_list)

pal_df <- tibble(
  cols = list(
  "./data"),
  pals = list(
    eval_palette("ggsci::legacy_tron", 7, 'd', 1)
    )
  )

rTable <- r_table_theming(r_df,
                          title = "Directory Files",
                          subtitle = NULL,
                          footnotes_df,
                          source_note = md("**source**: '/data/'"),
                          pal_df,
                          tbl_font_size = pct(80))

```
:::

::: table-flex
```{r}
#| label: "tbl-dataDirectory"
#| tbl-cap: "Files and folders"
#| echo: false
rTable
```
:::

## Establish Connection

:::p-1
An in-memory, DuckDB database connection was established for this project. The analysis in this project heavily relies on SQL syntax, and most of the computations are performed in database for efficiency.
:::

::: code-fold-flex
```{python}
#| label: "establishConnection"
#| code-summary: "Set up an in-memory database."
import duckdb

conn = duckdb.connect(':memory:')
```
:::

### Customize Database Settings

::: code-fold-flex
```{python}
#| label: "setMemoryLimit"
#| code-summary: "Set the database memory limit."
try:
  conn.sql(f"SET memory_limit = '24GB';")
  print(f"Successfully changed memory limit to 24GB")
except:
  print(f"An error occurred in changing the memory limit.")
```
:::

::: code-fold-flex
```{python}
#| label: "setOrder"
#| code-summary: "Set the default ordering for the database."
try:
  conn.sql(f"SET default_order = 'ASC';")
  print(f"Successfully changed the default order to ascending.")
except:
  print(f"The default order could not be changed.")
```
:::

## Setup Database

### Create New Data Types

:::p-1
Enum data types significantly sped up many database operations. Enums are similar to the category data type in Python, and the factor datatype in R. [@holanda_duckdb_2021]
:::

::: code-fold-flex
```{python}
#| label: "enum_series_to_dict"
#| code-summary: "Data structures setup for new data types."
import pandas as pd

enums = ['internet_hours_enum'
         ,'enroll_season_enum'
         ,'disease_risk_enum'
         ,'sii_enum', 'age_enum'
         ,'sex_enum'
         ,'pciat_season_enum'
         ,'weekday_enum'
         ,'quarter_enum'
         ,'hour_enum'
         ,'minute_enum'
         ,'second_enum'
         ,'id_actigraphy_enum']

#siiSeries = pd.Series(data = ['None', 'Mild', 'Moderate', 'Severe'], dtype = str)
siiSeries = pd.Series(data = ['0', '1', '2', '3'], dtype = str)
ageSeries = pd.Series(data = range(5, 23), dtype = str)
sexSeries = pd.Series(data = ['0', '1'], dtype = str)
pciatSeasonSeries = pd.Series(data = ['Fall'
                                      ,'Spring'
                                      ,'Summer'
                                      ,'Winter'], dtype = str)

internetHrsSeries = pd.Series(data = ['0', '1', '2', '3'], dtype = str)
quarterSeries = pd.Series(data = range(1, 5), dtype = str)
weekdaySeries = pd.Series(data = range(1, 8), dtype = str)
hourSeries = pd.Series(data = range(0, 24), dtype = str)
minuteSeries = pd.Series(data = range(0, 60), dtype = str)
secondSeries = pd.Series(data = range(0, 60), dtype = str)
diseaseRiskSeries = pd.Series(data = ['Underweight'
                                      ,'Normal'
                                      ,'Increased'
                                      ,'High'
                                      ,'Very High'
                                      ,'Extremely High'], dtype = str)

id_df = conn.execute(f"""
     SELECT 
         DISTINCT(id) AS id
     FROM 
         read_parquet('data/series_train*/*/*'
         ,hive_partitioning = true)
     ORDER BY 
         id ASC;
     """).df() 

idList = id_df['id'].to_list()
idSeries = pd.Series(data = idList, dtype = str)

enumDict = {
  'disease_risk_enum': f"{tuple(diseaseRiskSeries)}"
  ,'enroll_season_enum': f"{tuple(pciatSeasonSeries)}"
  ,'sii_enum': f"{tuple(siiSeries)}"
  ,'age_enum': f"{tuple(ageSeries)}"
  ,'sex_enum': f"{tuple(sexSeries)}"
  ,'pciat_season_enum': f"{tuple(pciatSeasonSeries)}"
  ,'quarter_enum': f"{tuple(quarterSeries)}"
  ,'weekday_enum': f"{tuple(weekdaySeries)}"
  ,'hour_enum': f"{tuple(hourSeries)}"
  ,'minute_enum': f"{tuple(minuteSeries)}"
  ,'second_enum': f"{tuple(secondSeries)}"
  ,'id_actigraphy_enum': f"{tuple(idSeries)}"
  ,'internet_hours_enum': f"{tuple(internetHrsSeries)}"
  }

if len(enumDict) == len(enums):
  print(f"drops and creates are same length")
else:
  print(f"drops and creates are not the same length")
```
:::

:::p-1
This is designed to manage ENUM types in a DuckDB database in a modular and user-friendly way. It provides functions to create and drop ENUM types, and it uses Python data structures like pandas Series and dictionaries to handle inputs and outputs.

It is part of the data preprocessing and database setup pipeline in this project. It ensures that the database schema (specifically ENUM types) is properly configured before performing further operations, such as aggregating light exposure data.
:::

::: code-fold-flex
```{python}
#| label: "try_create_drop"
#| code-summary: "An approach for defining and verifying custom data types in the database."

def try_create(conn,type_str: str, enum_str: str) -> pd.Series:
  """
  :::WHY(s):::
    Is there a modular approach to creating ENUM types in a database?
  :::HOW(s):::
    Utilize python data structures such as pandas Series and dictionary as parameters.
  """
  # the try, except can help categorize the outputs for ui/ux
  try:
    conn.execute(f"CREATE TYPE {type_str} AS ENUM {enum_str};")
    return pd.Series(type_str, index = ['created'])
  except duckdb.duckdb.CatalogException:
    return pd.Series(type_str, index = ['already existed'])

def try_drop(conn, type_str: str) -> pd.Series:
  """
  :::WHY(s):::
    Is there a modular approach to dropping ENUM types in a database?
  :::HOW(s):::
    Utilize python data structures such as pandas Series and dictionary as parameters.
  """
  # the try, except can help categorize the outputs for ui/ux
  try:
    conn.execute(f"DROP TYPE {type_str};")
    return pd.Series(type_str, index = ['dropped'])
  except duckdb.duckdb.CatalogException:
    return pd.Series(type_str, index = ['did not exist'])
  
droplist = []
for e in enums:   
   droplist.append(try_drop(conn, type_str = e))
   
dropFrame = pd.DataFrame(droplist)
dropFrame = dropFrame.sort_values(by = dropFrame.columns.to_list(),
                                  ascending = True,
                                  ignore_index = True)
createList = []
for eType, eSeries in enumDict.items():
    createList.append(try_create(conn,
                                 type_str = eType,
                                 enum_str = eSeries))
createFrame = pd.DataFrame(createList)
createFrame = createFrame.sort_values(by = createFrame.columns.to_list(),
                                      ascending = True,
                                      ignore_index = True)

pydf = pd.concat([dropFrame, createFrame], axis = 1)
```
:::

::: code-fold-flex
```{r}
#| label: "enumDropcreate"
#| code-summary: "table setup"

r_df <- py$pydf

if (colnames(r_df[1]) == "did not exist"){
  locations_list = list("did not exist", "created") 
  notes_list =  list("Enums that did not exist.",
                     "Enums that now exist.")
  } else {
  locations_list = list("dropped", "created")
  notes_list =  list("Enums that were dropped.",
                     "Enums that now exist.")
  }

# Create a table dataframe, or tibble, of footnotes
footnotes_df <- tibble(notes = notes_list, locations = locations_list)

pal_df <- tibble(
  cols = locations_list,
  pals = list(
    eval_palette("ggsci::legacy_tron", 7, 'd', 1))
  )

#tbl_font_size = pct(80))
rTable <- r_table_theming(r_df,
                          title = "Custom Data Types",
                          subtitle = NULL,
                          footnotes_df,
                          source_note = md("**source**: Kaggle"),
                          pal_df,
                          multiline_feet = TRUE,
                          tbl_font_size = pct(80)
                          )
```
:::

::: table-flex
```{r}
#| label: "tbl-enumOutput"
#| tbl-cap: "Enums dropped and created."
#| echo: false
rTable
```
:::

### Initial Extraction Transformation and Load Pipeline

::: p-1
Transformations are applied while reading in data from files to minimize steps needed. Not sure why the data had hyphens in the columns names, originally. These hyphens interfere with SQL database queries, so it might be useful to replace them with underscore characters. There is no need to overwrite the original CSV's to maintain a clear trail of data trasformations.
:::

::: code-fold-flex
```{python}
#| label: "setupPipeline"
#| code-summary: "Initial data pipeline extraction, transformation and loading to the database."

def setup_duckdb_pipeline(
  csvDict: dict, 
  parquetDict: dict, 
  conn: duckdb.DuckDBPyConnection) -> None:
    
  try:
    {
      table_name: duckdb.sql(f"""
      CREATE OR REPLACE TABLE {table_name} AS 
      SELECT 
        *
      FROM 
        df;
      """, connection = conn) 
      for table_name, df in csvDict.items()
      }
    for key, value in csvDict.items():
      result = conn.execute(f"SELECT COUNT(*) FROM {key}").fetchone()
      print(f"Successfully created table: {key}, Row count: {result[0]}")
  except Exception as e:
    print(f"Error loading table: {str(e)}")
    raise
  
  if parquetDict:
    write_datasets(conn, parquetDict)
  
# Create tables from Parquet files
def write_datasets (
  conn: duckdb.DuckDBPyConnection, parquetDict: dict):
      try:
        {
          table_name: duckdb.sql(f"""
           CREATE OR REPLACE TABLE {table_name} AS
           SELECT 
             id::id_actigraphy_enum AS id
             ,quarter::TEXT::quarter_enum AS quarter
             ,weekday::TEXT::weekday_enum AS weekday
             ,light
             ,(time_of_day / 3_600_000_000_000) AS hour_of_day
             ,relative_date_PCIAT
           FROM read_parquet(
             '{file_path}'
             ,hive_partitioning = true
             );""", connection=conn)
           for table_name, file_path in parquetDict.items()
           }
        for key, value in parquetDict.items():
          result = conn.execute(f"SELECT COUNT(*) FROM {key}").fetchone()
          print(
            f"Successfully created table: {key}, Row count, {result[0]}")
      except Exception as e:
        print(f"Error writing dataset: {str(e)}")
        raise

trainCsvDf = pd.read_csv("data/train.csv")
testCsvDf = pd.read_csv("data/test.csv")

dictDf = pd.read_csv("data/data_dictionary.csv")

trainCsvDf.columns = trainCsvDf.columns.str.replace('-','_') 
trainCsvDf.columns = trainCsvDf.columns.str.lower() 
testCsvDf.columns = testCsvDf.columns.str.replace('-','_') 
testCsvDf.columns = testCsvDf.columns.str.lower() 

dictDf.Field = dictDf.Field.replace("-", "_", regex = True)

csvDict = {
  "TrainCsv": trainCsvDf
  ,"TestCsv": testCsvDf
  ,"DataDict": dictDf
  }

parquetDict = {
  "ActigraphyTrain": 'data/series_train.parquet*/*/*'
  ,"ActigraphyTest": 'data/series_test*/*/*'
  }

try:
  setup_duckdb_pipeline(csvDict, parquetDict, conn)
except:
  print(f"Could not set up data pipeline.")
```
:::

::: code-fold-flex
```{python}
#| label: "verifyAT"
#| code-summary: "Verify ActigraphyTrain in the database."

pydf = conn.sql(f"""
SELECT *
FROM ActigraphyTrain
ORDER BY 
  id ASC
  ,relative_date_PCIAT ASC
  ,hour_of_day ASC
LIMIT 10;""").df()
```
:::

::: code-fold-flex
```{r}
#| label: "ActigraphyTrainSetup"
#| code-summary: "table setup"

r_df <- py$pydf

r_df <- r_df |>
  dplyr::select(id, quarter, weekday, relative_date_PCIAT, light, hour_of_day)

notes_list =  list(
  "Unique participant identifier code, to track related data from actigraphy to hbn datasets.",
  "Annual quarter related to month of observation.",
  "Day of the week (Monday = 1)",
  "Light exposure, measured in 'lux'",
  "24-hour time converted to a float for easier conversion for time-series analysis.",
  "Days since questionairre, also convenient to help order daily values for time-series analysis."
  )

locations_list = list("id",
                      "quarter",
                      "weekday",
                      "light",
                      "hour_of_day",
                      "relative_date_PCIAT")

# Create a table data-frame, or tibble, of footnotes
footnotes_df <- tibble(
  notes = notes_list, 
  locations = locations_list)

pal_df <- tibble(
  cols = locations_list,
  pals = list(eval_palette("harrypotter::ravenclaw", 7, 'c', 1)))

rTable <- r_table_theming(r_df,
                          title = "Actigraphy Data - Training Set",
                          subtitle = NULL,
                          footnotes_df,
                          source_note = md("**source**: Kaggle"),
                          pal_df,
                          multiline_feet = TRUE
                          #tbl_font_size = pct(85)
                          )
```
:::

::: table-flex
```{r}
#| label: "tbl-ActigraphyTrain"
#| tbl-cap: "A preview of ActigraphyTrain database table."
#| echo: false
rTable
```
:::

## EDA

:::p-1
This is designed to filter and create new database tables based on regular expression (regex) patterns applied to column names. It is part of a data preprocessing pipeline. The code modularizes the process of selecting columns and creating new tables, making it reusable and efficient.
:::

::: code-fold-flex
```{python}
#| label: "create_table_with_regex_columns()"
#| code-summary: "Creates several tables based on column label strings."
#| warning: false

import re

def filter_columns_by_regex(col_dict: dict, regex_pattern: str) -> dict:
  """
  :::WHY(s):::
    Can there be a way to use dictionaries to specify columns?

  :::GOAL(s):::
    Regex selection of column names from a dictionary.
  """
  return {
    col: dtype 
    for col, dtype in col_dict.items() 
    if re.search(regex_pattern, col)
    }

def create_table_with_regex_columns(
  conn: duckdb.duckdb
  ,source_table: str
  ,new_table_name: str 
  ,regex_pattern: str 
  ,col_dict: dict
  ) -> None:  

  """
  :::WHY(s):::
    There should to be a more streamlined way to utilize columnar information following regular string patterns to generate tables based on such information. This could follow and carry out pre-existing data modeling plans.

  :::GOAL(s):::
    To create new database tables with modularized SQL queries generated with the help of regex pattern matching selection.

  :::EXAMPLE(s):::
    The dictionary item:
      "Demographic": r"^id|^sii|^basic\S+"
 
    Would generate a table named 'Demographic' from data with columns for 'id', 'sii' and all columns starting with 'basic' strings. 
  """

  # filter columns
  filtered_col_dict = filter_columns_by_regex(col_dict, regex_pattern)
  
  # regex column selecting via dict comprehension and vectorized filtering
  regex_select_sql = f"""
  CREATE OR REPLACE TABLE {new_table_name} AS 
  SELECT
    {', '.join([f'"{col}"' for col in filtered_col_dict.keys()])}
  FROM {source_table};
  """

  conn.execute(regex_select_sql)

# It'd be useful to get the data types for creating a new table of values
coltype_overview = conn.sql(f"""
  SELECT column_name
         ,data_type
  FROM 
    information_schema.columns
  WHERE 
    table_name = 'TrainCsv';""").df()

# Map the column names with data types
col_dict = dict(zip(coltype_overview.column_name, coltype_overview.data_type))

regex_dict_train = {
  "Demographic": r"^id|^sii|^basic\S+"
  ,"Physical": r"^id|^sii|^physical\S+"
  ,"FgVital": r"^id|^sii|^fitness_E\S+"
  ,"FgChild": r"^id|^sii|^fgc\S+"
  ,"Bia": r"^id|^sii|^bia\S+"
  ,"Paqa": r"^id|^sii|^paq_a\S+"
  ,"Pciat": r"^id|^sii|^pciat\S+"
  ,"Sds": r"^id|^sii|^sds\S+"
  ,"InternetUse": r"^id|^sii|^preint\S+"
  }

# There's no Pciat in the test set
regex_dict_test = {
  "DemographicTest": r"^id|^basic\S+"
  ,"PhysicalTest": r"^id|^physical\S+"
  ,"FgVitalTest": r"^id|^fitness_E\S+"
  ,"FgChildTest": r"^id|^fgc\S+"
  ,"BiaTest": r"^id|^bia\S+"
  ,"PaqaTest": r"^id|^paq_a\S+"
 # ,"Pciat_OfTest": r"^id|^pciat\S+" 
  ,"SdsTest": r"^id|^sds\S+"
  ,"InternetUseTest": r"^id|^preint\S+"
  }

# Loop through the data structures to create tables for the train set
for new_table_name, regex_pattern in regex_dict_train.items():
  create_table_with_regex_columns(
    conn 
    ,'TrainCsv'
    ,new_table_name
    ,regex_pattern
    ,col_dict
    ) 

# Loop through the data structures to create tables for the test set
for new_table_name, regex_pattern in regex_dict_test.items():
  create_table_with_regex_columns(
    conn
    ,'TestCsv'
    ,new_table_name
    ,regex_pattern 
    ,col_dict
    )

```
:::

::: code-fold-flex
```{python}
#| label: "joinDemoAndActigraphyOnId"
#| code-summary: "Perform a join on matching Ids to combine demographics data with actigraphy data such as time and light exposure."

conn.sql("""
CREATE OR REPLACE TABLE IntermediateActigraphy AS
SELECT id
       ,basic_demos_enroll_season::TEXT::enroll_season_enum AS enroll_season
       ,basic_demos_age::TEXT::age_enum AS age
       ,basic_demos_sex::TEXT AS sex
       ,sii::INTEGER::TEXT::sii_enum AS sii
FROM 
  Demographic
ORDER BY
  id ASC;
""")

conn.sql("""
CREATE OR REPLACE TABLE 
  ActigraphyTrain
AS
  SELECT
    ia.*
    ,at.hour_of_day
    ,at.weekday
    ,at.quarter
    ,at.light
  FROM 
    ActigraphyTrain at 
  LEFT JOIN 
    IntermediateActigraphy ia
  ON 
    ia.id = at.id;
""")

conn.sql("""
CREATE OR REPLACE TABLE IntermediateActigraphy AS
SELECT
  id
  ,basic_demos_enroll_season::TEXT::enroll_season_enum AS enroll_season
  ,basic_demos_age::TEXT::age_enum AS age
  ,basic_demos_sex::TEXT AS sex
FROM 
  DemographicTest
ORDER BY
  id ASC;
""")

conn.sql("""
CREATE OR REPLACE TABLE 
  ActigraphyTest
AS
  SELECT
    ia.*
    ,at.hour_of_day
    ,at.quarter
    ,at.weekday
    ,at.light
  FROM 
    ActigraphyTest at 
  LEFT JOIN 
    IntermediateActigraphy ia
  ON 
    ia.id = at.id;
""")

conn.sql("DROP TABLE IntermediateActigraphy;")

try:
 conn.sql(f"CHECKPOINT;")
 print(f"Cleared unused allocated space from memory.")
except:
  print(f"Could not clear from memory.")
```
:::

### Actigraphy, Physical, and Demographics

::: p-1
It might be useful to explore how the time of day and associated exposure to light might correlate to other factors, such as sii, physical characteristics, and internet use.
:::

#### Informal Hypothesis

:::p-1
*Hypothesis*: More time spent indoors on the internet might be associated with lower light exposure during daytime (Q1-Q2, Q2-Q3) and higher light exposure during nighttime (min-Q1, Q3-max).

The BMI risk categories found at, [@zierle_physiology_2024], have been borrowed for this project. The SQL query syntax, therefore, aims to reflect a structure of evidence-based classification. Though not used in this project, it might be useful to classify lux statistics in a way that reflect a classification standard [@actigraphcorp_lux_2024].
:::

#### Lux by Hour of Day Quartile

:::p-1
Repetitive sql commands are quite verbose and repetitive, at times. Python functions, alongside data structures, such as dataframes, and dictionaries could actually help make running SQL commands less repetitive and more modular. 

The code is designed to process and aggregate light exposure data from actigraphy datasets (likely from wearable devices) using DuckDB as an in-memory database. The code is modularized into several functions to handle different tasks, such as calculating quartiles, detecting specific columns, creating intermediate tables, and joining these tables into a final aggregated result.

Additionally, the code is useful for analyzing light exposure data over different times of the day (based on quartiles of hour_of_day). It aggregates the data into meaningful segments and creates a final table that can be used for further analysis or visualization. Included is a try-except block to handle any potential errors during the table joining process, ensuring that the user is informed if something goes wrong.
:::

::: code-fold-flex
```{python}
#| label: "intermediateLighter"
#| code-summary: "Extract actigraphy data based on the time of day quartile."


def quartiler(
  conn: duckdb.duckdb, 
  col_name: str, 
  source_name: str) -> dict:
  """
  ::WHY(s):: 
    SQL can sometimes require a lot of code for repetitive commands, but in a Python environment, database queries can be modularized.
  
  ::GOAL(s)::  
    To process SQL queries into useful quartile information represented by intuitive key labels.
  """
  summaryDf = conn.sql(f"""
  SUMMARIZE
  SELECT
    {col_name}
  FROM 
    {source_name};""").df()

  quartileDict = {
    'min': summaryDf['min'][0]
    ,'Q1': summaryDf.q25[0]
    ,'Q2': summaryDf.q50[0]
    ,'Q3': summaryDf.q75[0]
    ,'max': summaryDf['max'][0]
    }
  
  return quartileDict

def siiDetect (detect_frame: pd.DataFrame) -> bool:
  if 'sii' in detect_frame.column_name.values:
    return True

def intermediateLighter(
  conn: duckdb.duckdb,
  new_tables: list,
  quarters: dict, 
  quartuples: pd.Series,
  from_table: str) -> None:
  """
  :::WHY(s):::
    Tables based on the same parameters but different parameter values could be modularized.
    
  :::GOAL(s):::
    Process repetitive SQL efficiently and intuitively using data structures that simplify the process. 
  """
  
  detect_frame = conn.execute(f"""
  SELECT *
  FROM 
    information_schema.columns
  WHERE 
    table_name = '{from_table}';
  """).df()
  
  for i in list(range(4)):
    if siiDetect(detect_frame) == True:
      conn.sql(f"""
        CREATE OR REPLACE TABLE {new_tables[i]} AS
        SELECT
          id
          ,enroll_season
          ,age
          ,sex
          ,sii
          ,AVG(light) AS {quartuples.index[i]}
        FROM 
          '{from_table}'
        WHERE 
          hour_of_day BETWEEN 
              {quarters[quartuples.iloc[i][0]]}::DOUBLE 
            AND 
              {quarters[quartuples.iloc[i][1]]}::DOUBLE
        GROUP BY 
          ALL
        ORDER BY 
          id ASC;""")
    else:
      conn.sql(f"""
        CREATE OR REPLACE TABLE {new_tables[i]} AS
        SELECT
          id
          ,enroll_season
          ,age
          ,sex
          ,AVG(light) AS {quartuples.index[i]}
        FROM 
          '{from_table}'
        WHERE 
          hour_of_day BETWEEN 
              {quarters[quartuples.iloc[i][0]]}::DOUBLE 
            AND 
              {quarters[quartuples.iloc[i][1]]}::DOUBLE
        GROUP BY 
          ALL
        ORDER BY 
          id ASC;""")

def joinLights (from_to_tables: dict) -> None:
  
  for from_table, to_table in from_to_tables.items():
    
    new_tables = ['Light1', 'Light2', 'Light3', 'Light4']
    
    quarters = quartiler(conn, 'hour_of_day', from_table)

    intermediateLighter(conn, new_tables, quarters, quartuples, from_table)
  
    conn.sql(f"""
    CREATE OR REPLACE TABLE {to_table} AS
    SELECT 
      l1.*
      ,l2.q1_q2
      ,l3.q2_q3
      ,l4.q3_max
    FROM 
      Light1 l1
    LEFT JOIN Light2 l2 ON l1.id = l2.id
    LEFT JOIN Light3 l3 ON l1.id = l3.id
    LEFT JOIN Light4 l4 ON l1.id = l4.id;
    """)
    
    for table in new_tables:
      conn.sql(f"DROP TABLE {table};")
      
quartuples = pd.Series(
  data = 
  [('min','Q1')
  ,('Q1', 'Q2')
  ,('Q2', 'Q3')
  ,('Q3' ,'max')]
  ,index =
  ['min_q1'
  ,'q1_q2'
  ,'q2_q3'
  ,'q3_max'])

from_to_tables = {
  'ActigraphyTrain': 'AggregatedLights'
  ,'ActigraphyTest': 'AggregatedLightsTest'
  }

try:
  joinLights(from_to_tables)
  pydf = conn.sql("SELECT * FROM AggregatedLights LIMIT 10;").df()
  print(f"Joined the lux tables and saved a preview of the aggregated table.")
except:
  print(f"Lux tables were not joined.")
```
:::

::: code-fold-flex
```{python}
#| label: "checkpointLights"
#| code-summary: "Free unused database memory from dropped tables and such."
try:
  conn.sql(f"CHECKPOINT;")
  print(f"Cleared unused memory.")
except:
  print(f"Could not clean unused memory.")
```
:::

::: code-fold-flex
```{r}
#| label: "aggLightSetup"
#| code-summary: "table setup"

r_df <- py$pydf

notes_list =  list(
  "Unique participant identifier.",
  "Participant enrollment into study season.",
  "Age in years.",
  "Sex (0 = Female, 1 = Male)",
  "Severity Impairment Index",
  "Mean light exposure across Q1 of a 24 hour time scale.",
  "Mean light exposure across Q2 of a 24 hour time scale.",
  "Mean light exposure across Q3 of a 24 hour time scale.",
  "Mean light exposure across Q4 of a 24 hour time scale."
  )

locations_list = list(
  "id"
  ,"enroll_season"
  ,"age"
  ,"sex"
  ,"sii"
  ,'min_q1'
  ,'q1_q2'
  ,'q2_q3'
  ,'q3_max')

# Create a table data-frame, or tibble, of footnotes
footnotes_df <- tibble(notes = notes_list, 
                       locations = locations_list)

pal_df <- tibble(
  cols = locations_list,
  pals = list(
    eval_palette("harrypotter::ravenclaw", 7, 'c', 1))
  )

rTable <- r_table_theming(r_df,
                          title = "Quartile Data - Lights",
                          subtitle = NULL,
                          footnotes_df,
                          source_note = md("**source**: Kaggle"),
                          pal_df,
                          multiline_feet = TRUE)
```
:::

:::p-1
@tbl-AggregatedLights, returns the averages of light exposure, grouped by individual participants across the 4 6-hour time-frames of the day.
:::

::: table-flex
```{r}
#| label: "tbl-AggregatedLights"
#| tbl-cap: "Quantiles related to mean lux exposure."
#| echo: false
rTable
```
:::

::: code-fold-flex
```{python}
#| label: "Create tables from the lights dictionary."
#| code-summary: "Join light data with internet use hours data."

lightsDict = {
  'AggregatedLights': 'InternetUse', 
  'AggregatedLightsTest': 'InternetUseTest'
  }

for from_tbl, join_tbl in lightsDict.items():
  conn.sql(f"""
  CREATE OR REPLACE TABLE '{from_tbl}' AS
  SELECT
    ft.*
    ,jt.preint_eduhx_computerinternet_hoursday::INTEGER::TEXT::internet_hours_enum AS useHrs 
  FROM
    '{from_tbl}' ft
    LEFT JOIN 
      {join_tbl} jt
    ON 
      ft.id = jt.id;
  """)
```
:::

:::p-1
Checking the internet use hours parameter to get an overview along with null percentage to see if we might be able to use it, as-is, for joining with the corresponding lux quartile.
:::

::: code-fold-flex
```{python}
#| label: "describeSummary"
#| code-summary: "Potentially useful for quick stats."
#| include: false

all_al = conn.sql(f"SELECT * FROM AggregatedLightsTest;").df()
pyDescribe = all_al.describe()

duckSummary = conn.sql(f"""
SUMMARIZE
SELECT *
FROM AggregatedLightsTest;
""").df()

duckSummary = duckSummary.set_index('column_name')

pyDescribe, duckSummary
```
:::

#### Sii, Lux, and Internet Use

:::p-1
@tbl-useToSii, is returning some descriptive statistics, such as count and mean, across the behavioral index, and mean lux by quartile, grouped by internet use hours.
:::

::: code-fold-flex
```{python}
#| label: "aggLights"
#| code-summary: "Group-averaging sii, internet use hours, and pre-averaged lux quartile by age and sex."

pydf = conn.sql(f"""
SELECT
  useHrs
  ,COUNT(*) AS observations
  ,AVG(sii::INTEGER) AS sii
  ,AVG(min_q1) AS 'q1'
  ,AVG(q1_q2) AS 'q2'
  ,AVG(q2_q3) AS 'q3'
  ,AVG(q3_max) AS 'q4'
FROM 
  AggregatedLights al
GROUP BY 
  useHrs;""").df()

#obsSum = pydf.observations.sum()

pydf = pydf.dropna()
```
:::

::: code-fold-flex
```{r}
#| label: "lightUseHrs"
#| code-summary: "table setup"

r_df <- py$pydf

notes_list =  list(
  "Hours spent on the internet, as reported by caregivers of the participants.",
  "Count of observations in the dataset.",
  "Problematic behavior score.",
  "Mean lux (12am to 6am)",
  "Mean lux (6am to 12pm)",
  "Mean lux (12pm to 6pm)",
  "Mean lux (6pm to 12am)"
  )

locations_list = list(
  "useHrs", 
  "observations",
  "sii",
  "q1",
  "q2",
  "q3",
  "q4"
  )

footnotes_df <- tibble(
  notes = notes_list, 
  locations = locations_list
  )

pal_df <- tibble(
  cols = locations_list,
  pals = list(
    eval_palette("harrypotter::ravenclaw", 7, 'c', 1))
  )

rTable <- r_table_theming(r_df,
                          title = "Problematic Behavior to Internet Usage",
                          subtitle = NULL,
                          footnotes_df,
                          source_note = md("**source**: Kaggle"),
                          pal_df,
                          multiline_feet = FALSE
                          )
```
:::


::: table-flex
```{r}
#| label: "tbl-useToSii"
#| tbl-cap: "Grouped averages of lux to sii."
#| echo: false
rTable
```
:::

::: code-fold-flex
```{r}
#| label: "useSiiSetup"
#| code-summary: "plot setup"
#| message: false

library(scales)

r_plot <- ggplot(data = r_df, 
                 mapping = aes(x = useHrs, y = sii)
                 ) +
  geom_col(aes(fill = sii)) +
  scale_fill_gradient(low = muted("yellow"), high = muted("purple")) +
  labs(title = 'Internet Use to Behavioral Index', 
       x = 'Hours of Internet Use', 
       y = "Problematic Behavior Index (sii)")

r_plot <- r_plot + ggplot_theming()
```
:::

```{r}
#| label: "fig-useSii"
#| fig-cap: "Categorized bmi risk to internet use and sii."
#| echo: false
r_plot
```


#### Disease Risk Categories

:::p-1
Something that people might not see everyday is SQL sytnax used within a Python environment. The performance of the sql database, in this case, duckdb, likely outperforms pandas, polars, and such. My aim in this project is to demonstrate some competence with sql, but also approach practical use-case scenarios.
:::

::: code-fold-flex
```{python}
#| label: "caseWhenUnionBmi"
#| code-summary: "Combining SQL with Python data structures to organized data according to several conditions, such as: bmi, age, sex, and waist_circumference."

riskyDictionary = {
  'Risk1': 
    (f",CASE WHEN ph.physical_bmi < 18.5 THEN 'Underweight'"
    ,f"WHEN ph.physical_bmi BETWEEN 18.5 AND 24.9 THEN 'Normal'"
    ,f"WHEN ph.physical_bmi BETWEEN 25.0 AND 29.9 THEN 'Increased'"
    ,f"WHEN ph.physical_bmi BETWEEN 30.0 AND 34.9 THEN 'High'"
    ,f"WHEN ph.physical_bmi BETWEEN 35.0 AND 39.9 THEN 'Very High'"
    ,f"WHEN ph.physical_bmi >= 40 THEN 'Extremely High'"
    ,f"ph.physical_waist_circumference <= 35 AND al.sex = '0'"),
  'Risk2':
    (f",CASE WHEN ph.physical_bmi < 18.5 THEN 'Underweight'"
    ,f"WHEN ph.physical_bmi BETWEEN 18.5 AND 24.9 THEN 'Normal'"
    ,f"WHEN ph.physical_bmi BETWEEN 25.0 AND 29.9 THEN 'High'"
    ,f"WHEN ph.physical_bmi BETWEEN 30.0 AND 34.9 THEN 'Very High'"
    ,f"WHEN ph.physical_bmi BETWEEN 35.0 AND 39.9 THEN 'Very High'"
    ,f"WHEN ph.physical_bmi >= 40 THEN 'Extremely High'"
    ,f"ph.physical_waist_circumference > 35 AND al.sex = '0'"),
  'Risk3':
    (f",CASE WHEN ph.physical_bmi < 18.5 THEN 'Underweight'"
    ,f"WHEN ph.physical_bmi BETWEEN 18.5 AND 24.9 THEN 'Normal'"
    ,f"WHEN ph.physical_bmi BETWEEN 25.0 AND 29.9 THEN 'Increased'"
    ,f"WHEN ph.physical_bmi BETWEEN 30.0 AND 34.9 THEN 'High'"
    ,f"WHEN ph.physical_bmi BETWEEN 35.0 AND 39.9 THEN 'Very High'"
    ,f"WHEN ph.physical_bmi >= 40 THEN 'Extremely High'"
    ,f"ph.physical_waist_circumference <= 40 AND al.sex = '1'"),
  'Risk4':
    (f",CASE WHEN ph.physical_bmi < 18.5 THEN 'Underweight'"
    ,f"WHEN ph.physical_bmi BETWEEN 18.5 AND 24.9 THEN 'Normal'"
    ,f"WHEN ph.physical_bmi BETWEEN 25.0 AND 29.9 THEN 'High'"
    ,f"WHEN ph.physical_bmi BETWEEN 30.0 AND 34.9 THEN 'Very High'"
    ,f"WHEN ph.physical_bmi BETWEEN 35.0 AND 39.9 THEN 'Very High'"
    ,f"WHEN ph.physical_bmi >= 40 THEN 'Extremely High'"
    ,f"ph.physical_waist_circumference > 40 AND al.sex = '1'")
    }

riskyDf = pd.DataFrame(data = riskyDictionary)

for key, value in riskyDf.items():
  try:
    conn.sql(f"""
    CREATE OR REPLACE TABLE {key} AS
    SELECT
      al.*
      {value[0]}
      {value[1]}
      {value[2]}
      {value[3]}
      {value[4]}
      {value[5]}
      ELSE NULL
      END AS risk_cat
    ,risk_cat::disease_risk_enum AS risk_category
    FROM 
      Physical ph 
    LEFT JOIN 
      AggregatedLights al
    ON 
      al.id = ph.id
    WHERE 
      {value[6]}
    ORDER BY 
      al.id ASC;
      """)
    result = conn.sql(f"SELECT COUNT(*) FROM {key}").fetchone()
    print(f"created table: {key}, \n\tRow count: {result[0]}\n")
  except:
    print(f"Error loading this table: {key}")

conn.sql(f"""
CREATE OR REPLACE TABLE 
  DiseaseRiskDemographic AS
SELECT * EXCLUDE(risk_cat) FROM Risk1
UNION BY NAME
SELECT * EXCLUDE(risk_cat) FROM Risk2
UNION BY NAME
SELECT * EXCLUDE(risk_cat) FROM Risk3
UNION BY NAME 
SELECT * EXCLUDE(risk_cat) FROM Risk4;
""")

```
:::

:::p-1
It might be useful to see what some descriptive statistics might look when grouped by risk categories. 
:::

::: code-fold-flex
```{python}
#| label: "riskCatPy"
#| code-summary: "Grouped averaging by risk category."

conn.sql(f"""
CREATE OR REPLACE TABLE RiskCategorySummary AS
SELECT 
  risk_category
  ,AVG(sii::NUMERIC) AS sii
  ,AVG(useHrs::INTEGER) AS useHrs
  ,AVG(min_q1) AS 'q1'
  ,AVG(q1_q2) AS 'q2'
  ,AVG(q2_q3) AS 'q3'
  ,AVG(q3_max) AS 'q4'
FROM 
  DiseaseRiskDemographic
GROUP BY 
  risk_category;
""")

pydf = conn.sql("""
SELECT * 
FROM RiskCategorySummary;
""").df()

```
:::

::: code-fold-flex
```{r}
#| label: "diseaseriskLux"
#| code-summary: "table setup"

r_df <- py$pydf

notes_list =  list("The disease risk category.",
                   "Problematic behavior index.",
                   "Reported interet usage (hours).",
                   "12am to 6am",
                   "6am to 12pm.",
                   "12pm to 6pm",
                   "6pm to 12am")

locations_list = list("risk_category"
                      ,"sii"
                      ,"useHrs"
                      ,"q1"
                      ,"q2"
                      ,"q3"
                      ,"q4")

# Create a table dataframe, or tibble, of footnotes
footnotes_df <- tibble(notes = notes_list, 
                       locations = locations_list)

pal_df <- tibble(
  cols = locations_list,
  pals = list(eval_palette("harrypotter::ravenclaw", 7, 'c', 1))
  )

rTable <- r_table_theming(r_df,
                          title = "Disease Risk Categories to Lux",
                          subtitle = NULL,
                          footnotes_df,
                          source_note = md("**source**: Kaggle"),
                          pal_df,
                          multiline_feet = FALSE,
                          
                          )
```
:::

:::p-1
@tbl-diseaseRiskLux shows the grouped-averages of the problematic behavior index, internet use hours, and mean lux across the time of day quartiles by disease risk category of participants.
:::

::: table-flex
```{r}
#| label: "tbl-diseaseRiskLux"
#| tbl-cap: "Disease risk categories by lux exposure."
#| echo: false
rTable
```
:::

::: code-fold-flex
```{r}
#| label: "elongateQuarterLux"
#| code-summary: "Pivoting the data longer to accommodate more useful plot output."
#| message: false

library(tidyr)
library(dplyr)

data_long <- r_df |>
  pivot_longer(
    cols = c(q1, q2, q3, q4),
    names_to = "quarter",
    values_to = "lux") |>
  mutate(quarter = factor(quarter, 
                          levels = c("q1",
                                     "q2",
                                     "q3",
                                     "q4")))
```
:::

:::code-fold-flex
```{r}
#| label: "longerDiseaseCat"
#| code-summary: "table setup"

locations_list = list("risk_category", "sii", "useHrs", "quarter", "lux")
notes_list =  list("Disease risk based on BMI, waist circumference, and sex.",
                   "Problematic behavior index.",
                   "Hours of reported internet use.",
                   "Quartile of day.",
                   "Average light exposure.")

# Create a table dataframe, or tibble, of footnotes
footnotes_df <- tibble(notes = notes_list, locations = locations_list)

pal_df <- tibble(
  cols = locations_list,
  pals = list(eval_palette("harrypotter::ravenclaw", 7, 'c', 1)))

#tbl_font_size = pct(80))
rTable <- r_table_theming(data_long,
                          title = "Disease Risk Categories to Lux",
                          subtitle = "Pivoted Longer",
                          footnotes_df,
                          source_note = md("**source**: Kaggle"),
                          pal_df,
                          multiline_feet = TRUE,
                          tbl_font_size = pct(80)
                          )
```
:::

:::p-1
@tbl-longerDiseaseCat is a pivoted version of the previous table, @tbl-diseaseRiskLux.
:::

:::table-flex
```{r}
#| label: "tbl-longerDiseaseCat"
#| echo: false
rTable
```
:::

::: code-fold-flex
```{r}
#| label: "luxRiskSetup"
#| code-summary: "plot setup"

r_plot <- ggplot(data = data_long, 
                 aes(x = quarter, 
                     y = lux, 
                     group = risk_category, 
                     color = risk_category)) +
  geom_line(linewidth = 1) +
  geom_point(size = 3) +
  labs(title = "Is Disease risk related to light exposure\n during certain times of the day?",
       x = "Time Period",
       y = "Mean Lux Value",
       color = "Risk Category")

r_plot <- r_plot + ggplot_theming()

```
:::

@fig-luxRiskCat was an attempt at visualizing the relationship, if any, by mean lux exposure across participants, time of day quartile, and disease risk. Disease risk was based on BMI, waist-circumference, and sex data. 

```{r}
#| label: "fig-luxRiskCat"
#| fig-cap: "Grouped risk by lux and time period."
#| echo: false
r_plot
```

## Summary Profiles

::: p-1
In the future, we might want to explore further, but for now we can save these descriptive statistics tables to files.
:::

::: code-fold-flex
```{python}
#| label: "summaryProfiles"
#| code-summary: "Create future training and testing data profiles from summary statistics to inform modeling choices."
#| message: false

summary_dict = {
  'ActigraphyTrain': 
    ('ActigraphyTrainSummary'
    ,'data/profiles/actigraphy_train_profile.csv'),
  'ActigraphyTest': 
    ('ActigraphyTestSummary'
    ,'data/profiles/actigraphy_test_profile.csv'),
  'TrainCsv': 
    ('TrainCsvSummary'
    ,'data/profiles/train_csv_profile.csv'),
  'TestCsv': 
    ('TestCsvSummary'
    ,'data/profiles/test_csv_profile.csv')
    }

summary_frame = pd.DataFrame(summary_dict)

for key, value in summary_frame.items():
  try:
    py_summary_df = conn.sql(f"SUMMARIZE SELECT * FROM '{key}';").df()
    conn.register(value[0], py_summary_df)
    conn.sql(f"COPY {value[0]} TO '{value[1]}' (HEADER , DELIMITER ',');")
    print(f"'{value[1]}' was CREATED\n")
  except:
    print(f"'{value[1]}' was not CREATED\n")
```
:::

```{python}
#| label: "infoTables"
#| include: false
info_tables = conn.sql(f"Select * From main.duckdb_tables;").df()
```

## Close Connection

At the end of the analysis, we can safely close the in-memory database from RAM. The author of this analysis had 32GB of DDR4 RAM, originally with a set limit of 24GB used by the database.

::: code-fold-flex
```{python}
#| label: "closeConnection"
#| code-summary: "Releases all database memory and de-references the connection variable."
#| eval: true

try:
  conn.close()
  print(f"database connection closed")
except:
  print(f"could not close the connection")
```
:::

## Closing Thoughts

:::p-1
The purpose of the article was to explore a Kaggle dataset and improve workflow methodologies and output. It can be expanded to be more insightful for this particular domain, but that was not the goal. The objective has been achieved, as better outputting functions have been developed alongside improved CSS styling. 

This particular dataset contained a lot of missing data, despite having come from large dataset. Due to this incomplete data across many of the columns,some of the data transformations resulted in as low as ~50 observations from the original ~1000. The leaderboard that came along with this dataset from a past competition suggests that the model accuracy tops out only around 0.48, [@tbl-leaderboard]. I believe, while some patterns could be gleaned, the predictive power of any modeling attempts might as well have been doomed.
:::

:::code-fold-flex
```{python}
#| label: "leaderboardRead"
#| code-summary: "Reads a recently downloaded the private leaderboard csv (from the competition website's Leaderboard tab) to a dataframe."

leaderboard_path = "data/privateLeaderboard.csv"
leaderboard_df = pd.read_csv(leaderboard_path)

py_df = leaderboard_df.head(n = 10)
```
:::

:::code-fold-flex
```{r}
#| label: "leaderboardSetup"
#| code-summary: "table setup"

r_df <- py$py_df

notes_list =  list("Overall ranking",
                   "Unique team ID.",
                   "Chosen name of the team.",
                   "Their final submission date.",
                   "The accuracy score of model.",
                   "Total competition submissions.",
                   "Names of the team members.")

locations_list = list(
  "Rank", 
  "TeamId", 
  "TeamName", 
  "LastSubmissionDate", 
  "Score", 
  "SubmissionCount", 
  "TeamMemberUserNames")

# Create a table dataframe, or tibble, of footnotes
footnotes_df <- tibble(notes = notes_list, 
                       locations = locations_list)

pal_df <- tibble(
  cols = locations_list,
  pals = list(eval_palette("ggsci::legacy_tron", 7, 'd', 1))
  )

rTable <- r_table_theming(r_df,
                          title = "Leaderboard Rankings",
                          subtitle = NULL,
                          footnotes_df,
                          source_note = md("**source**: Kaggle"),
                          pal_df,
                          multiline_feet = FALSE,
                          target_everything = TRUE,
                          color_by_columns = "Rank",
                          tbl_font_size = pct(75)
                          )
```
:::

::: table-flex
```{r}
#| label: "tbl-leaderboard"
#| echo: false
rTable
```
:::

