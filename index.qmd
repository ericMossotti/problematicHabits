---
title: Problematic Internet Habits in Children
authors:
  - name: Eric Mossotti
    affiliation: None
    roles: data analysis
    corresponding: true



bibliography: bibliography/references.bib
citation-location: margin
citations-hover: true
link-citations: true
csl: bibliography/csl/apa.csl

code-fold: true

lightbox: auto
---

## Rationale and Initial Thoughts

The grounds for analyses in this case is the problematic behavior as it pertains to the subjects' experience and their parents' via questionnaires, physical fitness tests, worn instrument measurements and demographics. The resulting data is to be processed and connections drawn.

There are quite a few variables to consider here. The largest amount of data was collected in real time. This was recorded by a device that could measure acceleration in 3D space, ambient light, time, and the quarter of the year while the participant went about their lives. Many health issues could stem from a lack of exercise. It may follow that the more someone is actively using the internet, the less likely they are to meet daily activity recommendations.

Some issues is that it is uncertain where this feat of data collection took place. Perhaps, this was for the sake of privacy? The exact dates are not given for the real-time data either. So, we have to go by the quarter of the year and or season in that respect. Not sure if seasonal information is consequential to anything.

The goal of the models is to, ultimately, make progress in the field of childhood to adolescent psychology. Progress, in this case, is defined by accurately predicting risks of undesirable outcomes based on a young persons' daily activities.

Nowadays, life is centered around the internet. At the same time, our biology has not undergone any systemic changes. First hand experience certainly tells me that that high levels of internet activity can have a negative impact on the mind.

Is the quantity of internet activity itself truly determinant of negative outcome?

### Output Formatters

#### Tables

```{r}
#| label: "r_table_theming()"
#| code-summary: "x"
# https://emilhvitfeldt.github.io/paletteer/

library(gt)
library(purrr)
library(tibble)
library(reticulate)

r_table_theming <- function(rDataFrame,
                            title,
                            subtitle,
                            footnotes_df,
                            source_note) {
  r_table <- gt(rDataFrame) |>
    data_color(palette = "ggsci::legacy_tron",
               columns = footnotes_df$locations[[1]],
               target_columns = everything()) |>
    tab_header(title = title, subtitle = subtitle)
  
  r_table <- r_table |>
      tab_options(
        heading.background.color = '#222',
        column_labels.background.color = '#333',
        #table.background.color = '#222',
        footnotes.background.color = '#222',
        source_notes.background.color = '#222'
        )
    
    r_table <- r_table |>
      tab_source_note(source_note = source_note)

  # Footnotes are added to the accumulator, building up to the final result
  r_table <- seq_len(nrow(footnotes_df)) |>
    reduce(\(acc, i) {
      tab_footnote(
        acc,
        footnote = footnotes_df$notes[[i]],
        location = cells_column_labels(columns = footnotes_df$locations[[i]]),
        placement = "auto"
        )
      }, .init = r_table)
  
  return(r_table)
}

```

#### Plots

```{r}
#| label: "ggplot_theming()"
#| eval: true

library(reticulate)
library(ggplot2)

ggplot_theming_flipped_axes <- function() {
  theme_linedraw() +
    theme(
      plot.title = element_text(color = 'white'),
      plot.background = element_rect(fill = '#222'),
      panel.background = element_rect(fill = '#222'),
      panel.grid.major.x = element_line(linetype = 'dashed'),
      panel.grid.minor.x = element_line(linetype = "dotted"),
      panel.grid.major.y = element_line(linetype = 'solid'),
      panel.grid.minor.y = element_line(linetype = 'dotted'),
      axis.title = element_text(color = 'gray100'),
      axis.text = element_text(color = 'gray'))
}

ggplot_theming <- function() {
  theme_linedraw() +
    theme(
      plot.title = element_text(color = 'white'),
      plot.background = element_rect(fill = 'transparent'),
      panel.background = element_rect(fill = 'transparent'),
      panel.grid.major.x = element_line(linetype = 'solid'),
      panel.grid.minor.x = element_line(linetype = "dotted"),
      panel.grid.major.y = element_line(linetype = 'dashed'),
      panel.grid.minor.y = element_line(linetype = 'dotted'),
      axis.title = element_text(color = 'gray100'),
      axis.text = element_text(color = 'gray'))
  }
```

## Import/Extract

```{python}
#| eval: false
!kaggle competitions download -c child-mind-institute-problematic-internet-use
```

```{python}
#| label: "mass import"

from zipfile import ZipFile
import os
import duckdb
import pandas as pd
import re
```

```{python}
#| label: "makeDir"
#| code-summary: "Check if the directory, 'data', exists."
try:
  os.mkdir('data')
except FileExistsError:
  print(f"data exists")
```

```{python}
#| label: "unzipping"
#| code-summary: "Extracts the files and lists files."

zip_path = "child-mind-institute-problematic-internet-use.zip"

if not os.path.exists('data/train.csv'):
  with ZipFile(zip_path) as zf:
    ZipFile.extractall(zf, path = 'data')
    print(
      f"files were extracted"
      ,pd.DataFrame(data = os.listdir('data'), columns = ['data'])
      ,sep = "\n"*2
      )
else:
  print(
    f"files were already extracted"
    ,pd.DataFrame(data = os.listdir('data'), columns = ['data'])
    ,sep = "\n"*2
    )
```

### Establish a Database Connection

```{python}
conn = duckdb.connect(':memory:')
conn.sql(f"SET memory_limit = '24GB';")
conn.sql(f"SET default_order = 'ASC';")
```

### New Data Types (Enums/Enumerations)

I noticed that using ENUM data types significantly sped up many database operations [@holanda_duckdb_2021].

```{python}
#| label: "enum_series_to_dict"
#| code-summary: "Defines the series and dictionary objects for enum functions."

enums = ['internet_hours_enum'
         ,'enroll_season_enum'
         ,'disease_risk_enum'
         ,'sii_enum', 'age_enum'
         ,'sex_enum'
         ,'pciat_season_enum'
         ,'weekday_enum'
         ,'quarter_enum'
         ,'hour_enum'
         ,'minute_enum'
         ,'second_enum'
         ,'id_actigraphy_enum']

#siiSeries = pd.Series(data = ['None', 'Mild', 'Moderate', 'Severe'], dtype = str)
siiSeries = pd.Series(data = ['0', '1', '2', '3'], dtype = str)
ageSeries = pd.Series(data = range(5, 23), dtype = str)
sexSeries = pd.Series(data = ['0', '1'], dtype = str)
pciatSeasonSeries = pd.Series(data = ['Fall'
                                      ,'Spring'
                                      ,'Summer'
                                      ,'Winter'], dtype = str)

internetHrsSeries = pd.Series(data = ['0', '1', '2', '3'], dtype = str)
quarterSeries = pd.Series(data = range(1, 5), dtype = str)
weekdaySeries = pd.Series(data = range(1, 8), dtype = str)
hourSeries = pd.Series(data = range(0, 24), dtype = str)
minuteSeries = pd.Series(data = range(0, 60), dtype = str)
secondSeries = pd.Series(data = range(0, 60), dtype = str)
diseaseRiskSeries = pd.Series(data = ['Underweight'
                                      ,'Normal'
                                      ,'Increased'
                                      ,'High'
                                      ,'Very High'
                                      ,'Extremely High'], dtype = str)

id_df = conn.execute(f"""
     SELECT 
         DISTINCT(id) AS id
     FROM 
         read_parquet('data/series_train*/*/*'
         ,hive_partitioning = true)
     ORDER BY 
         id ASC;
     """).df() 

idList = id_df['id'].to_list()
idSeries = pd.Series(data = idList, dtype = str)

enumDict = {
  'disease_risk_enum': f"{tuple(diseaseRiskSeries)}"
  ,'enroll_season_enum': f"{tuple(pciatSeasonSeries)}"
  ,'sii_enum': f"{tuple(siiSeries)}"
  ,'age_enum': f"{tuple(ageSeries)}"
  ,'sex_enum': f"{tuple(sexSeries)}"
  ,'pciat_season_enum': f"{tuple(pciatSeasonSeries)}"
  ,'quarter_enum': f"{tuple(quarterSeries)}"
  ,'weekday_enum': f"{tuple(weekdaySeries)}"
  ,'hour_enum': f"{tuple(hourSeries)}"
  ,'minute_enum': f"{tuple(minuteSeries)}"
  ,'second_enum': f"{tuple(secondSeries)}"
  ,'id_actigraphy_enum': f"{tuple(idSeries)}"
  ,'internet_hours_enum': f"{tuple(internetHrsSeries)}"
  }

if len(enumDict) == len(enums):
  print(f"drops and creates are same length")
else:
  print(f"drops and creates are not the same length")
```

```{python}
#| label: "try_create_drop()"
#| code-summary: "Can there be a modular approach to creating and dropping ENUM types?\n \tYes."

def try_create(conn,type_str: str, enum_str: str) -> pd.Series:
  """
  :::WHY(s):::
    Is there a modular approach to creating ENUM types in a database?
  :::HOW(s):::
    Utilize python data structures such as pandas Series and dictionary as parameters.
  """
  # the try, except can help categorize the outputs for ui/ux
  try:
    conn.execute(f"CREATE TYPE {type_str} AS ENUM {enum_str};")
    return pd.Series(type_str, index = ['created'])
  except duckdb.duckdb.CatalogException:
    return pd.Series(type_str, index = ['already existed'])

def try_drop(conn, type_str: str) -> pd.Series:
  """
  :::WHY(s):::
    Is there a modular approach to dropping ENUM types in a database?
  :::HOW(s):::
    Utilize python data structures such as pandas Series and dictionary as parameters.
  """
  # the try, except can help categorize the outputs for ui/ux
  try:
    conn.execute(f"DROP TYPE {type_str};")
    return pd.Series(type_str, index = ['dropped'])
  except duckdb.duckdb.CatalogException:
    return pd.Series(type_str, index = ['did not exist'])
  
droplist = []
for e in enums:   
   droplist.append(try_drop(conn, type_str = e))
   
dropFrame = pd.DataFrame(droplist)
dropFrame = dropFrame.sort_values(by = dropFrame.columns.to_list()
                                  ,ascending=True
                                  ,ignore_index=True)
createList = []
for eType, eSeries in enumDict.items():
    createList.append(try_create(conn
                                 ,type_str = eType
                                 ,enum_str = eSeries))
createFrame = pd.DataFrame(createList)
createFrame = createFrame.sort_values(by = createFrame.columns.to_list()
                                      ,ascending=True
                                      ,ignore_index=True)

pd.concat([dropFrame, createFrame], axis = 1)
```

Not sure why the data had hyphens in the columns names, originally. These hyphens interfere with SQL query operations, so I have the mind to replace them with underscore characters. I'm not going to overwrite the original CSVs as I wish to maintain integrity and a clear trail of data processing steps. Also, having the original data to fall back on if need be, without re-downloading could be helpful.

### Setup Database Pipeline

```{python}
#| label: "setupPipeline"
#| code-summary: "Does the extraction and initial transformation following download of compressed files."

def setup_duckdb_pipeline(
  csvDict: dict, 
  parquetDict: dict, 
  conn: duckdb.DuckDBPyConnection) -> None:
  try:
    {
      table_name: duckdb.sql(f"""
      CREATE OR REPLACE TABLE {table_name} AS 
      SELECT 
        *
      FROM 
        df;
      """, connection = conn) 
      for table_name, df in csvDict.items()
      }
    for key, value in csvDict.items():
      result = conn.execute(f"SELECT COUNT(*) FROM {key}").fetchone()
      print(f"Successfully created table: {key}, Row count: {result[0]}")
  except Exception as e:
    print(f"Error loading table: {str(e)}")
    raise
  
  if parquetDict:
    write_datasets(conn, parquetDict)
  
# Create tables from Parquet files
def write_datasets (
  conn: duckdb.DuckDBPyConnection, parquetDict: dict):
      try:
        {
          table_name: duckdb.sql(f"""
           CREATE OR REPLACE TABLE {table_name} AS
           SELECT 
             id::id_actigraphy_enum AS id
             ,quarter::TEXT::quarter_enum AS quarter
             ,weekday::TEXT::weekday_enum AS weekday
             ,light
             ,time_of_day
             ,relative_date_PCIAT
           FROM read_parquet(
             '{file_path}'
             ,hive_partitioning = true
             );""", connection=conn)
           for table_name, file_path in parquetDict.items()
           }
        for key, value in parquetDict.items():
          result = conn.execute(f"SELECT COUNT(*) FROM {key}").fetchone()
          print(
            f"Successfully created table: {key}, Row count, {result[0]}")
      except Exception as e:
        print(f"Error writing dataset: {str(e)}")
        raise

trainCsvDf = pd.read_csv("data/train.csv")
testCsvDf = pd.read_csv("data/test.csv")

dictDf = pd.read_csv("data/data_dictionary.csv")

trainCsvDf.columns = trainCsvDf.columns.str.replace('-','_') 
trainCsvDf.columns = trainCsvDf.columns.str.lower() 
testCsvDf.columns = testCsvDf.columns.str.replace('-','_') 
testCsvDf.columns = testCsvDf.columns.str.lower() 

dictDf.Field = dictDf.Field.replace("-", "_", regex = True)

csvDict = {
  "TrainCsv": trainCsvDf
  ,"TestCsv": testCsvDf
  ,"DataDict": dictDf
  }

parquetDict = {
  "ActigraphyTrain": 'data//series_train.parquet*/*/*'
  ,"ActigraphyTest": 'data//series_test*/*/*'
  }

try:
  setup_duckdb_pipeline(csvDict, parquetDict, conn)
  conn.sql(f"CHECKPOINT;")
except:
  print(f"Could not set up data pipeline.")
  
```

### Feature Tables

```{python}
#| label: "create_table_with_regex_columns()"
#| warning: false

import re

def filter_columns_by_regex(col_dict: dict, regex_pattern: str) -> dict:
  """
  :::WHY(s):::
    Can there be a way to use dictionaries to specify columns?
 
  :::GOAL(s):::
    Regex selection of column names from a dictionary.
  """
  return {
    col: dtype 
    for col, dtype in col_dict.items() 
    if re.search(regex_pattern, col)
    }


def create_table_with_regex_columns(
  conn: duckdb.duckdb
  ,source_table: str
  ,new_table_name: str 
  ,regex_pattern: str 
  ,col_dict: dict
  ) -> None:  
  
  """
  :::WHY(s):::
    There should to be a more streamlined way to utilize columnar information following regular string patterns to generate tables based on such information. This could follow and carry out pre-existing data modeling plans.
  
  :::GOAL(s):::
    To create new database tables with modularized SQL queries generated with the help of regex pattern matching selection.
  
  :::EXAMPLE(s):::
    The dictionary item:
      "Demographic": r"^id|^sii|^basic\S+"
  
    Would generate a table named 'Demographic' from data with columns for 'id', 'sii' and all columns starting with 'basic' strings. 
  """

  # filter columns
  filtered_col_dict = filter_columns_by_regex(col_dict, regex_pattern)
  
  # regex column selecting via dict comprehension and vectorized filtering
  regex_select_sql = f"""
  CREATE OR REPLACE TABLE {new_table_name} AS 
  SELECT
    {', '.join([f'"{col}"' for col in filtered_col_dict.keys()])}
  FROM {source_table};
  """

  conn.execute(regex_select_sql)
 
# It'd be useful to get the data types for creating a new table of values
coltype_overview = conn.execute(f"""
  SELECT 
    column_name
    ,data_type
  FROM 
    information_schema.columns
  WHERE 
    table_name = 'TrainCsv';""").df()

# Map the column names with data types
col_dict = dict(zip(coltype_overview.column_name, coltype_overview.data_type))

regex_dict_train = {
  "Demographic": r"^id|^sii|^basic\S+"
  ,"Physical": r"^id|^sii|^physical\S+"
  ,"FgVital": r"^id|^sii|^fitness_E\S+"
  ,"FgChild": r"^id|^sii|^fgc\S+"
  ,"Bia": r"^id|^sii|^bia\S+"
  ,"Paqa": r"^id|^sii|^paq_a\S+"
  ,"Pciat": r"^id|^sii|^pciat\S+"
  ,"Sds": r"^id|^sii|^sds\S+"
  ,"InternetUse": r"^id|^sii|^preint\S+"
  }

# There's no Pciat in the test set
regex_dict_test = {
  "Demographic_OfTest": r"^id|^basic\S+"
  ,"Physical_OfTest": r"^id|^physical\S+"
  ,"FgVital_OfTest": r"^id|^fitness_E\S+"
  ,"FgChild_OfTest": r"^id|^fgc\S+"
  ,"Bia_OfTest": r"^id|^bia\S+"
  ,"Paqa_OfTest": r"^id|^paq_a\S+"
 # ,"Pciat_OfTest": r"^id|^pciat\S+" 
  ,"Sds_OfTest": r"^id|^sds\S+"
  ,"InternetUse_OfTest": r"^id|^preint\S+"
  }
 

# Loop through the data structures to create tables for the train set
for new_table_name, regex_pattern in regex_dict_train.items():
  create_table_with_regex_columns(
    conn 
    ,'TrainCsv'
    ,new_table_name
    ,regex_pattern
    ,col_dict
    ) 

# Loop through the data structures to create tables for the test set
for new_table_name, regex_pattern in regex_dict_test.items():
  create_table_with_regex_columns(
    conn
    ,'TestCsv'
    ,new_table_name
    ,regex_pattern 
    ,col_dict
    )

```

```{python}
conn.sql(f"SELECT * from ActigraphyTest LIMIT 10;").df()
```

```{python}
# 3.6e+12
conn.sql(f"""
CREATE OR REPLACE TABLE ActigraphyTest AS
SELECT
  id
  ,light
  ,weekday
  ,quarter
  ,(time_of_day / 3_600_000_000_000) AS hour_of_day
FROM 
  ActigraphyTest;
""")
```

```{python}
# 3.6e+12
conn.sql(f"""
CREATE OR REPLACE TABLE ActigraphyTrain AS
SELECT
 id
 ,light
 ,weekday
 ,quarter
 ,(time_of_day / 3_600_000_000_000) AS hour_of_day
FROM 
  ActigraphyTrain;
""")

conn.sql(f"CHECKPOINT;")
```

```{python}
conn.sql(f"SELECT * FROM ActigraphyTrain LIMIT 10;").df()
```

```{python}
conn.sql(f"""
CREATE OR REPLACE TABLE IntermediateActigraphy AS
SELECT
  id
  ,basic_demos_enroll_season::TEXT::enroll_season_enum AS enroll_season
  ,basic_demos_age::TEXT::age_enum AS age
  ,basic_demos_sex::TEXT AS sex
  ,sii::INTEGER::TEXT::sii_enum AS sii
FROM 
  Demographic
ORDER BY
  id ASC;
""")

conn.sql(f"CHECKPOINT;")
```

```{python}
conn.sql(f"""
CREATE OR REPLACE TABLE 
  ActigraphyTrain
AS
  SELECT
    ia.*
    ,at.hour_of_day
    ,at.light
  FROM 
    ActigraphyTrain at 
  LEFT JOIN 
    IntermediateActigraphy ia
  ON 
    ia.id = at.id;
""")

conn.sql(f"CHECKPOINT;")
```

```{python}
#| label: "quartiler()"
#| code-summary: "Extract quartile summary values into a dictionary with intuitive key labels."

def quartiler (
  conn: duckdb.duckdb, 
  col_name: str, 
  source_name: str) -> dict:
  """
  ::WHY(s):: 
    SQL can sometimes require a lot of code for repetitive commands, but in a Python environment, database queries can be modularized.
  
  ::GOAL(s)::  
    To process SQL queries into useful quartile information represented by intuitive key labels.
  """

  summaryDf = conn.sql(f"""
  SUMMARIZE
  SELECT
    {col_name}
  FROM 
    {source_name};""").df()

  quartileDict = {
    'min': summaryDf['min'][0]
    ,'Q1': summaryDf.q25[0]
    ,'Q2': summaryDf.q50[0]
    ,'Q3': summaryDf.q75[0]
    ,'max': summaryDf['max'][0]
    }
  
  return quartileDict

```

### Data Model

::: column-screen-inset
```{mermaid}
%%| label: fig-childMermaid
%%| fig-cap: "child"
%%| file: diagrams/emermaid.mmd
```
:::

## Actigraphy, Physical, and Demographics

cool [@zierle-ghosh_physiology_2024].

```{python}

def intermediateLighter(conn: duckdb.duckdb, new_tables: list, quarters: dict, quartuples: pd.Series) -> None:
  """
  :::WHY(s):::
    Tables based on the same parameters but different parameter values could be modularized.
    
  :::GOAL(s):::
    Process repetitive SQL efficiently and intuitively using data structures that simplify the process.
  """
  for i in list(range(4)):
    conn.sql(f"""
      CREATE OR REPLACE TABLE '{new_tables[i]}' AS
      SELECT
        id
        ,enroll_season
        ,age
        ,sex
        ,sii
        ,AVG(light) AS '{quartuples.index[i]}'
      FROM 
        ActigraphyTrain
      WHERE 
        hour_of_day BETWEEN 
            '{quarters[quartuples.iloc[i][0]]}'::DOUBLE 
          AND 
            '{quarters[quartuples.iloc[i][1]]}'::DOUBLE
      GROUP BY 
        ALL
      ORDER BY 
        id ASC;""")
        
# execute quartiler()
quarters = quartiler(conn, 'hour_of_day', 'ActigraphyTrain')

quartuples = pd.Series(
  data = 
  [('min','Q1')
  ,('Q1', 'Q2')
  ,('Q2', 'Q3')
  ,('Q3' ,'max')]
  ,index =
  ['min_q1'
  ,'q1_q2'
  ,'q2_q3'
  ,'q3_max'])

new_tables = ['Light1', 'Light2', 'Light3', 'Light4']

intermediateLighter(conn, new_tables, quarters, quartuples)

conn.sql(f"CHECKPOINT;")

```

```{python}
conn.sql(f"""
CREATE OR REPLACE TABLE AggregatedAnalysis AS
SELECT 
  l1.*
  ,l2.q1_q2
  ,l3.q2_q3
  ,l4.q3_max
FROM 
  Light1 l1
LEFT JOIN Light2 l2 ON l1.id = l2.id
LEFT JOIN Light3 l3 ON l1.id = l3.id
LEFT JOIN Light4 l4 ON l1.id = l4.id;
""")

conn.sql(f"CHECKPOINT;")
```

```{python}
conn.sql(f"SELECT * FROM AggregatedAnalysis LIMIT 20;").df()
```

```{python}
conn.sql(f"""
CREATE OR REPLACE TABLE AggregatedAnalysis AS
SELECT
  aa.*
  ,preint_eduhx_computerinternet_hoursday AS useHrs 
FROM
  AggregatedAnalysis aa
LEFT JOIN 
  InternetUse iu
ON
  aa.id = iu.id;
""")
```

```{python}
all_aa = conn.sql(f"SELECT * FROM AggregatedAnalysis;").df()
all_aa['useHrs'].describe()
```

```{python}
conn.sql(f"""
SUMMARIZE
SELECT useHrs
FROM AggregatedAnalysis;
""").df()
```

```{python}
riskyDictionary = {
  'Risk1': 
    (f",CASE WHEN ph.physical_bmi < 18.5 THEN 'Underweight'"
    ,f"WHEN ph.physical_bmi BETWEEN 18.5 AND 24.9 THEN 'Normal'"
    ,f"WHEN ph.physical_bmi BETWEEN 25.0 AND 29.9 THEN 'Increased'"
    ,f"WHEN ph.physical_bmi BETWEEN 30.0 AND 34.9 THEN 'High'"
    ,f"WHEN ph.physical_bmi BETWEEN 35.0 AND 39.9 THEN 'Very High'"
    ,f"WHEN ph.physical_bmi >= 40 THEN 'Extremely High'"
    ,f"ph.physical_waist_circumference <= 35 AND aa.sex = '0'"),
  'Risk2':
    (f",CASE WHEN ph.physical_bmi < 18.5 THEN 'Underweight'"
    ,f"WHEN ph.physical_bmi BETWEEN 18.5 AND 24.9 THEN 'Normal'"
    ,f"WHEN ph.physical_bmi BETWEEN 25.0 AND 29.9 THEN 'High'"
    ,f"WHEN ph.physical_bmi BETWEEN 30.0 AND 34.9 THEN 'Very High'"
    ,f"WHEN ph.physical_bmi BETWEEN 35.0 AND 39.9 THEN 'Very High'"
    ,f"WHEN ph.physical_bmi >= 40 THEN 'Extremely High'"
    ,f"ph.physical_waist_circumference > 35 AND aa.sex = '0'"),
  'Risk3':
    (f",CASE WHEN ph.physical_bmi < 18.5 THEN 'Underweight'"
    ,f"WHEN ph.physical_bmi BETWEEN 18.5 AND 24.9 THEN 'Normal'"
    ,f"WHEN ph.physical_bmi BETWEEN 25.0 AND 29.9 THEN 'Increased'"
    ,f"WHEN ph.physical_bmi BETWEEN 30.0 AND 34.9 THEN 'High'"
    ,f"WHEN ph.physical_bmi BETWEEN 35.0 AND 39.9 THEN 'Very High'"
    ,f"WHEN ph.physical_bmi >= 40 THEN 'Extremely High'"
    ,f"ph.physical_waist_circumference <= 40 AND aa.sex = '1'"),
  'Risk4':
    (f",CASE WHEN ph.physical_bmi < 18.5 THEN 'Underweight'"
    ,f"WHEN ph.physical_bmi BETWEEN 18.5 AND 24.9 THEN 'Normal'"
    ,f"WHEN ph.physical_bmi BETWEEN 25.0 AND 29.9 THEN 'High'"
    ,f"WHEN ph.physical_bmi BETWEEN 30.0 AND 34.9 THEN 'Very High'"
    ,f"WHEN ph.physical_bmi BETWEEN 35.0 AND 39.9 THEN 'Very High'"
    ,f"WHEN ph.physical_bmi >= 40 THEN 'Extremely High'"
    ,f"ph.physical_waist_circumference > 40 AND aa.sex = '1'")}
```

```{python}
riskyDf = pd.DataFrame(data = riskyDictionary)
```

```{python}
for key, value in riskyDf.items():
  try:
    conn.sql(f"""
    CREATE OR REPLACE TABLE {key} AS
    SELECT
      aa.*
      {value[0]}
      {value[1]}
      {value[2]}
      {value[3]}
      {value[4]}
      {value[5]}
      ELSE NULL
      END AS risk_cat
    ,risk_cat::disease_risk_enum AS risk_category
    FROM 
      Physical ph 
    LEFT JOIN 
      AggregatedAnalysis aa 
    ON 
      aa.id = ph.id
    WHERE 
      {value[6]}
    ORDER BY 
      aa.id ASC;""")
    result = conn.execute(f"SELECT COUNT(*) FROM {key}").fetchone()
    print(f"Successfully created table: {key}, Row count: {result[0]}")
  except:
    print(f"Error loading this table: {key}")
    
conn.sql(f"CHECKPOINT")
```

```{python}
conn.sql(f"""
CREATE OR REPLACE TABLE 
  DiseaseRiskDemographic AS
SELECT * EXCLUDE(risk_cat) FROM Risk1
UNION BY NAME
SELECT * EXCLUDE(risk_cat) FROM Risk2
UNION BY NAME
SELECT * EXCLUDE(risk_cat) FROM Risk3
UNION BY NAME 
SELECT * EXCLUDE(risk_cat) FROM Risk4;
""")
```

```{python}
conn.sql(f"""
CREATE OR REPLACE TABLE RiskCategorySummary AS
SELECT 
  risk_category
  ,AVG(sii::INTEGER) AS sii
  ,AVG(useHrs) AS useHrs
  ,AVG(min_q1) AS q1_avg_light_exposure
  ,AVG(q1_q2) AS q2_avg_light_exposure
  ,AVG(q2_q3) AS q3_avg_light_exposure
  ,AVG(q3_max) AS q4_avg_light_exposure
FROM 
  DiseaseRiskDemographic
GROUP BY 
  risk_category;
""")
```

```{python}
conn.sql(f"""
SELECT * 
FROM RiskCategorySummary;
""").df()
```

```{python}
pydf = conn.execute(f"""
SELECT 
  risk_category
  ,AVG(min_q1) AS qLight
FROM 
  DiseaseRiskDemographic
GROUP BY 
  risk_category;""").df()

```

```{r}
r_df <- py$pydf
```

```{r}
#| label: tbl-risk

notes_list =  list("The disease risk category.", "The average light exposure for Q1.")
locations_list = list("risk_category", "qLight")

# Create a table dataframe, or tibble, of footnotes
footnotes_df <- tibble(notes = notes_list, locations = locations_list)

rTable <- r_table_theming(rDataFrame = r_df,
                          title = "Title",
                          subtitle = NULL,
                          footnotes_df,
                          source_note = md("**source**: Kaggle"))

rTable
```

```{r}
#| label: "ggplot example"

r_plot <- ggplot(data = r_df, 
                 mapping = aes(x = risk_category, y = qLight)
                 ) +
  geom_col(fill = 'skyblue') +
  labs(title = 'Risk Categories', 
       x = 'Category', y = 'Light Exposure') +
  coord_flip()

r_plot + ggplot_theming_flipped_axes()
```

## Close Connection

```{python}
#| eval: true
try:
  conn.close()
  print(f"database connection closed")
except:
  print(f"could not close the connection")
```
