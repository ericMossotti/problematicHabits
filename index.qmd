---
title: Problematic Internet Habits in Children
authors:
  - name: Eric Mossotti
    affiliation: None
    roles: data analysis
    corresponding: true



bibliography: bibliography/references.bib
citation-location: margin
citations-hover: true
link-citations: true
csl: bibliography/csl/apa.csl

code-fold: true

lightbox: auto
---

## Rationale and Initial Thoughts

The grounds for analyses in this case is the problematic behavior as it pertains to the subjects' experience and their parents' via questionnaires, physical fitness tests, worn instrument measurements and demographics. The resulting data is to be processed and connections drawn.

There are quite a few variables to consider here. The largest amount of data was collected in real time. This was recorded by a device that could measure acceleration in 3D space, ambient light, time, and the quarter of the year while the participant went about their lives. Many health issues could stem from a lack of exercise. It may follow that the more someone is actively using the internet, the less likely they are to meet daily activity recommendations.

Some issues is that it is uncertain where this feat of data collection took place. Perhaps, this was for the sake of privacy? The exact dates are not given for the real-time data either. So, we have to go by the quarter of the year and or season in that respect. Not sure if seasonal information is consequential to anything.

The goal of the models is to, ultimately, make progress in the field of childhood to adolescent psychology. Progress, in this case, is defined by accurately predicting risks of undesirable outcomes based on a young persons' daily activities.

Nowadays, life is centered around the internet. At the same time, our biology has not undergone any systemic changes. First hand experience certainly tells me that that high levels of internet activity can have a negative impact on the mind.

Is the quantity of internet activity itself truly determinant of negative outcome?

### Output Formatters

#### Tables

```{r}
#| label: "r_table_theming()"
#| code-summary: "x"
# https://emilhvitfeldt.github.io/paletteer/

library(gt)
library(purrr)
library(tibble)
library(reticulate)

r_table_theming <- function(rDataFrame,
                            title,
                            subtitle,
                            footnotes_df,
                            source_note) {
  r_table <- gt(rDataFrame) |>
    data_color(palette = "ggsci::legacy_tron",
               columns = footnotes_df$locations[[1]],
               target_columns = everything()) |>
    tab_header(title = title, subtitle = subtitle)
  
  r_table <- r_table |>
      tab_options(
        heading.background.color = '#222',
        column_labels.background.color = '#333',
        #table.background.color = '#222',
        footnotes.background.color = '#222',
        source_notes.background.color = '#222'
        )
    
    r_table <- r_table |>
      tab_source_note(source_note = source_note)

  # Footnotes are added to the accumulator, building up to the final result
  r_table <- seq_len(nrow(footnotes_df)) |>
    reduce(\(acc, i) {
      tab_footnote(
        acc,
        footnote = footnotes_df$notes[[i]],
        location = cells_column_labels(columns = footnotes_df$locations[[i]]),
        placement = "auto"
        )
      }, .init = r_table)
  
  return(r_table)
}

```

#### Plots

```{r}
#| label: "ggplot_theming()"
#| eval: true

library(reticulate)
library(ggplot2)

ggplot_theming_flipped_axes <- function() {
  theme_linedraw() +
    theme(
      plot.title = element_text(color = 'white'),
      plot.background = element_rect(fill = '#222'),
      panel.background = element_rect(fill = '#222'),
      panel.grid.major.x = element_line(linetype = 'dashed'),
      panel.grid.minor.x = element_line(linetype = "dotted"),
      panel.grid.major.y = element_line(linetype = 'solid'),
      panel.grid.minor.y = element_line(linetype = 'dotted'),
      axis.title = element_text(color = 'gray100'),
      axis.text = element_text(color = 'gray'),
      legend.background = element_rect(fill = '#222'),
      legend.text = element_text(color = 'gray'),
      legend.title = element_text(color = 'white'))
}

ggplot_theming <- function() {
  theme_linedraw() +
    theme(
      plot.title = element_text(color = 'white'),
      plot.background = element_rect(fill = '#222'),
      panel.background = element_rect(fill = '#222'),
      panel.grid.major.x = element_line(linetype = 'solid'),
      panel.grid.minor.x = element_line(linetype = "dotted"),
      panel.grid.major.y = element_line(linetype = 'dashed'),
      panel.grid.minor.y = element_line(linetype = 'dotted'),
      axis.title = element_text(color = 'gray100'),
      axis.text = element_text(color = 'gray'),
      legend.background = element_rect(fill = '#222'),
      legend.text = element_text(color = 'gray'),
      legend.title = element_text(color = 'white'))
  }
```

## Import/Extract

```{python}
#| eval: false
!kaggle competitions download -c child-mind-institute-problematic-internet-use
```

```{python}
#| label: "mass import"

from zipfile import ZipFile
import os
import duckdb
import pandas as pd
import re
from pandas.io.formats.style import Styler
import numpy as np
```

```{python}
#| label: "makeDir"
#| code-summary: "Check if the directory, 'data', exists."

try:
  os.mkdir('./data')
except FileExistsError:
  print(f"data exists")
```

```{python}
#| label: tbl-unzipping
#| tbl-cap: "unzippers"
#| code-summary: "Extracts the files and lists files."

zip_path = "child-mind-institute-problematic-internet-use.zip"

if not os.path.exists('./data/train.csv'):
  with ZipFile(zip_path) as zf:
    ZipFile.extractall(zf, path = './data')
    print("Extracted files", sep = "\n"*2)
else:
  print("Files already exist", sep = "\n"*2)

extracts = pd.DataFrame(data = os.listdir('./data'), columns = ['./data'])

```

```{r}

r_df <- py$extracts

notes_list =  list("./data directory file list")
locations_list = list("./data")

# Create a table dataframe, or tibble, of footnotes
footnotes_df <- tibble(notes = notes_list, locations = locations_list)

rTable <- r_table_theming(rDataFrame = r_df,
                          title = "Directory Files",
                          subtitle = NULL,
                          footnotes_df,
                          source_note = md("**source**: './data'"))

rTable
```

### Establish a Database Connection

```{python}
conn = duckdb.connect(':memory:')
conn.sql(f"SET memory_limit = '24GB';")
conn.sql(f"SET default_order = 'ASC';")
```

### New Data Types (Enums/Enumerations)

I noticed that using ENUM data types significantly sped up many database operations [@holanda_duckdb_2021].

```{python}
#| label: "enum_series_to_dict"
#| code-summary: "Defines the series and dictionary objects for enum functions."

enums = ['internet_hours_enum'
         ,'enroll_season_enum'
         ,'disease_risk_enum'
         ,'sii_enum', 'age_enum'
         ,'sex_enum'
         ,'pciat_season_enum'
         ,'weekday_enum'
         ,'quarter_enum'
         ,'hour_enum'
         ,'minute_enum'
         ,'second_enum'
         ,'id_actigraphy_enum']

#siiSeries = pd.Series(data = ['None', 'Mild', 'Moderate', 'Severe'], dtype = str)
siiSeries = pd.Series(data = ['0', '1', '2', '3'], dtype = str)
ageSeries = pd.Series(data = range(5, 23), dtype = str)
sexSeries = pd.Series(data = ['0', '1'], dtype = str)
pciatSeasonSeries = pd.Series(data = ['Fall'
                                      ,'Spring'
                                      ,'Summer'
                                      ,'Winter'], dtype = str)

internetHrsSeries = pd.Series(data = ['0', '1', '2', '3'], dtype = str)
quarterSeries = pd.Series(data = range(1, 5), dtype = str)
weekdaySeries = pd.Series(data = range(1, 8), dtype = str)
hourSeries = pd.Series(data = range(0, 24), dtype = str)
minuteSeries = pd.Series(data = range(0, 60), dtype = str)
secondSeries = pd.Series(data = range(0, 60), dtype = str)
diseaseRiskSeries = pd.Series(data = ['Underweight'
                                      ,'Normal'
                                      ,'Increased'
                                      ,'High'
                                      ,'Very High'
                                      ,'Extremely High'], dtype = str)

id_df = conn.execute(f"""
     SELECT 
         DISTINCT(id) AS id
     FROM 
         read_parquet('data/series_train*/*/*'
         ,hive_partitioning = true)
     ORDER BY 
         id ASC;
     """).df() 

idList = id_df['id'].to_list()
idSeries = pd.Series(data = idList, dtype = str)

enumDict = {
  'disease_risk_enum': f"{tuple(diseaseRiskSeries)}"
  ,'enroll_season_enum': f"{tuple(pciatSeasonSeries)}"
  ,'sii_enum': f"{tuple(siiSeries)}"
  ,'age_enum': f"{tuple(ageSeries)}"
  ,'sex_enum': f"{tuple(sexSeries)}"
  ,'pciat_season_enum': f"{tuple(pciatSeasonSeries)}"
  ,'quarter_enum': f"{tuple(quarterSeries)}"
  ,'weekday_enum': f"{tuple(weekdaySeries)}"
  ,'hour_enum': f"{tuple(hourSeries)}"
  ,'minute_enum': f"{tuple(minuteSeries)}"
  ,'second_enum': f"{tuple(secondSeries)}"
  ,'id_actigraphy_enum': f"{tuple(idSeries)}"
  ,'internet_hours_enum': f"{tuple(internetHrsSeries)}"
  }

if len(enumDict) == len(enums):
  print(f"drops and creates are same length")
else:
  print(f"drops and creates are not the same length")
```

```{python}
#| label: "try_create_drop()"
#| code-summary: "Can there be a modular approach to creating and dropping ENUM types?\n \tYes."

def try_create(conn,type_str: str, enum_str: str) -> pd.Series:
  """
  :::WHY(s):::
    Is there a modular approach to creating ENUM types in a database?
  :::HOW(s):::
    Utilize python data structures such as pandas Series and dictionary as parameters.
  """
  # the try, except can help categorize the outputs for ui/ux
  try:
    conn.execute(f"CREATE TYPE {type_str} AS ENUM {enum_str};")
    return pd.Series(type_str, index = ['created'])
  except duckdb.duckdb.CatalogException:
    return pd.Series(type_str, index = ['already existed'])

def try_drop(conn, type_str: str) -> pd.Series:
  """
  :::WHY(s):::
    Is there a modular approach to dropping ENUM types in a database?
  :::HOW(s):::
    Utilize python data structures such as pandas Series and dictionary as parameters.
  """
  # the try, except can help categorize the outputs for ui/ux
  try:
    conn.execute(f"DROP TYPE {type_str};")
    return pd.Series(type_str, index = ['dropped'])
  except duckdb.duckdb.CatalogException:
    return pd.Series(type_str, index = ['did not exist'])
  
droplist = []
for e in enums:   
   droplist.append(try_drop(conn, type_str = e))
   
dropFrame = pd.DataFrame(droplist)
dropFrame = dropFrame.sort_values(by = dropFrame.columns.to_list()
                                  ,ascending=True
                                  ,ignore_index=True)
createList = []
for eType, eSeries in enumDict.items():
    createList.append(try_create(conn
                                 ,type_str = eType
                                 ,enum_str = eSeries))
createFrame = pd.DataFrame(createList)
createFrame = createFrame.sort_values(by = createFrame.columns.to_list()
                                      ,ascending=True
                                      ,ignore_index=True)

pd.concat([dropFrame, createFrame], axis = 1)
```

Not sure why the data had hyphens in the columns names, originally. These hyphens interfere with SQL query operations, so I have the mind to replace them with underscore characters. I'm not going to overwrite the original CSVs as I wish to maintain integrity and a clear trail of data processing steps. Also, having the original data to fall back on if need be, without re-downloading could be helpful.

### Meaningful Database Pipeline Setup

Doing my best to apply transformations as I read the data into the database from the files to minimize steps needed to derive meaning from the data.

```{python}
#| label: "setupPipeline"
#| code-summary: "Does the extraction and initial transformation following download of compressed files."

def setup_duckdb_pipeline(
  csvDict: dict, 
  parquetDict: dict, 
  conn: duckdb.DuckDBPyConnection) -> None:
    
  try:
    {
      table_name: duckdb.sql(f"""
      CREATE OR REPLACE TABLE {table_name} AS 
      SELECT 
        *
      FROM 
        df;
      """, connection = conn) 
      for table_name, df in csvDict.items()
      }
    for key, value in csvDict.items():
      result = conn.execute(f"SELECT COUNT(*) FROM {key}").fetchone()
      print(f"Successfully created table: {key}, Row count: {result[0]}")
  except Exception as e:
    print(f"Error loading table: {str(e)}")
    raise
  
  if parquetDict:
    write_datasets(conn, parquetDict)
  
# Create tables from Parquet files
def write_datasets (
  conn: duckdb.DuckDBPyConnection, parquetDict: dict):
    
      try:
        {
          table_name: duckdb.sql(f"""
           CREATE OR REPLACE TABLE {table_name} AS
           SELECT 
             id::id_actigraphy_enum AS id
             ,quarter::TEXT::quarter_enum AS quarter
             ,weekday::TEXT::weekday_enum AS weekday
             ,light
             ,(time_of_day / 3_600_000_000_000) AS hour_of_day
             ,relative_date_PCIAT
           FROM read_parquet(
             '{file_path}'
             ,hive_partitioning = true
             );""", connection=conn)
           for table_name, file_path in parquetDict.items()
           }
        for key, value in parquetDict.items():
          result = conn.execute(f"SELECT COUNT(*) FROM {key}").fetchone()
          print(
            f"Successfully created table: {key}, Row count, {result[0]}")
      except Exception as e:
        print(f"Error writing dataset: {str(e)}")
        raise

trainCsvDf = pd.read_csv("data/train.csv")
testCsvDf = pd.read_csv("data/test.csv")

dictDf = pd.read_csv("data/data_dictionary.csv")

trainCsvDf.columns = trainCsvDf.columns.str.replace('-','_') 
trainCsvDf.columns = trainCsvDf.columns.str.lower() 
testCsvDf.columns = testCsvDf.columns.str.replace('-','_') 
testCsvDf.columns = testCsvDf.columns.str.lower() 

dictDf.Field = dictDf.Field.replace("-", "_", regex = True)

csvDict = {
  "TrainCsv": trainCsvDf
  ,"TestCsv": testCsvDf
  ,"DataDict": dictDf
  }

parquetDict = {
  "ActigraphyTrain": 'data//series_train.parquet*/*/*'
  ,"ActigraphyTest": 'data//series_test*/*/*'
  }

try:
  setup_duckdb_pipeline(csvDict, parquetDict, conn)
  conn.sql(f"CHECKPOINT;")
  pydf = conn.sql(f"""
  SELECT *
  FROM ActigraphyTrain
  LIMIT 10;""").df()
except:
  print(f"Could not set up data pipeline.")
  
```

```{python}
pydf = conn.sql(f"""
SELECT *
FROM ActigraphyTrain
ORDER BY ALL
LIMIT 10;""").df()
```

```{r}
#| label: tbl-ActigraphyTrain

r_df <- py$pydf

notes_list =  list(
  "Unique participant identifier code, to track related data from actigraphy to hbn datasets.",
  "Annual quarter related to month of observation.",
  "Day of the week (Monday = 1)",
  "Light exposure, measured in 'lux'",
  "24-hour time converted to a float for easier conversion for time-series analysis.",
  "Days since questionairre, also convenient to help order daily values for time-series analysis."
  )

locations_list = list("id",
                      "quarter",
                      "weekday",
                      "light",
                      "hour_of_day",
                      "relative_date_PCIAT")

# Create a table data-frame, or tibble, of footnotes
footnotes_df <- tibble(notes = notes_list, 
                       locations = locations_list)

rTable <- r_table_theming(rDataFrame = r_df,
                          title = "Actigraphy Data - Training Set",
                          subtitle = NULL,
                          footnotes_df,
                          source_note = md("**source**: Kaggle"))

rTable
```

### Feature Tables

It might be helpful to create tables with enum data types to be joined efficiently to other tables without redundant data. I first read in the tables to the database, and then create tables based on id as enum data types information and then drop the original csv based tables

CSV based tables -

Featured tables

```{python}
#| label: "create_table_with_regex_columns()"
#| warning: false

import re

def filter_columns_by_regex(col_dict: dict, regex_pattern: str) -> dict:
  """
  :::WHY(s):::
    Can there be a way to use dictionaries to specify columns?

  :::GOAL(s):::
    Regex selection of column names from a dictionary.
  """
  return {
    col: dtype 
    for col, dtype in col_dict.items() 
    if re.search(regex_pattern, col)
    }

def create_table_with_regex_columns(
  conn: duckdb.duckdb
  ,source_table: str
  ,new_table_name: str 
  ,regex_pattern: str 
  ,col_dict: dict
  ) -> None:  

  """
  :::WHY(s):::
    There should to be a more streamlined way to utilize columnar information following regular string patterns to generate tables based on such information. This could follow and carry out pre-existing data modeling plans.

  :::GOAL(s):::
    To create new database tables with modularized SQL queries generated with the help of regex pattern matching selection.

  :::EXAMPLE(s):::
    The dictionary item:
      "Demographic": r"^id|^sii|^basic\S+"
 
    Would generate a table named 'Demographic' from data with columns for 'id', 'sii' and all columns starting with 'basic' strings. 
  """

  # filter columns
  filtered_col_dict = filter_columns_by_regex(col_dict, regex_pattern)
  
  # regex column selecting via dict comprehension and vectorized filtering
  regex_select_sql = f"""
  CREATE OR REPLACE TABLE {new_table_name} AS 
  SELECT
    {', '.join([f'"{col}"' for col in filtered_col_dict.keys()])}
  FROM {source_table};
  """

  conn.execute(regex_select_sql)

# It'd be useful to get the data types for creating a new table of values
coltype_overview = conn.execute(f"""
  SELECT 
    column_name
    ,data_type
  FROM 
    information_schema.columns
  WHERE 
    table_name = 'TrainCsv';""").df()

# Map the column names with data types
col_dict = dict(zip(coltype_overview.column_name, coltype_overview.data_type))

regex_dict_train = {
  "Demographic": r"^id|^sii|^basic\S+"
  ,"Physical": r"^id|^sii|^physical\S+"
  ,"FgVital": r"^id|^sii|^fitness_E\S+"
  ,"FgChild": r"^id|^sii|^fgc\S+"
  ,"Bia": r"^id|^sii|^bia\S+"
  ,"Paqa": r"^id|^sii|^paq_a\S+"
  ,"Pciat": r"^id|^sii|^pciat\S+"
  ,"Sds": r"^id|^sii|^sds\S+"
  ,"InternetUse": r"^id|^sii|^preint\S+"
  }

# There's no Pciat in the test set
regex_dict_test = {
  "DemographicTest": r"^id|^basic\S+"
  ,"PhysicalTest": r"^id|^physical\S+"
  ,"FgVitalTest": r"^id|^fitness_E\S+"
  ,"FgChildTest": r"^id|^fgc\S+"
  ,"BiaTest": r"^id|^bia\S+"
  ,"PaqaTest": r"^id|^paq_a\S+"
 # ,"Pciat_OfTest": r"^id|^pciat\S+" 
  ,"SdsTest": r"^id|^sds\S+"
  ,"InternetUseTest": r"^id|^preint\S+"
  }

# Loop through the data structures to create tables for the train set
for new_table_name, regex_pattern in regex_dict_train.items():
  create_table_with_regex_columns(
    conn 
    ,'TrainCsv'
    ,new_table_name
    ,regex_pattern
    ,col_dict
    ) 

# Loop through the data structures to create tables for the test set
for new_table_name, regex_pattern in regex_dict_test.items():
  create_table_with_regex_columns(
    conn
    ,'TestCsv'
    ,new_table_name
    ,regex_pattern 
    ,col_dict
    )

conn.sql(f"DROP TABLE TrainCsv;")
conn.sql(f"DROP TABLE TestCsv;")
```

```{python}
#| label: "joinDemoAndActigraphyOnId"
#| code-summary: "Perform a join on matching Ids to combine demographics data with actigraphy data such as time and light exposure."

conn.execute(f"""
CREATE OR REPLACE TABLE IntermediateActigraphy AS
SELECT
  id
  ,basic_demos_enroll_season::TEXT::enroll_season_enum AS enroll_season
  ,basic_demos_age::TEXT::age_enum AS age
  ,basic_demos_sex::TEXT AS sex
  ,sii::INTEGER::TEXT::sii_enum AS sii
FROM 
  Demographic
ORDER BY
  id ASC;
""")

conn.execute(f"""
CREATE OR REPLACE TABLE 
  ActigraphyTrain
AS
  SELECT
    ia.*
    ,at.hour_of_day
    ,at.light
  FROM 
    ActigraphyTrain at 
  LEFT JOIN 
    IntermediateActigraphy ia
  ON 
    ia.id = at.id;
""")

conn.execute(f"""
CREATE OR REPLACE TABLE IntermediateActigraphy AS
SELECT
  id
  ,basic_demos_enroll_season::TEXT::enroll_season_enum AS enroll_season
  ,basic_demos_age::TEXT::age_enum AS age
  ,basic_demos_sex::TEXT AS sex
FROM 
  DemographicTest
ORDER BY
  id ASC;
""")

conn.execute(f"""
CREATE OR REPLACE TABLE 
  ActigraphyTest
AS
  SELECT
    ia.*
    ,at.hour_of_day
    ,at.light
  FROM 
    ActigraphyTest at 
  LEFT JOIN 
    IntermediateActigraphy ia
  ON 
    ia.id = at.id;
""")

conn.execute(f"""
DROP TABLE IntermediateActigraphy;
""")

conn.execute(f"CHECKPOINT;")
```

### Data Model

::: column-screen-inset
```{mermaid}
%%| label: fig-childMermaid
%%| fig-cap: "child"
%%| file: diagrams/emermaid.mmd
```
:::

## Actigraphy, Physical, and Demographics

It might be useful to explore how the time of day and associated exposure to light might correlate to other factors, such as sii, physical characteristics, and internet use.

**Hypothesis**:\
More time spent indoors on the internet might be associated with lower light exposure during daytime and higher light exposure during nighttime.

**Test**:

**Day**

-   Q1_Q2

-   Q2_Q3

**Night**

-   min_Q1

-   Q3_max

BMI Risk Categories [@zierle-ghosh_physiology_2024].

Lux Exposure Categories [@noauthor_lux_nodate].

### Quartiles of light exposure by hour of day.

```{python}
#| label: "quartiler()"
#| code-summary: "Extract quartile summary values into a dictionary with intuitive key labels."

def quartiler(
  conn: duckdb.duckdb, 
  col_name: str, 
  source_name: str) -> dict:
  """
  ::WHY(s):: 
    SQL can sometimes require a lot of code for repetitive commands, but in a Python environment, database queries can be modularized.
  
  ::GOAL(s)::  
    To process SQL queries into useful quartile information represented by intuitive key labels.
  """

  summaryDf = conn.sql(f"""
  SUMMARIZE
  SELECT
    {col_name}
  FROM 
    {source_name};""").df()

  quartileDict = {
    'min': summaryDf['min'][0]
    ,'Q1': summaryDf.q25[0]
    ,'Q2': summaryDf.q50[0]
    ,'Q3': summaryDf.q75[0]
    ,'max': summaryDf['max'][0]
    }
  
  return quartileDict

# execute quartiler()
quarters = quartiler(conn, 'hour_of_day', 'ActigraphyTrain')

quartuples = pd.Series(
  data = 
  [('min','Q1')
  ,('Q1', 'Q2')
  ,('Q2', 'Q3')
  ,('Q3' ,'max')]
  ,index =
  ['min_q1'
  ,'q1_q2'
  ,'q2_q3'
  ,'q3_max'])
```

```{python}
#| label: "intermediateLighter()"
#| code-summary: "Extract actigraphy data based on time of day quartiles"

def intermediateLighter(
  conn: duckdb.duckdb,
  new_tables: list,
  quarters: dict, 
  quartuples: pd.Series,
  from_table: str) -> None:
  """
  :::WHY(s):::
    Tables based on the same parameters but different parameter values could be modularized.
    
  :::GOAL(s):::
    Process repetitive SQL efficiently and intuitively using data structures that simplify the process. 
  
  """
  for i in list(range(4)):
    conn.sql(f"""
      CREATE OR REPLACE TABLE '{new_tables[i]}' AS
      SELECT
        id
        ,enroll_season
        ,age
        ,sex
        ,sii
        ,AVG(light) AS '{quartuples.index[i]}'
      FROM 
        '{from_table}'
      WHERE 
        hour_of_day BETWEEN 
            '{quarters[quartuples.iloc[i][0]]}'::DOUBLE 
          AND 
            '{quarters[quartuples.iloc[i][1]]}'::DOUBLE
      GROUP BY 
        ALL
      ORDER BY 
        id ASC;""")

def joinLights (from_to_tables: dict) -> None:
  
  for from_table, to_table in from_to_tables.items():
    
    new_tables = ['Light1', 'Light2', 'Light3', 'Light4']
    
    intermediateLighter(conn, new_tables, quarters, quartuples, from_table)
  
    conn.sql(f"CHECKPOINT;")
  
    conn.sql(f"""
    CREATE OR REPLACE TABLE '{to_table}' AS
    SELECT 
      l1.*
      ,l2.q1_q2
      ,l3.q2_q3
      ,l4.q3_max
    FROM 
      Light1 l1
    LEFT JOIN Light2 l2 ON l1.id = l2.id
    LEFT JOIN Light3 l3 ON l1.id = l3.id
    LEFT JOIN Light4 l4 ON l1.id = l4.id;
    """)

from_to_tables = {
  'ActigraphyTrain': 'AggregatedLights'
  ,'ActigraphyTest': 'AggregatedLightsTest'
  }

joinLights(from_to_tables)
```

```{python}

lightdropList = list('Light1', 'Light2', 'Light3', 'Light4')

for lightdrop in lightdropList:
  conn.sql(f"DROP TABLE '{lightdrop}';")

conn.sql(f"CHECKPOINT;")

pydf = conn.sql(f"SELECT * FROM AggregatedLights LIMIT 10;").df()
```

```{r}
#| label: tbl-AggregatedLights

r_df <- py$pydf

notes_list =  list(
  "Unique participant identifier.",
  "Participant enrollment into study season.",
  "Age in years.",
  "Sex (0 = Female, 1 = Male)",
  "Severity Impairment Index",
  "Mean light exposure across Q1 of a 24 hour time scale.",
  "Mean light exposure across Q2 of a 24 hour time scale.",
  "Mean light exposure across Q3 of a 24 hour time scale.",
  "Mean light exposure across Q4 of a 24 hour time scale."
  )

locations_list = list(
  "id"
  ,"enroll_season"
  ,"age"
  ,"sex"
  ,"sii"
  ,'min_q1'
  ,'q1_q2'
  ,'q2_q3'
  ,'q3_max')

# Create a table data-frame, or tibble, of footnotes
footnotes_df <- tibble(notes = notes_list, 
                       locations = locations_list)

rTable <- r_table_theming(rDataFrame = r_df,
                          title = "Quartile Data - Lights",
                          subtitle = NULL,
                          footnotes_df,
                          source_note = md("**source**: Kaggle"))

rTable

```

```{python}

lightsDict = {
  'AggregatedLights': 'InternetUse', 
  'AggregatedLightsTest': 'InternetUseTest'
  }

for from_tbl, join_tbl in lightsDict.items():
  conn.sql(f"""
  CREATE OR REPLACE TABLE '{from_tbl}' AS
  SELECT
    ft.*
    ,jt.preint_eduhx_computerinternet_hoursday::INTEGER::TEXT::internet_hours_enum AS useHrs 
  FROM
    '{from_tbl}' ft
    LEFT JOIN 
      {join_tbl} jt
    ON 
      ft.id = jt.id;
  """)
```

Checking the internet use hours parameter to get an overview along with null percentage to see if we might be able to use it, as-is, for joining with light quartiles.

```{python}

all_al = conn.sql(f"SELECT * FROM AggregatedLights;").df()
pyDescribe = all_al.describe()

duckSummary = conn.sql(f"""
SUMMARIZE
SELECT *
FROM AggregatedLights;
""").df()

duckSummary = duckSummary.set_index('column_name')

pyDescribe, duckSummary, duckSummary.at['sii', 'null_percentage']
```

Group-averaging sii, internet use hours, and pre-averaged lux quartiles by age and sex.

```{python}

conn.sql(f"""
CREATE OR REPLACE TABLE LuxAndSiiByAgeAndSex AS
SELECT
  useHrs
  ,COUNT(*) AS observations
  ,AVG(sii::INTEGER) AS sii
  ,AVG(min_q1) AS '12am_6am_avg_lux'
  ,AVG(q1_q2) AS '6am_12pm_avg_lux'
  ,AVG(q2_q3) AS '12pm_6pm_avg_lux'
  ,AVG(q3_max) AS '6pm_12am_avg_lux'
FROM 
  AggregatedLights al
GROUP BY 
  useHrs;""")

```

```{python}

pydf = conn.sql(f"""
SELECT * FROM LuxAndSiiByAgeAndSex;
""").df()

luxCategories.observations.sum()
```

```{r}
#| label: tbl-lightUseHrs

r_df <- py$pydf

notes_list =  list("Hours spent on the internet, as reported by caregivers of the participants.", "Problematic behavior score.")
locations_list = list("useHrs", "sii")

# Create a table dataframe, or tibble, of footnotes
footnotes_df <- tibble(notes = notes_list, locations = locations_list)

rTable <- r_table_theming(rDataFrame = r_df,
                          title = "Problematic Behavior to Internet Usage",
                          subtitle = NULL,
                          footnotes_df,
                          source_note = md("**source**: Kaggle"))

rTable
```

```{r}
#| label: "fig-agex"
library(scales)

r_plot <- ggplot(data = r_df, 
                 mapping = aes(x = useHrs, y = sii)
                 ) +
  geom_col(aes(fill = sii)) +
  scale_fill_gradient(low = muted("yellow"), high = muted("purple")) +
  labs(title = 'Risk Categories', 
       x = 'Hours of Internet Use', 
       y = 'Light Exposure')
 # coord_flip()

r_plot + ggplot_theming()
```

```{python}

riskyDictionary = {
  'Risk1': 
    (f",CASE WHEN ph.physical_bmi < 18.5 THEN 'Underweight'"
    ,f"WHEN ph.physical_bmi BETWEEN 18.5 AND 24.9 THEN 'Normal'"
    ,f"WHEN ph.physical_bmi BETWEEN 25.0 AND 29.9 THEN 'Increased'"
    ,f"WHEN ph.physical_bmi BETWEEN 30.0 AND 34.9 THEN 'High'"
    ,f"WHEN ph.physical_bmi BETWEEN 35.0 AND 39.9 THEN 'Very High'"
    ,f"WHEN ph.physical_bmi >= 40 THEN 'Extremely High'"
    ,f"ph.physical_waist_circumference <= 35 AND al.sex = '0'"),
  'Risk2':
    (f",CASE WHEN ph.physical_bmi < 18.5 THEN 'Underweight'"
    ,f"WHEN ph.physical_bmi BETWEEN 18.5 AND 24.9 THEN 'Normal'"
    ,f"WHEN ph.physical_bmi BETWEEN 25.0 AND 29.9 THEN 'High'"
    ,f"WHEN ph.physical_bmi BETWEEN 30.0 AND 34.9 THEN 'Very High'"
    ,f"WHEN ph.physical_bmi BETWEEN 35.0 AND 39.9 THEN 'Very High'"
    ,f"WHEN ph.physical_bmi >= 40 THEN 'Extremely High'"
    ,f"ph.physical_waist_circumference > 35 AND al.sex = '0'"),
  'Risk3':
    (f",CASE WHEN ph.physical_bmi < 18.5 THEN 'Underweight'"
    ,f"WHEN ph.physical_bmi BETWEEN 18.5 AND 24.9 THEN 'Normal'"
    ,f"WHEN ph.physical_bmi BETWEEN 25.0 AND 29.9 THEN 'Increased'"
    ,f"WHEN ph.physical_bmi BETWEEN 30.0 AND 34.9 THEN 'High'"
    ,f"WHEN ph.physical_bmi BETWEEN 35.0 AND 39.9 THEN 'Very High'"
    ,f"WHEN ph.physical_bmi >= 40 THEN 'Extremely High'"
    ,f"ph.physical_waist_circumference <= 40 AND al.sex = '1'"),
  'Risk4':
    (f",CASE WHEN ph.physical_bmi < 18.5 THEN 'Underweight'"
    ,f"WHEN ph.physical_bmi BETWEEN 18.5 AND 24.9 THEN 'Normal'"
    ,f"WHEN ph.physical_bmi BETWEEN 25.0 AND 29.9 THEN 'High'"
    ,f"WHEN ph.physical_bmi BETWEEN 30.0 AND 34.9 THEN 'Very High'"
    ,f"WHEN ph.physical_bmi BETWEEN 35.0 AND 39.9 THEN 'Very High'"
    ,f"WHEN ph.physical_bmi >= 40 THEN 'Extremely High'"
    ,f"ph.physical_waist_circumference > 40 AND al.sex = '1'")
    }

```

```{python}

riskyDf = pd.DataFrame(data = riskyDictionary)

for key, value in riskyDf.items():
  try:
    conn.sql(f"""
    CREATE OR REPLACE TABLE {key} AS
    SELECT
      al.*
      {value[0]}
      {value[1]}
      {value[2]}
      {value[3]}
      {value[4]}
      {value[5]}
      ELSE NULL
      END AS risk_cat
    ,risk_cat::disease_risk_enum AS risk_category
    FROM 
      Physical ph 
    LEFT JOIN 
      AggregatedLights al
    ON 
      al.id = ph.id
    WHERE 
      {value[6]}
    ORDER BY 
      al.id ASC;
      """)
    result = conn.execute(f"SELECT COUNT(*) FROM {key}").fetchone()
    print(f"Successfully created table: {key}, Row count: {result[0]}")
  except:
    print(f"Error loading this table: {key}")

conn.sql(f"""
CREATE OR REPLACE TABLE 
  DiseaseRiskDemographic AS
SELECT * EXCLUDE(risk_cat) FROM Risk1
UNION BY NAME
SELECT * EXCLUDE(risk_cat) FROM Risk2
UNION BY NAME
SELECT * EXCLUDE(risk_cat) FROM Risk3
UNION BY NAME 
SELECT * EXCLUDE(risk_cat) FROM Risk4;
""")

conn.sql(f"CHECKPOINT")
```

```{python}
#| include: false
#| eval: false

tablesInfo = conn.sql(f"SELECT * FROM information_schema.tables;").df()
```

```{python}

conn.sql(f"""
CREATE OR REPLACE TABLE RiskCategorySummary AS
SELECT 
  risk_category
  ,AVG(sii::INTEGER) AS sii
  ,AVG(useHrs) AS useHrs
  ,AVG(min_q1) AS q1_avg_light_exposure
  ,AVG(q1_q2) AS q2_avg_light_exposure
  ,AVG(q2_q3) AS q3_avg_light_exposure
  ,AVG(q3_max) AS q4_avg_light_exposure
FROM 
  DiseaseRiskDemographic
GROUP BY 
  risk_category;
""")

pydf = conn.execute(f"""
SELECT 
  risk_category
  ,AVG(min_q1) AS qLight
FROM 
  DiseaseRiskDemographic
GROUP BY 
  risk_category;""").df()
```

```{r}
#| label: tbl-riskefe

r_df <- py$pydf

notes_list =  list("The disease risk category.", "The average light exposure for Q1.")
locations_list = list("risk_category", "qLight")

# Create a table dataframe, or tibble, of footnotes
footnotes_df <- tibble(notes = notes_list, locations = locations_list)

rTable <- r_table_theming(rDataFrame = r_df,
                          title = "Title",
                          subtitle = NULL,
                          footnotes_df,
                          source_note = md("**source**: Kaggle"))

rTable
```

```{r}
#| label: "ggplot example"

r_plot <- ggplot(data = r_df, 
                 mapping = aes(x = risk_category, y = qLight)
                 ) +
  geom_col(fill = 'skyblue') +
  labs(title = 'Risk Categories', 
       x = 'Category', y = 'Light Exposure') +
  coord_flip()

r_plot + ggplot_theming_flipped_axes()
```

## Close Connection

```{python}
#| eval: true
try:
  conn.close()
  print(f"database connection closed")
except:
  print(f"could not close the connection")
```
